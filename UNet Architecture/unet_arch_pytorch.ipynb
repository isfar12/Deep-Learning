{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1677cb78",
   "metadata": {},
   "source": [
    "# UNet Architecture Implementation in PyTorch\n",
    "\n",
    "## Overview\n",
    "UNet is a convolutional neural network architecture designed for **semantic segmentation** tasks. It was originally developed for biomedical image segmentation but is now widely used across various domains including medical imaging, satellite imagery, and object detection.\n",
    "\n",
    "### Key Features:\n",
    "- **Encoder-Decoder Architecture**: Captures context (encoder) and enables precise localization (decoder)\n",
    "- **Skip Connections**: Concatenates features from encoder to decoder, preserving spatial information\n",
    "- **Symmetric Design**: Creates a U-shaped architecture (hence the name \"UNet\")\n",
    "\n",
    "### Architecture Flow:\n",
    "1. **Contracting Path (Encoder)**: Captures context through downsampling\n",
    "2. **Bottleneck**: Lowest resolution with highest number of feature channels\n",
    "3. **Expanding Path (Decoder)**: Enables precise localization through upsampling\n",
    "4. **Skip Connections**: Bridges encoder and decoder for better gradient flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a1d0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53809b",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "**Purpose**: Import all necessary libraries for building and testing the UNet model.\n",
    "\n",
    "### Library Breakdown:\n",
    "- **os, random, Path**: File system operations and randomization\n",
    "- **numpy**: Numerical operations\n",
    "- **PIL (Image)**: Image loading and processing\n",
    "- **torch & torch.nn**: Core PyTorch functionality for building neural networks\n",
    "- **torch.nn.functional (F)**: Functional operations like padding\n",
    "- **Dataset, DataLoader**: Data loading utilities\n",
    "- **matplotlib.pyplot**: Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1792d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8602c",
   "metadata": {},
   "source": [
    "## Building Block 1: DoubleConv\n",
    "\n",
    "### Purpose:\n",
    "The **DoubleConv** is the fundamental building block of UNet. It applies two consecutive convolution operations, each followed by batch normalization and ReLU activation.\n",
    "\n",
    "### Architecture Pattern:\n",
    "```\n",
    "Input → Conv2d → BatchNorm → ReLU → Conv2d → BatchNorm → ReLU → Output\n",
    "```\n",
    "\n",
    "### Parameter Explanation:\n",
    "\n",
    "#### 1. **in_channels** (int)\n",
    "- **What**: Number of input feature channels/maps\n",
    "- **Why**: Tells the network how many feature maps to expect as input\n",
    "- **Scenarios**:\n",
    "  - `in_channels=3`: RGB image input (Red, Green, Blue channels)\n",
    "  - `in_channels=1`: Grayscale image or single feature map\n",
    "  - `in_channels=64`: Feature maps from previous layer\n",
    "  - `in_channels=512`: Deep layer with many learned features\n",
    "\n",
    "#### 2. **out_channels** (int)\n",
    "- **What**: Number of output feature channels/maps to produce\n",
    "- **Why**: Determines how many different features/patterns the layer will learn\n",
    "- **Scenarios**:\n",
    "  - `out_channels=64`: Early layers learn basic features (edges, textures)\n",
    "  - `out_channels=512`: Deep layers learn complex semantic features (shapes, objects)\n",
    "  - **More channels = More capacity** but also more parameters and memory\n",
    "- **Trade-off**: \n",
    "  - Higher values → Better feature representation but slower training\n",
    "  - Lower values → Faster but may miss important patterns\n",
    "\n",
    "#### 3. **mid_channels** (int, optional, default=None)\n",
    "- **What**: Number of channels after the first convolution\n",
    "- **Why**: Allows asymmetric channel progression for flexibility\n",
    "- **Default behavior**: If `None`, uses `out_channels` (symmetric)\n",
    "- **Scenarios**:\n",
    "  ```python\n",
    "  # Symmetric (default): 64 → 128 → 128\n",
    "  DoubleConv(64, 128)  # mid_channels automatically = 128\n",
    "  \n",
    "  # Asymmetric: 64 → 96 → 128\n",
    "  DoubleConv(64, 128, mid_channels=96)  # Gradual channel increase\n",
    "  ```\n",
    "- **Use case**: Memory optimization or gradual feature expansion\n",
    "\n",
    "### Conv2d Parameters Deep Dive:\n",
    "\n",
    "#### **kernel_size=3**\n",
    "- **What**: 3×3 filter/receptive field\n",
    "- **Why**: \n",
    "  - Standard in deep networks (proven effective)\n",
    "  - Small enough to stack many layers\n",
    "  - Large enough to capture local patterns\n",
    "  - 3×3 is optimal balance (2 stacked 3×3 = 5×5 receptive field with fewer parameters)\n",
    "- **Alternative scenarios**:\n",
    "  - `kernel_size=1`: Point-wise convolution (like OutConv)\n",
    "  - `kernel_size=5`: Larger receptive field but 2.7× more parameters\n",
    "  - `kernel_size=7`: Common in first layer for capturing large patterns\n",
    "\n",
    "#### **padding=1**\n",
    "- **What**: Adds 1 pixel border around input before convolution\n",
    "- **Why**: **Preserves spatial dimensions**\n",
    "- **Math**: Output size = (Input size - Kernel size + 2×Padding) / Stride + 1\n",
    "  ```\n",
    "  With padding=1:  (H - 3 + 2×1) / 1 + 1 = H  ✓ Same size\n",
    "  Without padding:  (H - 3 + 0) / 1 + 1 = H-2  ✗ Shrinks\n",
    "  ```\n",
    "- **Scenarios**:\n",
    "  - `padding=1` with `kernel_size=3`: Maintains size (used in UNet)\n",
    "  - `padding=0`: Reduces size (valid convolution)\n",
    "  - `padding=2` with `kernel_size=5`: Maintains size\n",
    "- **Why it matters**: UNet needs consistent sizes for skip connections\n",
    "\n",
    "#### **bias=False**\n",
    "- **What**: Removes learnable bias term from convolution\n",
    "- **Why**: BatchNorm2d already includes bias (shift parameter β)\n",
    "- **Math**: \n",
    "  ```\n",
    "  With bias:     Conv(x) = W*x + b, then BN(Conv(x)) = γ·norm(W*x + b) + β\n",
    "  Without bias:  Conv(x) = W*x,     then BN(Conv(x)) = γ·norm(W*x) + β\n",
    "  ```\n",
    "- **Benefit**: \n",
    "  - Saves parameters (no redundant bias)\n",
    "  - BatchNorm's β serves as the bias term\n",
    "  - Slightly faster computation\n",
    "- **When to use bias=True**: When NOT using BatchNorm\n",
    "\n",
    "### BatchNorm2d Parameters:\n",
    "\n",
    "- **What**: Normalizes activations to have mean≈0 and variance≈1\n",
    "- **Formula**: `output = γ × (input - μ) / √(σ² + ε) + β`\n",
    "  - μ: batch mean, σ²: batch variance\n",
    "  - γ: learnable scale, β: learnable shift\n",
    "- **Why**:\n",
    "  - **Stabilizes training**: Prevents internal covariate shift\n",
    "  - **Allows higher learning rates**: More stable gradients\n",
    "  - **Regularization effect**: Acts like dropout during training\n",
    "- **Scenarios**:\n",
    "  - Training: Uses batch statistics (mean/var of current batch)\n",
    "  - Inference: Uses running statistics (accumulated during training)\n",
    "\n",
    "### ReLU Parameters:\n",
    "\n",
    "#### **inplace=True**\n",
    "- **What**: Modifies input tensor directly instead of creating new tensor\n",
    "- **Why**: **Saves memory**\n",
    "- **Scenarios**:\n",
    "  ```python\n",
    "  # inplace=False (default): x_output = ReLU(x_input)  → 2 tensors in memory\n",
    "  # inplace=True:            x = ReLU(x)               → 1 tensor in memory\n",
    "  ```\n",
    "- **Memory saving**: With 100 layers, saves 100× intermediate tensors\n",
    "- **Trade-off**: Can't access original values (not needed in forward pass)\n",
    "- **When NOT to use**: If you need gradients of input (handled automatically by PyTorch)\n",
    "\n",
    "### Image Processing Flow:\n",
    "1. **First Convolution**: \n",
    "   - Input: [B, in_channels, H, W]\n",
    "   - Applies 3×3 filters with padding=1\n",
    "   - Extracts initial features (edges, colors, textures)\n",
    "   - Output: [B, mid_channels, H, W]\n",
    "   \n",
    "2. **Batch Normalization**: \n",
    "   - Normalizes across batch dimension\n",
    "   - Ensures stable distribution of activations\n",
    "   - Output: [B, mid_channels, H, W]\n",
    "   \n",
    "3. **ReLU Activation**: \n",
    "   - Applies: f(x) = max(0, x)\n",
    "   - Introduces non-linearity (allows learning complex patterns)\n",
    "   - Zeros out negative values\n",
    "   - Output: [B, mid_channels, H, W]\n",
    "   \n",
    "4. **Second Convolution**: \n",
    "   - Further refines the extracted features\n",
    "   - Combines patterns from first conv\n",
    "   - Output: [B, out_channels, H, W]\n",
    "   \n",
    "5. **Final BatchNorm + ReLU**: \n",
    "   - Normalizes and activates the final output\n",
    "   - Output: [B, out_channels, H, W]\n",
    "\n",
    "### Practical Example:\n",
    "```python\n",
    "# Scenario: Converting RGB image to 64 feature maps\n",
    "conv_block = DoubleConv(in_channels=3, out_channels=64)\n",
    "\n",
    "# Input: RGB image [1, 3, 256, 256]\n",
    "# After 1st Conv2d: [1, 64, 256, 256]  - spatial size preserved by padding\n",
    "# After 1st BN+ReLU: [1, 64, 256, 256] - normalized and activated\n",
    "# After 2nd Conv2d: [1, 64, 256, 256]  - refined features\n",
    "# Output after 2nd BN+ReLU: [1, 64, 256, 256]\n",
    "\n",
    "# Total learnable parameters:\n",
    "# Conv1: (3×3×3×64) + BatchNorm(64×2) = 1,728 + 128 = 1,856\n",
    "# Conv2: (3×3×64×64) + BatchNorm(64×2) = 36,864 + 128 = 36,992\n",
    "# Total: 38,848 parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"DoubleConv is a block that combines Convolution + Batch Normalization + ReLU,\n",
    "    and this sequence is applied twice.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b35f273",
   "metadata": {},
   "source": [
    "## Building Block 2: Down (Encoder Block)\n",
    "\n",
    "### Purpose:\n",
    "The **Down** block is responsible for the **contracting path** (encoder) in UNet. It reduces spatial dimensions while increasing the number of feature channels.\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input → MaxPool2d (2×2) → DoubleConv → Output\n",
    "```\n",
    "\n",
    "### Parameter Explanation:\n",
    "\n",
    "#### 1. **in_channels** (int)\n",
    "- **What**: Number of input feature channels\n",
    "- **Why**: Must match output channels from previous layer\n",
    "- **Scenarios**:\n",
    "  ```python\n",
    "  down1 = Down(64, 128)   # After initial 64-channel layer\n",
    "  down2 = Down(128, 256)  # After down1 outputs 128 channels\n",
    "  down3 = Down(256, 512)  # Progressive deepening\n",
    "  ```\n",
    "- **Pattern**: Each down block typically doubles channels\n",
    "\n",
    "#### 2. **out_channels** (int)\n",
    "- **What**: Number of output feature channels\n",
    "- **Why**: Increases model capacity as spatial size decreases\n",
    "- **Principle**: **Spatial resolution ↓ ⇒ Channel count ↑**\n",
    "- **Reasoning**:\n",
    "  - Less spatial information → Need more features to compensate\n",
    "  - Smaller feature maps → Can afford more channels (memory-wise)\n",
    "  - Higher-level semantics require richer representations\n",
    "\n",
    "### MaxPool2d(2) Parameters Deep Dive:\n",
    "\n",
    "#### **kernel_size=2**\n",
    "- **What**: Size of pooling window (2×2 region)\n",
    "- **Why**: Standard for downsampling by factor of 2\n",
    "- **Operation**: \n",
    "  ```\n",
    "  Input 4×4:           After MaxPool2d(2):\n",
    "  [1  2  3  4]         [6  8]\n",
    "  [5  6  7  8]    →    [14 16]\n",
    "  [9  10 11 12]\n",
    "  [13 14 15 16]\n",
    "  \n",
    "  Takes maximum from each 2×2 region\n",
    "  ```\n",
    "- **Scenarios**:\n",
    "  - `MaxPool2d(2)`: Halves dimensions (256→128)\n",
    "  - `MaxPool2d(3)`: Reduces by 3× (256→85) - less common\n",
    "  - `MaxPool2d(4)`: Quarters dimensions (256→64) - aggressive\n",
    "\n",
    "#### **Stride (default=kernel_size=2)**\n",
    "- **What**: Step size when sliding the pooling window\n",
    "- **Default**: Same as kernel_size (non-overlapping windows)\n",
    "- **Why stride=2**:\n",
    "  - Non-overlapping pooling (most common)\n",
    "  - Clean 50% reduction in each dimension\n",
    "  - No redundant computations\n",
    "- **Alternative**: `stride=1` would create overlapping pools (rarely used in UNet)\n",
    "\n",
    "### Why MaxPooling?\n",
    "\n",
    "#### **Advantages**:\n",
    "1. **Translation Invariance**: Small shifts in input don't change output much\n",
    "2. **Computational Efficiency**: Reduces feature map size → faster processing\n",
    "3. **Receptive Field Growth**: Each neuron \"sees\" larger area of original image\n",
    "4. **Feature Concentration**: Keeps strongest activations\n",
    "\n",
    "#### **Alternatives and Trade-offs**:\n",
    "```python\n",
    "# Option 1: MaxPool (used in UNet)\n",
    "nn.MaxPool2d(2)  \n",
    "# + Simple, no parameters\n",
    "# + Preserves strong activations\n",
    "# - Not learnable\n",
    "\n",
    "# Option 2: Strided Convolution\n",
    "nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=2, padding=1)\n",
    "# + Learnable downsampling\n",
    "# + Smoother reduction\n",
    "# - More parameters, slower\n",
    "\n",
    "# Option 3: Average Pooling\n",
    "nn.AvgPool2d(2)\n",
    "# + Smoother, less aggressive\n",
    "# - May lose sharp features\n",
    "# - Not common in segmentation\n",
    "```\n",
    "\n",
    "### Image Processing Flow:\n",
    "\n",
    "#### Step-by-Step Transformation:\n",
    "```python\n",
    "Input:  [Batch, 64, 256, 256]   # 64 feature channels, 256×256 spatial\n",
    "        ↓\n",
    "MaxPool2d(2):\n",
    "- Slides 2×2 window with stride=2\n",
    "- Takes maximum value in each window\n",
    "- Reduces spatial dimensions by half\n",
    "Result: [Batch, 64, 128, 128]   # Channels unchanged, size halved\n",
    "        ↓\n",
    "DoubleConv(64, 128):\n",
    "- Two 3×3 convolutions with padding\n",
    "- Increases channels from 64 to 128\n",
    "- Spatial size preserved by padding\n",
    "Result: [Batch, 128, 128, 128]  # More features, same spatial size\n",
    "```\n",
    "\n",
    "### Receptive Field Explanation:\n",
    "\n",
    "**What is Receptive Field?**\n",
    "- Area of the input image that affects a single output neuron\n",
    "\n",
    "**How Down blocks increase it**:\n",
    "```\n",
    "Layer 0 (input):     Receptive field = 1×1 pixel\n",
    "After Conv 3×3:      Receptive field = 3×3 pixels\n",
    "After MaxPool:       Receptive field = 6×6 pixels (doubled)\n",
    "After another Conv:  Receptive field = 10×10 pixels\n",
    "After another MaxPool: Receptive field = 20×20 pixels\n",
    "```\n",
    "\n",
    "**Why it matters**:\n",
    "- Larger receptive field → See more context\n",
    "- Early layers: Small receptive field (local patterns: edges, textures)\n",
    "- Deep layers: Large receptive field (global patterns: objects, scenes)\n",
    "\n",
    "### Practical Example with Numbers:\n",
    "\n",
    "```python\n",
    "# Creating a Down block\n",
    "down_block = Down(in_channels=64, out_channels=128)\n",
    "\n",
    "# Input: Feature maps from previous layer\n",
    "input_tensor = torch.randn(4, 64, 256, 256)  # Batch=4, Channels=64, Size=256×256\n",
    "\n",
    "# Process:\n",
    "output = down_block(input_tensor)\n",
    "\n",
    "# Output shape: torch.Size([4, 128, 128, 128])\n",
    "# Memory: \n",
    "#   Input:  4 × 64 × 256 × 256 = 16,777,216 values (64 MB float32)\n",
    "#   Output: 4 × 128 × 128 × 128 = 8,388,608 values (32 MB float32)\n",
    "# Memory saved: 50% (due to spatial reduction)\n",
    "\n",
    "# Parameters in DoubleConv(64, 128):\n",
    "# Conv1: 3×3×64×128 = 73,728\n",
    "# Conv2: 3×3×128×128 = 147,456\n",
    "# Total: ~221K parameters + BatchNorm\n",
    "```\n",
    "\n",
    "### Real-World Scenario:\n",
    "\n",
    "**Medical Image Segmentation (Cell Detection)**:\n",
    "```python\n",
    "# down1: Initial features (64→128)\n",
    "# Learns: Cell boundaries, membrane edges\n",
    "# Receptive field: ~10×10 pixels\n",
    "\n",
    "# down2: Mid-level features (128→256)\n",
    "# Learns: Cell shapes, nucleus patterns\n",
    "# Receptive field: ~40×40 pixels\n",
    "\n",
    "# down3: High-level features (256→512)\n",
    "# Learns: Cell clusters, tissue structures\n",
    "# Receptive field: ~160×160 pixels\n",
    "\n",
    "# down4: Semantic features (512→1024)\n",
    "# Learns: Organ regions, global context\n",
    "# Receptive field: ~640×640 pixels (whole tissue sample)\n",
    "```\n",
    "\n",
    "### Why Downsample?\n",
    "\n",
    "1. **Computational Efficiency**: \n",
    "   - 256×256 → 128×128 = 75% fewer computations\n",
    "   - Enables deeper networks\n",
    "\n",
    "2. **Hierarchical Features**:\n",
    "   - Early: Low-level (edges, colors)\n",
    "   - Middle: Mid-level (textures, patterns)\n",
    "   - Deep: High-level (objects, semantics)\n",
    "\n",
    "3. **Context Aggregation**:\n",
    "   - Combines information from larger regions\n",
    "   - Understands \"what\" is in the image\n",
    "\n",
    "4. **Memory Management**:\n",
    "   - Smaller feature maps use less GPU memory\n",
    "   - Allows higher batch sizes or more channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c89abc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e1a172",
   "metadata": {},
   "source": [
    "## Building Block 3: Up (Decoder Block)\n",
    "\n",
    "### Purpose:\n",
    "The **Up** block is responsible for the **expanding path** (decoder) in UNet. It increases spatial dimensions while reducing feature channels, and **crucially**, it incorporates skip connections from the encoder.\n",
    "\n",
    "### Architecture (Two Modes):\n",
    "\n",
    "#### Mode 1: Bilinear Upsampling\n",
    "```\n",
    "Input (from below) → Bilinear Upsample (2×) → Concatenate with Skip Connection → DoubleConv → Output\n",
    "```\n",
    "\n",
    "#### Mode 2: Transposed Convolution\n",
    "```\n",
    "Input (from below) → ConvTranspose2d → Concatenate with Skip Connection → DoubleConv → Output\n",
    "```\n",
    "\n",
    "### Parameter Explanation:\n",
    "\n",
    "#### 1. **in_channels** (int)\n",
    "- **What**: Total number of channels AFTER concatenation\n",
    "- **Why**: Must account for both upsampled features AND skip connection\n",
    "- **Critical Understanding**:\n",
    "  ```python\n",
    "  # Example: up1 = Up(1024, 512, bilinear=False)\n",
    "  # Input from below (x1): 1024 channels (bottleneck)\n",
    "  # After upsampling: 512 channels (halved by upsampling)\n",
    "  # Skip connection (x2): 512 channels (from encoder)\n",
    "  # After concatenation: 512 + 512 = 1024 channels\n",
    "  # Therefore, in_channels = 1024 ✓\n",
    "  ```\n",
    "- **Scenarios**:\n",
    "  ```python\n",
    "  # UNet progression:\n",
    "  Up(1024, 512)  # in=1024 because 512(upsampled) + 512(skip)\n",
    "  Up(512, 256)   # in=512 because 256(upsampled) + 256(skip)\n",
    "  Up(256, 128)   # in=256 because 128(upsampled) + 128(skip)\n",
    "  Up(128, 64)    # in=128 because 64(upsampled) + 64(skip)\n",
    "  ```\n",
    "\n",
    "#### 2. **out_channels** (int)\n",
    "- **What**: Number of output feature channels\n",
    "- **Why**: Progressively reduces channels as we go up (inverse of encoder)\n",
    "- **Pattern**: Typically halves at each level (1024→512→256→128→64)\n",
    "- **Reasoning**: \n",
    "  - Upsampling increases spatial detail\n",
    "  - Less need for many channels at high resolution\n",
    "  - Mirrors encoder structure symmetrically\n",
    "\n",
    "#### 3. **bilinear** (bool, default=True)\n",
    "- **What**: Chooses upsampling method\n",
    "- **Why**: Trade-off between speed/memory vs quality\n",
    "\n",
    "##### **bilinear=True (Interpolation-based)**:\n",
    "- **Method**: Mathematical interpolation (no learning)\n",
    "- **Process**: \n",
    "  ```\n",
    "  Original pixels:    Interpolated (bilinear):\n",
    "  [1  2]             [1    1.5   2  ]\n",
    "  [3  4]       →     [2    2.5   3  ]\n",
    "                     [3    3.5   4  ]\n",
    "  \n",
    "  New pixels are weighted averages of neighbors\n",
    "  ```\n",
    "- **Formula**: `f(x,y) = w1·p1 + w2·p2 + w3·p3 + w4·p4` (weighted average)\n",
    "- **Advantages**:\n",
    "  - ✓ No parameters (0 MB)\n",
    "  - ✓ Faster forward pass\n",
    "  - ✓ Less GPU memory\n",
    "  - ✓ Good for limited hardware\n",
    "  - ✓ No risk of checkerboard artifacts\n",
    "- **Disadvantages**:\n",
    "  - ✗ Not learnable (fixed interpolation)\n",
    "  - ✗ May miss fine details\n",
    "  - ✗ Slightly lower segmentation quality\n",
    "- **When to use**: \n",
    "  - Limited GPU memory\n",
    "  - Real-time applications\n",
    "  - Initial prototyping\n",
    "  - Medical imaging with clear boundaries\n",
    "\n",
    "##### **bilinear=False (Transposed Convolution)**:\n",
    "- **Method**: Learned upsampling using ConvTranspose2d\n",
    "- **Process**: \n",
    "  ```\n",
    "  Learnable 2×2 kernel:\n",
    "  [w1  w2]\n",
    "  [w3  w4]\n",
    "  \n",
    "  Input pixel \"x\" gets multiplied by kernel and spread:\n",
    "  x → [x·w1  x·w2]\n",
    "      [x·w3  x·w4]\n",
    "  \n",
    "  Overlapping regions get summed\n",
    "  ```\n",
    "- **Advantages**:\n",
    "  - ✓ Learnable (adapts to data)\n",
    "  - ✓ Better fine detail recovery\n",
    "  - ✓ Higher segmentation accuracy\n",
    "  - ✓ Can learn task-specific upsampling\n",
    "- **Disadvantages**:\n",
    "  - ✗ More parameters (~2-4x more)\n",
    "  - ✗ More GPU memory needed\n",
    "  - ✗ Risk of checkerboard artifacts\n",
    "  - ✗ Slower training\n",
    "- **When to use**:\n",
    "  - Sufficient GPU memory available\n",
    "  - Accuracy is priority\n",
    "  - Fine-grained segmentation needed\n",
    "  - Satellite/aerial imagery\n",
    "\n",
    "### Upsampling Methods Comparison:\n",
    "\n",
    "```python\n",
    "# Example: Upsample from 64×64 to 128×128\n",
    "\n",
    "# Method 1: Bilinear (bilinear=True)\n",
    "self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "# Parameters: 0\n",
    "# Output: Smooth interpolation\n",
    "# Speed: Fast (no learning)\n",
    "\n",
    "# Method 2: Transposed Conv (bilinear=False)\n",
    "self.up = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "# Parameters: 2×2×512×256 = 524,288 (512K params!)\n",
    "# Output: Learned upsampling\n",
    "# Speed: Slower (backprop through convolution)\n",
    "```\n",
    "\n",
    "### Handling Dimension Mismatch:\n",
    "\n",
    "**The Problem**:\n",
    "```\n",
    "Encoder (x2): [B, 512, 134, 134]  # May have odd dimensions from pooling\n",
    "Upsampled (x1): [B, 512, 128, 128]  # Clean power of 2\n",
    "# Cannot concatenate! Dimension mismatch!\n",
    "```\n",
    "\n",
    "**The Solution - Padding**:\n",
    "```python\n",
    "diffY = x2.size()[2] - x1.size()[2]  # 134 - 128 = 6\n",
    "diffX = x2.size()[3] - x1.size()[3]  # 134 - 128 = 6\n",
    "\n",
    "# Pad x1 to match x2:\n",
    "x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                diffY // 2, diffY - diffY // 2])\n",
    "# Pads: [left=3, right=3, top=3, bottom=3]\n",
    "# Result: x1 is now [B, 512, 134, 134] ✓\n",
    "```\n",
    "\n",
    "**Padding Parameters**:\n",
    "- **Format**: `[left, right, top, bottom]`\n",
    "- **Why split diffX//2 and diffX-diffX//2?**\n",
    "  - Handles odd differences (e.g., diff=7 → left=3, right=4)\n",
    "  - Centers the tensor symmetrically\n",
    "- **align_corners=True in Upsample**:\n",
    "  - Aligns corner pixels during interpolation\n",
    "  - Reduces dimension mismatch issues\n",
    "  - Makes skip connection fusion more accurate\n",
    "\n",
    "### Skip Connection Deep Dive:\n",
    "\n",
    "**What happens**:\n",
    "```python\n",
    "x = torch.cat([x2, x1], dim=1)\n",
    "```\n",
    "\n",
    "**Parameters**:\n",
    "- **[x2, x1]**: Order matters! Usually encoder features first\n",
    "- **dim=1**: Concatenate along channel dimension\n",
    "  ```\n",
    "  Before:\n",
    "  x2 (skip): [B, 512, 64, 64]  # High-res features from encoder\n",
    "  x1 (up):   [B, 512, 64, 64]  # Low-res features upsampled\n",
    "  \n",
    "  After concatenation:\n",
    "  x: [B, 1024, 64, 64]  # Combined features\n",
    "  ```\n",
    "\n",
    "**Why concatenate (not add)?**:\n",
    "```python\n",
    "# Option 1: Concatenation (used in UNet)\n",
    "x = torch.cat([x2, x1], dim=1)  # [B, 512+512, H, W]\n",
    "# + Preserves both feature sets completely\n",
    "# + Network learns how to combine them\n",
    "# - Doubles channel count (more parameters)\n",
    "\n",
    "# Option 2: Addition (used in ResNet)\n",
    "x = x2 + x1  # [B, 512, H, W]\n",
    "# + Fewer parameters\n",
    "# + Forces channel counts to match\n",
    "# - Information loss (forced merge)\n",
    "# - Less flexible\n",
    "\n",
    "# UNet uses concatenation for maximum information preservation!\n",
    "```\n",
    "\n",
    "### Image Processing Flow:\n",
    "\n",
    "```python\n",
    "# Example: up1 = Up(1024, 512, bilinear=False)\n",
    "\n",
    "# Inputs:\n",
    "x1 = [B, 1024, 32, 32]  # From bottleneck (deep, low-res)\n",
    "x2 = [B, 512, 64, 64]   # From encoder (skip connection, high-res)\n",
    "\n",
    "# Step 1: Upsample x1\n",
    "# ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "x1 = self.up(x1)\n",
    "# Result: [B, 512, 64, 64]  # Doubled spatial, halved channels\n",
    "\n",
    "# Step 2: Check dimensions\n",
    "diffY = 64 - 64 = 0  # No difference\n",
    "diffX = 64 - 64 = 0  # No difference\n",
    "# No padding needed ✓\n",
    "\n",
    "# Step 3: Concatenate\n",
    "x = torch.cat([x2, x1], dim=1)\n",
    "# Result: [B, 1024, 64, 64]  # 512 + 512 channels\n",
    "\n",
    "# Step 4: DoubleConv\n",
    "x = self.conv(x)  # DoubleConv(1024, 512)\n",
    "# Result: [B, 512, 64, 64]  # Final output\n",
    "```\n",
    "\n",
    "### Real-World Scenario Example:\n",
    "\n",
    "**Satellite Image Segmentation (Building Detection)**:\n",
    "\n",
    "```python\n",
    "# Encoder captured:\n",
    "# x1 (64×64): \"There's a rectangular structure here\" (semantic)\n",
    "# x2 (256×256): \"Exact pixel-level roof edges\" (spatial detail)\n",
    "\n",
    "# Problem: x1 knows WHAT (building) but not exactly WHERE\n",
    "#          x2 knows WHERE (edges) but lost during downsampling\n",
    "\n",
    "# Up block solution:\n",
    "# 1. Upsample x1 to 128×128: Bring semantic info to higher resolution\n",
    "# 2. Concatenate with x2 (128×128 skip): Add precise spatial detail\n",
    "# 3. DoubleConv: Fuse \"WHAT\" + \"WHERE\" → Precise building boundaries\n",
    "\n",
    "# Result: Accurate pixel-level building segmentation!\n",
    "```\n",
    "\n",
    "### Parameter Count Comparison:\n",
    "\n",
    "```python\n",
    "# bilinear=True\n",
    "up_layer_bilinear = Up(1024, 512, bilinear=True)\n",
    "# Upsample: 0 parameters\n",
    "# DoubleConv(1024, 512): ~4.7M parameters\n",
    "# Total: ~4.7M\n",
    "\n",
    "# bilinear=False  \n",
    "up_layer_learned = Up(1024, 512, bilinear=False)\n",
    "# ConvTranspose2d: 2×2×1024×512 = 2.1M parameters\n",
    "# DoubleConv(1024, 512): ~4.7M parameters\n",
    "# Total: ~6.8M (45% more parameters!)\n",
    "\n",
    "# For full UNet with 4 Up blocks:\n",
    "# bilinear=True: ~31M total parameters\n",
    "# bilinear=False: ~38M total parameters\n",
    "# Difference: 7M parameters (22% increase)\n",
    "```\n",
    "\n",
    "### Checkerboard Artifacts (bilinear=False issue):\n",
    "\n",
    "**What**: Checkerboard patterns in output\n",
    "**Cause**: Overlapping regions in ConvTranspose2d with certain kernel/stride combinations\n",
    "**Solution**: \n",
    "- Use kernel_size divisible by stride (2÷2=1 ✓)\n",
    "- Or use bilinear=True\n",
    "- Or use resize + convolution instead\n",
    "\n",
    "### When to Choose Which:\n",
    "\n",
    "| Scenario | bilinear=True | bilinear=False |\n",
    "|----------|---------------|----------------|\n",
    "| Limited GPU memory (< 8GB) | ✓ | ✗ |\n",
    "| Real-time inference required | ✓ | ✗ |\n",
    "| Maximum accuracy needed | ✗ | ✓ |\n",
    "| Medical imaging (clear boundaries) | ✓ | Either |\n",
    "| Natural images (complex textures) | ✗ | ✓ |\n",
    "| Prototyping/testing | ✓ | ✗ |\n",
    "| Production model | Either | ✓ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8c1ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a9ebe",
   "metadata": {},
   "source": [
    "## Building Block 4: OutConv (Output Layer)\n",
    "\n",
    "### Purpose:\n",
    "The **OutConv** is the final layer that produces the segmentation output. It uses a 1×1 convolution to map feature channels to the number of classes.\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "Input → Conv2d (1×1 kernel) → Output (Logits)\n",
    "```\n",
    "\n",
    "### Parameter Explanation:\n",
    "\n",
    "#### 1. **in_channels** (int)\n",
    "- **What**: Number of input feature channels from last Up block\n",
    "- **Why**: Must match the output of up4 (final decoder layer)\n",
    "- **Typical value**: 64 (standard UNet architecture)\n",
    "- **Scenario**: \n",
    "  ```python\n",
    "  up4_output = [B, 64, 572, 572]  # Last decoder output\n",
    "  outc = OutConv(in_channels=64, out_channels=n_classes)\n",
    "  ```\n",
    "\n",
    "#### 2. **out_channels** (int) = n_classes\n",
    "- **What**: Number of segmentation classes\n",
    "- **Why**: Each channel represents probability/logit for one class\n",
    "- **Determines**: Type of segmentation task\n",
    "\n",
    "##### **Scenarios**:\n",
    "\n",
    "**Binary Segmentation (n_classes=1)**:\n",
    "```python\n",
    "OutConv(64, 1)  # Single channel output\n",
    "# Examples:\n",
    "# - Background vs Foreground\n",
    "# - Tumor vs Healthy tissue\n",
    "# - Road vs Non-road\n",
    "# - Cell vs Background\n",
    "\n",
    "# Output: [B, 1, H, W]\n",
    "# Each pixel has ONE value (logit)\n",
    "# Apply sigmoid: probability of positive class\n",
    "# Decision: if sigmoid(logit) > 0.5 → Class 1, else → Class 0\n",
    "```\n",
    "\n",
    "**Multi-class Segmentation (n_classes>1)**:\n",
    "```python\n",
    "OutConv(64, 3)  # Three classes\n",
    "# Example: Cityscapes - Road, Vehicle, Pedestrian\n",
    "\n",
    "# Output: [B, 3, H, W]\n",
    "# Each pixel has 3 values (logits for each class)\n",
    "# Apply softmax: probabilities sum to 1\n",
    "# Decision: argmax(softmax(logits)) → Predicted class\n",
    "\n",
    "# Another example:\n",
    "OutConv(64, 21)  # Pascal VOC - 20 object classes + background\n",
    "# Output: [B, 21, H, W]\n",
    "```\n",
    "\n",
    "**Medical Multi-organ Segmentation**:\n",
    "```python\n",
    "OutConv(64, 5)  # n_classes=5\n",
    "# Classes: Background, Heart, Lung-Left, Lung-Right, Liver\n",
    "# Output: [B, 5, H, W]\n",
    "\n",
    "# Pixel [100, 200] logits: [-2.3, 0.5, 3.2, 1.1, -0.8]\n",
    "# After softmax: [0.01, 0.15, 0.72, 0.09, 0.03]\n",
    "# Prediction: Class 2 (Lung-Left) with 72% confidence\n",
    "```\n",
    "\n",
    "### kernel_size=1 Deep Dive:\n",
    "\n",
    "**What is 1×1 Convolution?**:\n",
    "```\n",
    "Regular 3×3 Conv:          1×1 Conv:\n",
    "Uses 3×3 neighborhood      Uses only center pixel\n",
    "╔═══╗                      ╔═══╗\n",
    "║ X ║                      ║   ║\n",
    "╚═══╝                      ║ X ║\n",
    "                           ║   ║\n",
    "                           ╚═══╝\n",
    "```\n",
    "\n",
    "**Mathematical Operation**:\n",
    "```python\n",
    "# Input: [B, 64, H, W]\n",
    "# Kernel: [1, 1, 64, n_classes]\n",
    "\n",
    "# For each spatial position (i, j):\n",
    "# For each output channel c:\n",
    "output[b, c, i, j] = Σ(k=0 to 63) input[b, k, i, j] × weight[c, k]\n",
    "\n",
    "# It's a LINEAR COMBINATION of input channels at each pixel\n",
    "# No spatial mixing - purely channel transformation\n",
    "```\n",
    "\n",
    "**Why 1×1 Instead of 3×3?**:\n",
    "\n",
    "1. **Efficiency**:\n",
    "   ```\n",
    "   3×3 Conv: 3×3×64×n_classes parameters\n",
    "   1×1 Conv: 1×1×64×n_classes parameters\n",
    "   # 9× fewer parameters!\n",
    "   \n",
    "   Example (n_classes=1):\n",
    "   3×3: 3×3×64×1 = 576 parameters\n",
    "   1×1: 1×1×64×1 = 64 parameters\n",
    "   ```\n",
    "\n",
    "2. **Spatial Independence**:\n",
    "   - Each pixel classified independently\n",
    "   - Spatial context already captured by encoder/decoder\n",
    "   - Final layer just needs to map features → classes\n",
    "\n",
    "3. **No Spatial Distortion**:\n",
    "   - Padding not needed (1×1 doesn't change spatial size)\n",
    "   - Output size exactly matches input size\n",
    "\n",
    "4. **Cleaner Semantics**:\n",
    "   - \"For this pixel, given its 64 features, which class is it?\"\n",
    "   - Not mixing with neighbor information at final stage\n",
    "\n",
    "**Example Visualization**:\n",
    "```python\n",
    "# Input from up4: [1, 64, 256, 256]\n",
    "# 64 feature channels per pixel\n",
    "\n",
    "# Pixel at (100, 150) has features:\n",
    "features = [0.2, -0.5, 1.3, ..., 0.8]  # 64 values\n",
    "\n",
    "# For binary segmentation (n_classes=1):\n",
    "# Weight vector: w = [w0, w1, ..., w63]\n",
    "# Output logit = Σ(features[i] × w[i])\n",
    "output[0, 0, 100, 150] = features @ weights  # Dot product\n",
    "\n",
    "# If output = 2.5 → sigmoid(2.5) = 0.92 → 92% probability of class 1\n",
    "```\n",
    "\n",
    "### No Activation Function?\n",
    "\n",
    "**Important**: OutConv outputs **raw logits**, not probabilities\n",
    "\n",
    "**Why no activation here?**:\n",
    "```python\n",
    "# Training (in loss function):\n",
    "# Binary: Uses BCEWithLogitsLoss (combines sigmoid + BCE)\n",
    "loss = nn.BCEWithLogitsLoss()(output, target)\n",
    "# Numerically more stable than separate sigmoid + BCE\n",
    "\n",
    "# Multi-class: Uses CrossEntropyLoss (combines softmax + NLL)\n",
    "loss = nn.CrossEntropyLoss()(output, target)\n",
    "# More stable than separate softmax + NLL\n",
    "\n",
    "# Inference (after training):\n",
    "# Binary:\n",
    "probabilities = torch.sigmoid(output)\n",
    "predictions = (probabilities > 0.5).float()\n",
    "\n",
    "# Multi-class:\n",
    "probabilities = torch.softmax(output, dim=1)\n",
    "predictions = torch.argmax(probabilities, dim=1)\n",
    "```\n",
    "\n",
    "### Image Processing Flow:\n",
    "\n",
    "```python\n",
    "# Binary Segmentation Example:\n",
    "input = [B, 64, 572, 572]  # 64 rich feature channels\n",
    "        ↓\n",
    "OutConv(64, 1)\n",
    "        ↓\n",
    "output = [B, 1, 572, 572]  # 1 channel with logits\n",
    "\n",
    "# Each pixel value is a logit:\n",
    "# Positive logit → More likely class 1\n",
    "# Negative logit → More likely class 0\n",
    "\n",
    "# Multi-class Example:\n",
    "input = [B, 64, 256, 256]\n",
    "        ↓\n",
    "OutConv(64, 10)\n",
    "        ↓\n",
    "output = [B, 10, 256, 256]  # 10 classes\n",
    "\n",
    "# For pixel (50, 50):\n",
    "# output[0, :, 50, 50] = [1.2, -0.5, 3.1, ..., 0.8]\n",
    "# After softmax: [0.12, 0.02, 0.78, ..., 0.08]\n",
    "# Prediction: Class 2 (index with max value)\n",
    "```\n",
    "\n",
    "### Complete Pipeline Example:\n",
    "\n",
    "```python\n",
    "# Medical Image: Tumor Segmentation\n",
    "# n_classes = 3: Background, Benign, Malignant\n",
    "\n",
    "# 1. Network learns features through encoder/decoder\n",
    "final_features = [1, 64, 512, 512]\n",
    "\n",
    "# 2. OutConv maps to class space\n",
    "outc = OutConv(64, 3)\n",
    "logits = outc(final_features)  # [1, 3, 512, 512]\n",
    "\n",
    "# 3. Convert to probabilities (inference)\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "# probs[0, :, 100, 100] might be:\n",
    "# [0.05, 0.85, 0.10]  # 85% benign, 10% malignant, 5% background\n",
    "\n",
    "# 4. Get final prediction\n",
    "pred_mask = torch.argmax(probs, dim=1)\n",
    "# pred_mask[0, 100, 100] = 1  # Benign class\n",
    "\n",
    "# 5. Visualize\n",
    "# Color code: Background=Black, Benign=Green, Malignant=Red\n",
    "```\n",
    "\n",
    "### Parameter Count:\n",
    "\n",
    "```python\n",
    "# OutConv(64, 1) - Binary\n",
    "# Conv2d(64, 1, kernel_size=1)\n",
    "# Parameters: 1×1×64×1 + 1(bias) = 65 parameters\n",
    "# Minimal! (< 0.01% of total network)\n",
    "\n",
    "# OutConv(64, 21) - Multi-class (Pascal VOC)\n",
    "# Parameters: 1×1×64×21 + 21(bias) = 1,365 parameters\n",
    "# Still minimal compared to 31M total network parameters\n",
    "\n",
    "# This is intentional design:\n",
    "# - Heavy computation in encoder/decoder (feature learning)\n",
    "# - Light final layer (just class mapping)\n",
    "```\n",
    "\n",
    "### Comparison with Alternatives:\n",
    "\n",
    "```python\n",
    "# UNet Standard (1×1 Conv)\n",
    "OutConv(64, n_classes)\n",
    "# ✓ Efficient\n",
    "# ✓ Per-pixel classification\n",
    "# ✓ Standard practice\n",
    "\n",
    "# Alternative 1: 3×3 Conv\n",
    "nn.Conv2d(64, n_classes, kernel_size=3, padding=1)\n",
    "# ✗ 9× more parameters\n",
    "# ✗ Unnecessary spatial mixing at final stage\n",
    "# ~ Might capture very fine detail (rarely needed)\n",
    "\n",
    "# Alternative 2: Fully Connected\n",
    "nn.Linear(64, n_classes)  # Applied per pixel\n",
    "# ✓ Same as 1×1 Conv mathematically\n",
    "# ✗ Less common in segmentation literature\n",
    "# ✗ Harder to visualize as spatial operation\n",
    "```\n",
    "\n",
    "### Real-World Tip:\n",
    "\n",
    "**When n_classes is large** (e.g., 150 classes in ADE20K dataset):\n",
    "```python\n",
    "# Option 1: Direct\n",
    "OutConv(64, 150)  # 64×150 = 9,600 parameters\n",
    "\n",
    "# Option 2: Bottleneck (if needed)\n",
    "nn.Sequential(\n",
    "    nn.Conv2d(64, 256, 1),  # Expand\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(256, 150, 1)  # Project to classes\n",
    ")\n",
    "# Sometimes better for very large n_classes\n",
    "# Adds intermediate representation capacity\n",
    "```\n",
    "\n",
    "For standard UNet, simple 1×1 is sufficient and preferred!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d72bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb42be1",
   "metadata": {},
   "source": [
    "## Complete UNet Architecture\n",
    "\n",
    "### Purpose:\n",
    "Assembles all building blocks into the complete UNet model for semantic segmentation.\n",
    "\n",
    "### Architecture Overview:\n",
    "\n",
    "```\n",
    "                          Input Image (e.g., 572×572×3)\n",
    "                                    ↓\n",
    "        ┌──────────────── DoubleConv (inc) ────────────────┐\n",
    "        │                 64 channels                       │ Skip\n",
    "        │                      ↓                            │ Connection\n",
    "        │              Down1 (MaxPool + Conv)               │ (x1)\n",
    "        │                 128 channels                      │\n",
    "        │                      ↓                            ↓\n",
    "        │              Down2 (MaxPool + Conv)          Up4 (Upsample + Concat + Conv)\n",
    "        │                 256 channels                 64 channels\n",
    "        │                      ↓                            ↑\n",
    "        │              Down3 (MaxPool + Conv)          Up3 (Upsample + Concat + Conv)\n",
    "        │                 512 channels                 128 channels\n",
    "        │                      ↓                            ↑\n",
    "        └──────→          Down4 (MaxPool + Conv)      Up2 (Upsample + Concat + Conv)\n",
    "                          1024/512 channels            256 channels\n",
    "                     (Bottleneck - deepest point)           ↑\n",
    "                                ↓                      Up1 (Upsample + Concat + Conv)\n",
    "                          (features x5)                512 channels\n",
    "                                                            ↑\n",
    "                                                       OutConv (1×1)\n",
    "                                                            ↓\n",
    "                                                    Output Segmentation Map\n",
    "```\n",
    "\n",
    "### Main Parameters Explanation:\n",
    "\n",
    "#### 1. **n_channels** (int)\n",
    "- **What**: Number of channels in input image\n",
    "- **Why**: Defines input format that network expects\n",
    "- **Scenarios**:\n",
    "  ```python\n",
    "  n_channels=1:  # Grayscale\n",
    "  # - Medical X-rays\n",
    "  # - Satellite SAR imagery  \n",
    "  # - Depth maps\n",
    "  # Input shape: [B, 1, H, W]\n",
    "  \n",
    "  n_channels=3:  # RGB Color\n",
    "  # - Natural images\n",
    "  # - Medical histology (stained tissue)\n",
    "  # - Aerial photography\n",
    "  # Input shape: [B, 3, H, W]\n",
    "  \n",
    "  n_channels=4:  # RGBA or Multi-spectral\n",
    "  # - Images with alpha channel\n",
    "  # - Satellite (RGB + NIR)\n",
    "  # Input shape: [B, 4, H, W]\n",
    "  \n",
    "  n_channels=7:  # Satellite Multispectral\n",
    "  # - Sentinel-2 satellite bands\n",
    "  # - Each channel captures different wavelength\n",
    "  # Input shape: [B, 7, H, W]\n",
    "  ```\n",
    "\n",
    "- **Impact on Model**:\n",
    "  ```python\n",
    "  # First convolution parameters:\n",
    "  n_channels=1: Conv2d(1, 64)   → 3×3×1×64 = 576 params\n",
    "  n_channels=3: Conv2d(3, 64)   → 3×3×3×64 = 1,728 params\n",
    "  n_channels=7: Conv2d(7, 64)   → 3×3×7×64 = 4,032 params\n",
    "  # Rest of network unchanged!\n",
    "  ```\n",
    "\n",
    "#### 2. **n_classes** (int)\n",
    "- **What**: Number of segmentation output classes\n",
    "- **Why**: Determines what the model predicts\n",
    "- **Scenarios**:\n",
    "\n",
    "  **n_classes=1: Binary Segmentation**\n",
    "  ```python\n",
    "  # Single foreground class\n",
    "  UNet(n_channels=3, n_classes=1)\n",
    "  \n",
    "  # Examples:\n",
    "  # Medical: Tumor present or not\n",
    "  # Satellite: Building or no building\n",
    "  # Industrial: Defect or no defect\n",
    "  \n",
    "  # Output: [B, 1, H, W]\n",
    "  # Loss: BCEWithLogitsLoss\n",
    "  # Inference: sigmoid → threshold at 0.5\n",
    "  ```\n",
    "\n",
    "  **n_classes=2: Binary (Alternative Format)**\n",
    "  ```python\n",
    "  # Two classes: Background & Foreground\n",
    "  UNet(n_channels=3, n_classes=2)\n",
    "  \n",
    "  # Output: [B, 2, H, W]\n",
    "  # Channel 0: Background probability\n",
    "  # Channel 1: Foreground probability\n",
    "  # Loss: CrossEntropyLoss\n",
    "  # Inference: softmax → argmax\n",
    "  \n",
    "  # Note: n_classes=1 vs n_classes=2 give same result\n",
    "  # but n_classes=1 uses less memory\n",
    "  ```\n",
    "\n",
    "  **n_classes=3-10: Few Classes**\n",
    "  ```python\n",
    "  UNet(n_channels=3, n_classes=5)\n",
    "  \n",
    "  # Example: Autonomous driving (simplified)\n",
    "  # Class 0: Background\n",
    "  # Class 1: Road\n",
    "  # Class 2: Vehicle\n",
    "  # Class 3: Pedestrian\n",
    "  # Class 4: Traffic sign\n",
    "  \n",
    "  # Output: [B, 5, H, W]\n",
    "  # Each pixel gets 5 logits\n",
    "  ```\n",
    "\n",
    "  **n_classes=20+: Many Classes**\n",
    "  ```python\n",
    "  UNet(n_channels=3, n_classes=21)\n",
    "  \n",
    "  # Example: Pascal VOC dataset\n",
    "  # 20 object classes + 1 background\n",
    "  # Classes: Person, car, dog, chair, etc.\n",
    "  \n",
    "  # More challenging:\n",
    "  UNet(n_channels=3, n_classes=150)\n",
    "  # ADE20K dataset - scene parsing\n",
    "  # Wall, building, sky, floor, tree, etc.\n",
    "  ```\n",
    "\n",
    "- **Memory Impact**:\n",
    "  ```python\n",
    "  # For 512×512 output:\n",
    "  n_classes=1:   512×512×1 = 262,144 values (1 MB)\n",
    "  n_classes=10:  512×512×10 = 2,621,440 values (10 MB)\n",
    "  n_classes=150: 512×512×150 = 39,321,600 values (150 MB)\n",
    "  # Per batch per forward pass!\n",
    "  ```\n",
    "\n",
    "#### 3. **bilinear** (bool, default=False)\n",
    "- **What**: Upsampling method for ALL Up blocks\n",
    "- **Why**: Global architecture decision affecting entire decoder\n",
    "\n",
    "##### **bilinear=False (Default - Learned Upsampling)**:\n",
    "```python\n",
    "model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "\n",
    "# Architecture changes:\n",
    "# Bottleneck: Uses 1024 channels\n",
    "self.down4 = Down(512, 1024)  # factor = 1\n",
    "\n",
    "# Up blocks use ConvTranspose2d:\n",
    "self.up1 = Up(1024, 512, bilinear=False)\n",
    "# Up.up = ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "\n",
    "# Total parameters: ~31M\n",
    "# GPU memory: ~8-12 GB for training (batch size 4-8)\n",
    "# Training time: ~100% (baseline)\n",
    "# Accuracy: Best possible (learnable upsampling)\n",
    "```\n",
    "\n",
    "##### **bilinear=True (Memory-Efficient)**:\n",
    "```python\n",
    "model = UNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "\n",
    "# Architecture changes:\n",
    "# Bottleneck: Uses 512 channels only!\n",
    "self.down4 = Down(512, 512)  # factor = 2\n",
    "\n",
    "# Up blocks use interpolation:\n",
    "self.up1 = Up(1024, 256, bilinear=True)\n",
    "# Up.up = Upsample(scale_factor=2, mode='bilinear')\n",
    "\n",
    "# Total parameters: ~24M (23% fewer!)\n",
    "# GPU memory: ~5-8 GB for training (same batch size)\n",
    "# Training time: ~80% (20% faster)\n",
    "# Accuracy: ~1-2% lower mIoU typically\n",
    "```\n",
    "\n",
    "##### **When to Choose**:\n",
    "\n",
    "| Factor | bilinear=False | bilinear=True |\n",
    "|--------|----------------|---------------|\n",
    "| GPU memory available | > 8 GB | < 8 GB |\n",
    "| Training time priority | Low | High |\n",
    "| Inference speed priority | Low | High |\n",
    "| Accuracy priority | Critical | Nice-to-have |\n",
    "| Dataset size | Large | Small |\n",
    "| Image complexity | High | Moderate |\n",
    "| Production deployment | Server | Edge device |\n",
    "\n",
    "**Hybrid Approach** (Advanced):\n",
    "```python\n",
    "# Can customize per layer:\n",
    "self.up1 = Up(1024, 512, bilinear=False)  # Learn first upsampling\n",
    "self.up2 = Up(512, 256, bilinear=True)    # Interpolate middle\n",
    "self.up3 = Up(256, 128, bilinear=True)    # Interpolate middle  \n",
    "self.up4 = Up(128, 64, bilinear=False)    # Learn final upsampling\n",
    "# Balance quality and efficiency!\n",
    "```\n",
    "\n",
    "### Channel Progression Detailed:\n",
    "\n",
    "#### **Encoder (Contracting Path)**:\n",
    "```python\n",
    "# Progressive downsampling with channel increase\n",
    "# Spatial size ÷2 at each step, channels ×2\n",
    "\n",
    "# Starting point:\n",
    "inc: [B, n_channels, H, W] → [B, 64, H, W]\n",
    "# Initial feature extraction\n",
    "# Example: [2, 3, 572, 572] → [2, 64, 572, 572]\n",
    "\n",
    "# Level 1:\n",
    "down1: [B, 64, H, W] → [B, 128, H/2, W/2]\n",
    "# MaxPool: 572 → 286\n",
    "# Example: [2, 64, 572, 572] → [2, 128, 286, 286]\n",
    "\n",
    "# Level 2:\n",
    "down2: [B, 128, H/2, W/2] → [B, 256, H/4, W/4]\n",
    "# MaxPool: 286 → 143\n",
    "# Example: [2, 128, 286, 286] → [2, 256, 143, 143]\n",
    "\n",
    "# Level 3:\n",
    "down3: [B, 256, H/4, W/4] → [B, 512, H/8, W/8]\n",
    "# MaxPool: 143 → 71\n",
    "# Example: [2, 256, 143, 143] → [2, 512, 71, 71]\n",
    "\n",
    "# Level 4 (Bottleneck):\n",
    "# bilinear=False:\n",
    "down4: [B, 512, H/8, W/8] → [B, 1024, H/16, W/16]\n",
    "# Example: [2, 512, 71, 71] → [2, 1024, 35, 35]\n",
    "\n",
    "# bilinear=True:\n",
    "down4: [B, 512, H/8, W/8] → [B, 512, H/16, W/16]\n",
    "# Example: [2, 512, 71, 71] → [2, 512, 35, 35]\n",
    "# Half the channels! (factor=2)\n",
    "```\n",
    "\n",
    "#### **Decoder (Expanding Path)**:\n",
    "```python\n",
    "# Progressive upsampling with channel decrease\n",
    "# Spatial size ×2 at each step, channels ÷2\n",
    "\n",
    "# bilinear=False:\n",
    "up1: [B, 1024, H/16, W/16] + skip[B, 512, H/8, W/8]\n",
    "     → [B, 512, H/8, W/8]\n",
    "# [2, 1024, 35, 35] + [2, 512, 71, 71] → [2, 512, 71, 71]\n",
    "\n",
    "up2: [B, 512, H/8, W/8] + skip[B, 256, H/4, W/4]\n",
    "     → [B, 256, H/4, W/4]\n",
    "# [2, 512, 71, 71] + [2, 256, 143, 143] → [2, 256, 143, 143]\n",
    "\n",
    "up3: [B, 256, H/4, W/4] + skip[B, 128, H/2, W/2]\n",
    "     → [B, 128, H/2, W/2]\n",
    "# [2, 256, 143, 143] + [2, 128, 286, 286] → [2, 128, 286, 286]\n",
    "\n",
    "up4: [B, 128, H/2, W/2] + skip[B, 64, H, W]\n",
    "     → [B, 64, H, W]\n",
    "# [2, 128, 286, 286] + [2, 64, 572, 572] → [2, 64, 572, 572]\n",
    "\n",
    "# Final:\n",
    "outc: [B, 64, H, W] → [B, n_classes, H, W]\n",
    "# [2, 64, 572, 572] → [2, 1, 572, 572]\n",
    "```\n",
    "\n",
    "### Forward Pass Explanation:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    # x: Input image [B, n_channels, H, W]\n",
    "    \n",
    "    # ENCODER (contracting path)\n",
    "    x1 = self.inc(x)      # [B, 64, H, W] - Initial features\n",
    "    x2 = self.down1(x1)   # [B, 128, H/2, W/2] - Low-level features\n",
    "    x3 = self.down2(x2)   # [B, 256, H/4, W/4] - Mid-level features\n",
    "    x4 = self.down3(x3)   # [B, 512, H/8, W/8] - High-level features\n",
    "    x5 = self.down4(x4)   # [B, 1024, H/16, W/16] - Semantic features\n",
    "    \n",
    "    # x1, x2, x3, x4 saved for skip connections!\n",
    "    \n",
    "    # DECODER (expanding path with skip connections)\n",
    "    x = self.up1(x5, x4)  # Takes x5 (from below) + x4 (skip) → [B, 512, H/8, W/8]\n",
    "    x = self.up2(x, x3)   # Takes x (from below) + x3 (skip) → [B, 256, H/4, W/4]\n",
    "    x = self.up3(x, x2)   # Takes x (from below) + x2 (skip) → [B, 128, H/2, W/2]\n",
    "    x = self.up4(x, x1)   # Takes x (from below) + x1 (skip) → [B, 64, H, W]\n",
    "    \n",
    "    # OUTPUT\n",
    "    logits = self.outc(x) # [B, n_classes, H, W] - Final segmentation\n",
    "    return logits\n",
    "```\n",
    "\n",
    "### Skip Connection Flow:\n",
    "```\n",
    "Encoder saves:        Decoder uses:\n",
    "x1 (64 ch, full res)  ────────────→ up4\n",
    "x2 (128 ch, 1/2 res)  ──────────→ up3\n",
    "x3 (256 ch, 1/4 res)  ────────→ up2\n",
    "x4 (512 ch, 1/8 res)  ──────→ up1\n",
    "\n",
    "Purpose:\n",
    "- Combine WHERE (high-res encoder) with WHAT (low-res decoder)\n",
    "- Preserve fine details lost during downsampling\n",
    "- Enable precise localization\n",
    "```\n",
    "\n",
    "### use_checkpointing() Method:\n",
    "\n",
    "**What it does**:\n",
    "```python\n",
    "model.use_checkpointing()\n",
    "# Wraps each major block with gradient checkpointing\n",
    "```\n",
    "\n",
    "**How it works**:\n",
    "- **Normal**: Stores all intermediate activations (memory-heavy)\n",
    "  ```\n",
    "  Forward: Compute + Store all activations\n",
    "  Backward: Use stored activations for gradients\n",
    "  Memory: High, Speed: Fast\n",
    "  ```\n",
    "\n",
    "- **With Checkpointing**: Stores only some activations\n",
    "  ```\n",
    "  Forward: Compute + Store only checkpoints\n",
    "  Backward: Recompute activations on-the-fly\n",
    "  Memory: Low (~50% reduction), Speed: Slower (~20% slower)\n",
    "  ```\n",
    "\n",
    "**When to use**:\n",
    "```python\n",
    "# Scenario 1: Large images\n",
    "input_size = (3, 1024, 1024)  # 1K resolution\n",
    "model.use_checkpointing()  # Reduces memory by ~5-8 GB\n",
    "\n",
    "# Scenario 2: Limited GPU\n",
    "# GPU: 8GB (e.g., RTX 2080)\n",
    "# Without checkpointing: Batch size = 2\n",
    "# With checkpointing: Batch size = 4 (2× more!)\n",
    "\n",
    "# Scenario 3: Huge batch sizes\n",
    "# For better batch norm statistics\n",
    "# Trade 20% speed for 100% more batch size\n",
    "```\n",
    "\n",
    "**Implementation detail**:\n",
    "```python\n",
    "self.inc = torch.utils.checkpoint(self.inc)\n",
    "# This is WRONG in actual PyTorch!\n",
    "# Should be: torch.utils.checkpoint.checkpoint()\n",
    "\n",
    "# Correct usage would be:\n",
    "def forward(self, x):\n",
    "    x1 = torch.utils.checkpoint.checkpoint(self.inc, x)\n",
    "    # etc...\n",
    "```\n",
    "\n",
    "### Model Statistics:\n",
    "\n",
    "```python\n",
    "# Example: UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "\n",
    "# Total parameters: ~31M\n",
    "# Breakdown:\n",
    "# - inc: 38K\n",
    "# - down1-4: ~8M\n",
    "# - up1-4: ~23M (most parameters!)\n",
    "# - outc: ~65\n",
    "\n",
    "# Memory usage (training, batch=4, size=512×512):\n",
    "# - Model parameters: ~124 MB\n",
    "# - Activations: ~6 GB\n",
    "# - Gradients: ~124 MB\n",
    "# - Optimizer states (Adam): ~248 MB\n",
    "# - Total: ~7 GB GPU memory\n",
    "\n",
    "# Inference (batch=1):\n",
    "# - Model: ~124 MB\n",
    "# - Activations: ~1 GB\n",
    "# - Total: ~1.5 GB GPU memory\n",
    "```\n",
    "\n",
    "### Real-World Configuration Examples:\n",
    "\n",
    "```python\n",
    "# Medical Imaging (CT Scans)\n",
    "model = UNet(n_channels=1, n_classes=3, bilinear=True)\n",
    "# 1 channel (grayscale), 3 organs, memory-efficient\n",
    "# Input: [B, 1, 512, 512]\n",
    "# Output: [B, 3, 512, 512]\n",
    "\n",
    "# Satellite Imagery (Building Detection)\n",
    "model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "# RGB, binary (building/not), best accuracy\n",
    "# Input: [B, 3, 1024, 1024]\n",
    "# Output: [B, 1, 1024, 1024]\n",
    "\n",
    "# Autonomous Driving (Scene Parsing)\n",
    "model = UNet(n_channels=3, n_classes=19, bilinear=False)\n",
    "# RGB, 19 Cityscapes classes, accuracy critical\n",
    "# Input: [B, 3, 1024, 2048]\n",
    "# Output: [B, 19, 1024, 2048]\n",
    "\n",
    "# Mobile Deployment (Portrait Segmentation)\n",
    "model = UNet(n_channels=3, n_classes=1, bilinear=True)\n",
    "# RGB, binary person mask, speed critical\n",
    "# Possible: Reduce channels (32 instead of 64)\n",
    "# Input: [B, 3, 256, 256]\n",
    "# Output: [B, 1, 256, 256]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8a5858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b2d20",
   "metadata": {},
   "source": [
    "## Model Summary and Visualization\n",
    "\n",
    "### Purpose:\n",
    "Display the complete architecture with layer-by-layer parameter counts and output shapes.\n",
    "\n",
    "This will show:\n",
    "- **Layer names** and types\n",
    "- **Output shapes** at each layer\n",
    "- **Parameter counts** (trainable weights)\n",
    "- **Total parameters** in the model\n",
    "- **Model size** in memory\n",
    "\n",
    "### What to Expect:\n",
    "For input size (3, 572, 572):\n",
    "- Initial layers progressively reduce spatial dimensions\n",
    "- Channel count increases in encoder (64→128→256→512→1024)\n",
    "- Channel count decreases in decoder (1024→512→256→128→64)\n",
    "- Final output: (1, 572, 572) for single-class segmentation\n",
    "\n",
    "### Understanding the Summary:\n",
    "- **Input Shape**: (batch_size, channels, height, width)\n",
    "- **Output Shape**: Shape after each layer\n",
    "- **Params**: Number of learnable parameters in that layer\n",
    "- **Total Params**: Sum of all parameters in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e46e2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 572, 572]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 572, 572]             128\n",
      "              ReLU-3         [-1, 64, 572, 572]               0\n",
      "            Conv2d-4         [-1, 64, 572, 572]          36,864\n",
      "       BatchNorm2d-5         [-1, 64, 572, 572]             128\n",
      "              ReLU-6         [-1, 64, 572, 572]               0\n",
      "        DoubleConv-7         [-1, 64, 572, 572]               0\n",
      "         MaxPool2d-8         [-1, 64, 286, 286]               0\n",
      "            Conv2d-9        [-1, 128, 286, 286]          73,728\n",
      "      BatchNorm2d-10        [-1, 128, 286, 286]             256\n",
      "             ReLU-11        [-1, 128, 286, 286]               0\n",
      "           Conv2d-12        [-1, 128, 286, 286]         147,456\n",
      "      BatchNorm2d-13        [-1, 128, 286, 286]             256\n",
      "             ReLU-14        [-1, 128, 286, 286]               0\n",
      "       DoubleConv-15        [-1, 128, 286, 286]               0\n",
      "             Down-16        [-1, 128, 286, 286]               0\n",
      "        MaxPool2d-17        [-1, 128, 143, 143]               0\n",
      "           Conv2d-18        [-1, 256, 143, 143]         294,912\n",
      "      BatchNorm2d-19        [-1, 256, 143, 143]             512\n",
      "             ReLU-20        [-1, 256, 143, 143]               0\n",
      "           Conv2d-21        [-1, 256, 143, 143]         589,824\n",
      "      BatchNorm2d-22        [-1, 256, 143, 143]             512\n",
      "             ReLU-23        [-1, 256, 143, 143]               0\n",
      "       DoubleConv-24        [-1, 256, 143, 143]               0\n",
      "             Down-25        [-1, 256, 143, 143]               0\n",
      "        MaxPool2d-26          [-1, 256, 71, 71]               0\n",
      "           Conv2d-27          [-1, 512, 71, 71]       1,179,648\n",
      "      BatchNorm2d-28          [-1, 512, 71, 71]           1,024\n",
      "             ReLU-29          [-1, 512, 71, 71]               0\n",
      "           Conv2d-30          [-1, 512, 71, 71]       2,359,296\n",
      "      BatchNorm2d-31          [-1, 512, 71, 71]           1,024\n",
      "             ReLU-32          [-1, 512, 71, 71]               0\n",
      "       DoubleConv-33          [-1, 512, 71, 71]               0\n",
      "             Down-34          [-1, 512, 71, 71]               0\n",
      "        MaxPool2d-35          [-1, 512, 35, 35]               0\n",
      "           Conv2d-36         [-1, 1024, 35, 35]       4,718,592\n",
      "      BatchNorm2d-37         [-1, 1024, 35, 35]           2,048\n",
      "             ReLU-38         [-1, 1024, 35, 35]               0\n",
      "           Conv2d-39         [-1, 1024, 35, 35]       9,437,184\n",
      "      BatchNorm2d-40         [-1, 1024, 35, 35]           2,048\n",
      "             ReLU-41         [-1, 1024, 35, 35]               0\n",
      "       DoubleConv-42         [-1, 1024, 35, 35]               0\n",
      "             Down-43         [-1, 1024, 35, 35]               0\n",
      "  ConvTranspose2d-44          [-1, 512, 70, 70]       2,097,664\n",
      "           Conv2d-45          [-1, 512, 71, 71]       4,718,592\n",
      "      BatchNorm2d-46          [-1, 512, 71, 71]           1,024\n",
      "             ReLU-47          [-1, 512, 71, 71]               0\n",
      "           Conv2d-48          [-1, 512, 71, 71]       2,359,296\n",
      "      BatchNorm2d-49          [-1, 512, 71, 71]           1,024\n",
      "             ReLU-50          [-1, 512, 71, 71]               0\n",
      "       DoubleConv-51          [-1, 512, 71, 71]               0\n",
      "               Up-52          [-1, 512, 71, 71]               0\n",
      "  ConvTranspose2d-53        [-1, 256, 142, 142]         524,544\n",
      "           Conv2d-54        [-1, 256, 143, 143]       1,179,648\n",
      "      BatchNorm2d-55        [-1, 256, 143, 143]             512\n",
      "             ReLU-56        [-1, 256, 143, 143]               0\n",
      "           Conv2d-57        [-1, 256, 143, 143]         589,824\n",
      "      BatchNorm2d-58        [-1, 256, 143, 143]             512\n",
      "             ReLU-59        [-1, 256, 143, 143]               0\n",
      "       DoubleConv-60        [-1, 256, 143, 143]               0\n",
      "               Up-61        [-1, 256, 143, 143]               0\n",
      "  ConvTranspose2d-62        [-1, 128, 286, 286]         131,200\n",
      "           Conv2d-63        [-1, 128, 286, 286]         294,912\n",
      "      BatchNorm2d-64        [-1, 128, 286, 286]             256\n",
      "             ReLU-65        [-1, 128, 286, 286]               0\n",
      "           Conv2d-66        [-1, 128, 286, 286]         147,456\n",
      "      BatchNorm2d-67        [-1, 128, 286, 286]             256\n",
      "             ReLU-68        [-1, 128, 286, 286]               0\n",
      "       DoubleConv-69        [-1, 128, 286, 286]               0\n",
      "               Up-70        [-1, 128, 286, 286]               0\n",
      "  ConvTranspose2d-71         [-1, 64, 572, 572]          32,832\n",
      "           Conv2d-72         [-1, 64, 572, 572]          73,728\n",
      "      BatchNorm2d-73         [-1, 64, 572, 572]             128\n",
      "             ReLU-74         [-1, 64, 572, 572]               0\n",
      "           Conv2d-75         [-1, 64, 572, 572]          36,864\n",
      "      BatchNorm2d-76         [-1, 64, 572, 572]             128\n",
      "             ReLU-77         [-1, 64, 572, 572]               0\n",
      "       DoubleConv-78         [-1, 64, 572, 572]               0\n",
      "               Up-79         [-1, 64, 572, 572]               0\n",
      "           Conv2d-80          [-1, 1, 572, 572]              65\n",
      "          OutConv-81          [-1, 1, 572, 572]               0\n",
      "================================================================\n",
      "Total params: 31,037,633\n",
      "Trainable params: 31,037,633\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.74\n",
      "Forward/backward pass size (MB): 5082.78\n",
      "Params size (MB): 118.40\n",
      "Estimated Total Size (MB): 5204.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(UNet(n_channels=3, n_classes=1).to('cpu'), (3, 572, 572))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72849f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
