# Image Captioning: TensorFlow vs PyTorch Implementation

## ğŸ“Š Overview

This document compares two implementations of image captioning models:
- **TensorFlow Version** (Original): Using Keras Functional API
- **PyTorch Version** (Custom): Using PyTorch nn.Module

Both implementations use VGG16 for feature extraction and LSTM for caption generation, but differ significantly in architecture and training approaches.

---

## ğŸ—ï¸ Architecture Comparison

### **TensorFlow Version (Original Kaggle)**

```
Image Features (4096) â”€â”€â–º Dropout(0.4) â”€â”€â–º Dense(256) â”€â”€â–º ReLU
                                                          â”‚
                                                          â”‚ ADD LAYER
Caption Sequence â”€â”€â–º Embedding(vocab, 256) â”€â”€â–º Dropout(0.4) â”€â”€â–º LSTM(256) â”€â”€â”˜
        â”‚                    â”‚                                               â”‚
        â”‚                    â””â”€â”€â”€ mask_zero=True                            â”‚
        â”‚                                                                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Dense(256) â”€â–º ReLU â”€â–º Dense(vocab) â”€â–º Softmax
```

**Key Features:**
- **Merge Strategy**: Uses `add()` layer to element-wise **add** image and text vectors
- **Masking**: `mask_zero=True` in Embedding layer automatically handles padding
- **Output**: Predicts next word directly from added features
- **Dropout**: Applied early (after image input and after embedding)

---

### **PyTorch Version (Your Implementation)**

```
Image Features (4096) â”€â”€â–º Linear(256) â”€â”€â–º ReLU â”€â”€â–º img_vec (256)
                                                        â”‚
                                                        â”‚ CONCATENATE
Caption Sequence â”€â”€â–º Embedding(vocab, 256) â”€â”€â–º LSTM(256) â”€â”€â–º hidden_state â”€â”€â”˜
        â”‚                    â”‚                              (256)             â”‚
        â”‚                    â””â”€â”€â”€ padding_idx=0                               â”‚
        â”‚                                                                      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Combined (512)
                                                                      â”‚
                                                                      â–¼
                                                      Linear(512â†’256) â”€â–º ReLU
                                                                      â”‚
                                                                      â–¼
                                                              Dropout(0.4)
                                                                      â”‚
                                                                      â–¼
                                                          Linear(256â†’vocab)
```

**Key Features:**
- **Merge Strategy**: Uses `torch.cat()` to **concatenate** image and text vectors (512-dim)
- **Hidden State**: Extracts LSTM's final hidden state (not output sequence)
- **Deeper Decoder**: 3-layer decoder with intermediate ReLU and Dropout
- **Dropout**: Applied late (before final prediction layer)

---

## ğŸ” Critical Architectural Differences

### **1. Feature Fusion Method**

| Aspect | TensorFlow (ADD) | PyTorch (CONCATENATE) |
|--------|------------------|------------------------|
| **Operation** | Element-wise addition: `img + text` | Concatenation: `[img \|\| text]` |
| **Dimension** | 256 (same as inputs) | 512 (double the inputs) |
| **Information Loss** | **Higher** - averaging reduces distinct features | **Lower** - preserves both modalities separately |
| **Model Learns** | Shared representation space | Independent + joint representations |
| **Interaction** | Forces alignment at embedding level | Learns alignment through dense layers |

**Why ADD might work better:**
- Forces the model to learn a **unified embedding space** where image and text features are directly comparable
- Simpler fusion = fewer parameters = **less overfitting** on small datasets (Flickr8k = 8,000 images)
- Mimics attention mechanism: model learns to "align" features implicitly

**Why CONCATENATE might struggle:**
- Doubles feature dimensionality â†’ more parameters â†’ **higher risk of overfitting**
- Model must learn fusion from scratch through decoder layers
- No explicit constraint to align image/text representations

---

### **2. Masking Strategy**

| Aspect | TensorFlow | PyTorch |
|--------|------------|---------|
| **Implementation** | `mask_zero=True` in Embedding + LSTM | `padding_idx=0` in Embedding only |
| **Automatic Masking** | âœ… LSTM automatically ignores masked positions | âŒ LSTM processes padding tokens |
| **Hidden State** | Clean (no padding contamination) | Potentially contaminated by padding |
| **Workaround** | None needed | Uses hidden state (naturally ignores padding) |

**Impact:**
- **TensorFlow**: Keras LSTM respects masking throughout sequence processing
- **PyTorch**: LSTM doesn't support masking directly, requiring architectural workarounds

---

### **3. Decoder Depth**

| Aspect | TensorFlow | PyTorch |
|--------|------------|---------|
| **Layers** | 2 layers (256 â†’ vocab) | 3 layers (512 â†’ 256 â†’ vocab) |
| **Dropout Position** | After image input & embedding | Before final prediction |
| **Complexity** | Lower | Higher |

**Why TensorFlow's simpler decoder works better:**
- Flickr8k is a **small dataset** (8,000 images, ~40K captions)
- Simpler model = better generalization on small data
- PyTorch's deeper decoder may **memorize training data** instead of learning generalizable patterns

---

### **4. Training Configuration**

| Aspect | TensorFlow | PyTorch |
|--------|------------|---------|
| **Epochs** | 20 | 100 (increased from 20) |
| **Batch Size** | 32 | 64 |
| **Steps/Epoch** | ~223 (7,200 Ã· 32) | ~100 (6,400 Ã· 64) |
| **Samples/Batch** | More iterations, smaller batches | Fewer iterations, larger batches |

**Impact:**
- Smaller batches (32) â†’ **noisier gradients** â†’ better exploration â†’ less overfitting
- Larger batches (64) â†’ smoother gradients â†’ faster convergence â†’ risk of overfitting

---

## ğŸ“ˆ Expected Performance Analysis

### **TensorFlow Version Advantages**

âœ… **Better Feature Fusion**: ADD layer forces unified embedding space  
âœ… **Automatic Masking**: Keras handles padding seamlessly  
âœ… **Simpler Architecture**: 2-layer decoder prevents overfitting on small dataset  
âœ… **Smaller Batches**: Better generalization through gradient noise  
âœ… **Proven Design**: Based on successful research architectures  

**Expected BLEU Scores**: 40-55% BLEU-1, 15-25% BLEU-4

---

### **PyTorch Version Challenges**

âš ï¸ **Feature Fusion**: Concatenation requires more data to learn alignment  
âš ï¸ **No Automatic Masking**: Requires workaround (hidden state extraction)  
âš ï¸ **Deeper Decoder**: 3-layer decoder overfits on 8,000 images  
âš ï¸ **Larger Batches**: Less gradient noise â†’ worse generalization  
âš ï¸ **Custom Architecture**: Deviates from proven designs  

**Current BLEU Scores** (100 epochs, mid-training): 26.79% BLEU-1, 2.45% BLEU-4  
**Expected Final**: 30-40% BLEU-1, 8-15% BLEU-4

---

## ğŸ”§ Why TensorFlow Works Better: Root Causes

### **1. ADD vs CONCATENATE: The Core Issue**

The TensorFlow model uses **element-wise addition** which:
- Forces the model to learn a **shared semantic space**
- Acts as an implicit regularizer (fewer parameters)
- Similar to how attention mechanisms work (alignment through vector operations)

**Research Evidence**: 
- "Show, Attend and Tell" (Xu et al., 2015) uses additive attention
- "Show and Tell" (Vinyals et al., 2015) uses similar fusion strategies
- Addition has strong theoretical grounding in multimodal learning

Your PyTorch concatenation approach:
- Creates 512-dim representation that must be learned from scratch
- No inherent constraint for alignment
- Requires **more training data** to learn proper fusion
- Works well with large datasets (MSCOCO: 80K images) but struggles with Flickr8k (8K images)

---

### **2. Overfitting on Small Dataset**

**Dataset Size Reality**:
- Flickr8k: 8,000 images, ~40,000 captions
- Your PyTorch model: ~8M parameters (estimate)
- TensorFlow model: ~5M parameters (estimate)

**Overfitting Indicators**:
- Training loss decreases (7.24 â†’ 3.31) âœ…
- Validation BLEU scores remain low (26.79%) âš ï¸
- **Classic overfitting pattern**

**Why TensorFlow resists overfitting:**
1. **Simpler fusion** (ADD vs CONCAT) = fewer parameters
2. **Shallower decoder** (2 vs 3 layers)
3. **Early dropout** (regularizes feature extraction, not just prediction)
4. **Smaller batches** (32 vs 64) = noisier gradients = better exploration

---

### **3. Masking and Padding Handling**

**TensorFlow's Advantage**:
```python
Embedding(vocab_size, 256, mask_zero=True)  # Automatically masks padding
# LSTM receives masked input â†’ ignores padding positions
# No manual intervention needed
```

**PyTorch's Challenge**:
```python
embedding = nn.Embedding(vocab_size, 256, padding_idx=0)  # Only embeds as zeros
# LSTM still processes padding tokens (no automatic masking)
# Workaround: use hidden state (naturally summarizes non-padding content)
```

**Impact**:
- TensorFlow: Clean, uncontaminated representations throughout
- PyTorch: Relies on workaround that may not be as effective

---

## ğŸ’¡ Recommendations to Improve PyTorch Version

### **Option 1: Match TensorFlow Architecture** (Recommended)

```python
class ImageCaptionModel(nn.Module):
    def __init__(self, vocab_size, pad_idx=0):
        super().__init__()
        
        # Image encoder
        self.img_fc = nn.Sequential(
            nn.Dropout(0.4),  # Early dropout like TensorFlow
            nn.Linear(4096, 256),
            nn.ReLU()
        )
        
        # Text encoder
        self.embedding = nn.Embedding(vocab_size, 256, padding_idx=pad_idx)
        self.dropout = nn.Dropout(0.4)
        self.lstm = nn.LSTM(256, 256, batch_first=True)
        
        # Decoder - SIMPLER (match TensorFlow)
        self.decoder = nn.Sequential(
            nn.Linear(256, 256),  # NOT 512 â†’ only 256 after ADD
            nn.ReLU(),
            nn.Linear(256, vocab_size)
        )
    
    def forward(self, img_features, captions):
        # Image path
        img_vec = self.img_fc(img_features)  # (B, 256)
        
        # Text path
        text_embed = self.embedding(captions)
        text_embed = self.dropout(text_embed)
        _, (hidden, _) = self.lstm(text_embed)
        text_vec = hidden.squeeze(0)  # (B, 256)
        
        # FUSION: ADD instead of CONCATENATE
        combined = img_vec + text_vec  # Element-wise addition (256-dim)
        
        # Decode
        output = self.decoder(combined)
        return output
```

**Changes:**
1. âœ… Replace CONCATENATE with ADD
2. âœ… Reduce decoder from 512â†’256â†’vocab to 256â†’256â†’vocab
3. âœ… Move dropout earlier (after embedding, before LSTM)
4. âœ… Remove late-stage dropout (simpler regularization)

---

### **Option 2: Reduce Batch Size**

```python
batch_size = 32  # Down from 64
# More gradient noise = better generalization on small datasets
```

---

### **Option 3: Add Weight Decay**

```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
# L2 regularization to prevent overfitting
```

---

### **Option 4: Implement Keras-Style Masking**

```python
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

# In forward():
lengths = (captions != pad_idx).sum(dim=1)  # Get actual lengths
text_embed = self.embedding(captions)
packed = pack_padded_sequence(text_embed, lengths.cpu(), batch_first=True, enforce_sorted=False)
_, (hidden, _) = self.lstm(packed)
# Now hidden state is truly padding-free
```

---

## ğŸ“Š Summary Table

| Feature | TensorFlow (Original) | PyTorch (Yours) | Winner |
|---------|----------------------|-----------------|--------|
| **Fusion Method** | ADD (256-dim) | CONCATENATE (512-dim) | ğŸ† TensorFlow |
| **Decoder Depth** | 2 layers | 3 layers | ğŸ† TensorFlow |
| **Masking** | Automatic (mask_zero) | Manual (hidden state) | ğŸ† TensorFlow |
| **Batch Size** | 32 | 64 | ğŸ† TensorFlow |
| **Dropout Position** | Early (regularizes features) | Late (only predictions) | ğŸ† TensorFlow |
| **Parameters** | ~5M | ~8M | ğŸ† TensorFlow |
| **Overfitting Risk** | Lower | Higher | ğŸ† TensorFlow |
| **Expected BLEU-1** | 40-55% | 30-40% | ğŸ† TensorFlow |

---

## ğŸ¯ Conclusion

The **TensorFlow version works better** primarily because:

1. **ADD fusion** creates a unified embedding space (vs CONCATENATE requiring learned fusion)
2. **Simpler architecture** prevents overfitting on small dataset (8K images)
3. **Automatic masking** ensures clean representations throughout
4. **Proven design** based on successful research (Show and Tell, Show Attend and Tell)
5. **Better regularization** through early dropout and smaller batches

Your PyTorch implementation is technically correct but architecturally over-engineered for the dataset size. The concatenation-based fusion and deeper decoder require **more training data** (e.g., MSCOCO with 80K images) to reach their full potential.

---

## ğŸš€ Next Steps

**To match TensorFlow performance:**
1. Implement Option 1 (ADD fusion + simpler decoder)
2. Reduce batch size to 32
3. Add weight decay regularization
4. Consider reducing epochs back to 20-30 (overfitting after 40+)

**Expected improvement**: 30-40% â†’ 45-55% BLEU-1 after architecture changes.

---

## ğŸ“š References

- Vinyals et al. (2015) - "Show and Tell: A Neural Image Caption Generator"
- Xu et al. (2015) - "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"
- Keras Masking Documentation: https://keras.io/api/layers/core_layers/masking/
- PyTorch Padding & Masking: https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html
