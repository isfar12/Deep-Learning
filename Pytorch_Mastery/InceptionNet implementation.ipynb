{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74b2ca7",
   "metadata": {},
   "source": [
    "# InceptionNet (GoogLeNet) Implementation\n",
    "\n",
    "## Understanding the Innovation: \"Going Deeper with Convolutions\"\n",
    "\n",
    "InceptionNet, also known as GoogLeNet, won the ImageNet 2014 competition. While ResNet solved the depth problem with skip connections, Inception solved it differently: **going wider instead of just deeper**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Silly Example: The Detective Team\n",
    "\n",
    "**Imagine you're trying to identify who ate the cookies:**\n",
    "\n",
    "### Traditional CNN (Single Kernel Size):\n",
    "```\n",
    "You hire ONE detective with ONE magnifying glass size\n",
    "```\n",
    "\n",
    "**Detective:** \"I can only see clues at 3√ó3 inch areas. I found crumbs!\"  \n",
    "**Problem:** What if the important clue was a large footprint? Or a tiny fingerprint? ü§î  \n",
    "**Result:** üç™‚ùå Missed evidence!\n",
    "\n",
    "---\n",
    "\n",
    "### InceptionNet (Multiple Kernel Sizes):\n",
    "```\n",
    "You hire FOUR detectives working in parallel:\n",
    "1. Detective with Macro Lens (1√ó1) ‚Üí Sees tiny details up close\n",
    "2. Detective with Normal Lens (3√ó3) ‚Üí Sees regular clues  \n",
    "3. Detective with Wide Lens (5√ó5) ‚Üí Sees big patterns\n",
    "4. Detective with Evidence Bag (pooling) ‚Üí Keeps important stuff\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- All four detectives examine the SAME crime scene simultaneously\n",
    "- Detective 1: \"Found tiny fingerprint!\"\n",
    "- Detective 2: \"Found crumb trail!\"\n",
    "- Detective 3: \"Found large footprint!\"\n",
    "- Detective 4: \"Bagged all important evidence!\"\n",
    "- They combine all findings ‚Üí Complete picture! üïµÔ∏è\n",
    "\n",
    "**Result:** üç™‚úÖ Caught the cookie thief!\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Real Lesson\n",
    "\n",
    "**Traditional CNN:** Choose one filter size (3√ó3 or 5√ó5). Hope it's right!\n",
    "\n",
    "**InceptionNet:** Use ALL sizes in parallel! Let the network decide what's important! üéØ\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Problem\n",
    "\n",
    "Traditional CNNs face a dilemma:\n",
    "- **Large kernels (5√ó5, 7√ó7)**: Capture wide context but are computationally expensive\n",
    "- **Small kernels (3√ó3)**: Efficient but have limited receptive field\n",
    "- **Which kernel size should we use?** ü§î\n",
    "\n",
    "### The Inception Solution: \"Why choose? Use them all!\"\n",
    "\n",
    "Instead of choosing one kernel size, **use multiple sizes in parallel** and let the network learn which is most useful!\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/inception_net.png\" alt=\"Full ResNet Architecture\" width=\"1500\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "```\n",
    "Input\n",
    "  ‚îú‚îÄ‚Üí 1√ó1 conv ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îú‚îÄ‚Üí 1√ó1 ‚Üí 3√ó3 conv ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "  ‚îú‚îÄ‚Üí 1√ó1 ‚Üí 5√ó5 conv ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§  ‚Üí Concatenate ‚Üí Output\n",
    "  ‚îî‚îÄ‚Üí 3√ó3 pool ‚Üí 1√ó1 conv ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "1. **Multi-scale feature extraction**: Different kernel sizes capture different levels of detail\n",
    "2. **Dimensionality reduction**: 1√ó1 convolutions reduce computational cost\n",
    "3. **Parallel processing**: All paths process simultaneously (efficient on GPUs)\n",
    "4. **Network in Network**: Using 1√ó1 convs as \"mini neural networks\" between layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a7c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c04a9a",
   "metadata": {},
   "source": [
    "## Building Block: Conv Block\n",
    "\n",
    "Before building the Inception module, we need a reusable convolutional block that combines Conv ‚Üí ReLU ‚Üí BatchNorm.\n",
    "\n",
    "### Why This Pattern?\n",
    "\n",
    "**Standard order in modern CNNs:**\n",
    "```python\n",
    "Conv2d ‚Üí ReLU ‚Üí BatchNorm\n",
    "```\n",
    "\n",
    "**What each does:**\n",
    "1. **Conv2d**: Applies convolutional filters to extract features\n",
    "2. **ReLU**: Adds non-linearity (allows learning complex patterns)\n",
    "3. **BatchNorm**: Normalizes activations for stable training\n",
    "\n",
    "**Why bundle them together?**\n",
    "- These three operations almost always appear together\n",
    "- Reduces code duplication\n",
    "- Makes architecture easier to read\n",
    "- Ensures consistent ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb997f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f0d2f",
   "metadata": {},
   "source": [
    "### Understanding the Conv Block Code\n",
    "\n",
    "```python\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `in_channels`: Number of input feature maps (e.g., 64)\n",
    "- `out_channels`: Number of output feature maps (e.g., 192)\n",
    "- `**kwargs`: Flexible parameters for Conv2d (kernel_size, stride, padding, etc.)\n",
    "\n",
    "**Why `**kwargs`?**\n",
    "- Allows passing different convolution configurations without changing the class\n",
    "- Examples: `kernel_size=3, padding=1` or `kernel_size=7, stride=2, padding=3`\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = self.conv(x)      # Apply convolution\n",
    "    x = self.relu(x)      # Add non-linearity\n",
    "    x = self.batchnorm(x) # Normalize\n",
    "    return x\n",
    "```\n",
    "\n",
    "**The flow:**\n",
    "```\n",
    "Input [B, in_channels, H, W]\n",
    "   ‚Üì Conv2d\n",
    "[B, out_channels, H', W']\n",
    "   ‚Üì ReLU (non-linearity)\n",
    "[B, out_channels, H', W']\n",
    "   ‚Üì BatchNorm (normalization)\n",
    "[B, out_channels, H', W']\n",
    "```\n",
    "\n",
    "This simple block will be reused throughout the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9b4e8",
   "metadata": {},
   "source": [
    "## Understanding the Inception Module Architecture\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/inception_module.png\" alt=\"InceptioModule\" width=\"600\">\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "### üçï Why Concatenation? The Pizza Toppings Analogy!\n",
    "\n",
    "**ResNet (Addition):**\n",
    "```\n",
    "Pizza base + [pepperoni OR mushrooms OR peppers]\n",
    "You blend all toppings into one mixed flavor\n",
    "```\n",
    "\n",
    "**InceptionNet (Concatenation):**\n",
    "```\n",
    "Pizza base + [pepperoni section | mushroom section | pepper section | olive section]\n",
    "Each topping stays separate, you taste each one distinctly!\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Addition**: Mixes everything together ‚Üí lose individual information\n",
    "- **Concatenation**: Keeps everything separate ‚Üí next layer decides what's important!\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Problem Inception Solves\n",
    "\n",
    "**The Dilemma:** Large kernels (5√ó5, 7√ó7) capture more context but are computationally expensive.\n",
    "\n",
    "**The Solution:** Use **dimensionality reduction** with 1√ó1 convolutions before larger kernels.\n",
    "\n",
    "---\n",
    "\n",
    "### The Four Parallel Paths\n",
    "\n",
    "**Path 1: Direct 1√ó1 Convolution**\n",
    "- Captures point-wise channel relationships\n",
    "- Provides a lightweight baseline path\n",
    "- üîé Like checking fingerprints (tiny details)\n",
    "\n",
    "**Path 2: 1√ó1 ‚Üí 3√ó3 Convolution**\n",
    "- 1√ó1 reduces channels (e.g., 192 ‚Üí 96)\n",
    "- 3√ó3 captures local spatial patterns\n",
    "- **Efficiency**: 42% fewer parameters than direct 3√ó3\n",
    "- üîç Like checking footprints (normal clues)\n",
    "\n",
    "**Path 3: 1√ó1 ‚Üí 5√ó5 Convolution**\n",
    "- 1√ó1 reduces channels even more (e.g., 192 ‚Üí 16)\n",
    "- 5√ó5 captures larger spatial context\n",
    "- **Efficiency**: 90% fewer parameters than direct 5√ó5\n",
    "- üî≠ Like aerial view (big picture)\n",
    "\n",
    "**Path 4: 3√ó3 MaxPool ‚Üí 1√ó1 Convolution**\n",
    "- Pooling preserves important features\n",
    "- 1√ó1 adjusts channel count for concatenation\n",
    "- Adds robustness to translations\n",
    "- üì¶ Like evidence bag (keeps best stuff)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Concatenation?\n",
    "\n",
    "```python\n",
    "outputs = [branch1_out, branch2_out, branch3_out, branch4_out]\n",
    "return torch.cat(outputs, 1)  # Concatenate along channel dimension\n",
    "```\n",
    "\n",
    "**Concatenation vs Addition:**\n",
    "- **Addition (ResNet style)**: `output = F(x) + x` ‚Üí Mixes features\n",
    "- **Concatenation (Inception style)**: `output = [F1(x), F2(x), F3(x), F4(x)]` ‚Üí Preserves all\n",
    "\n",
    "**Why Concatenation is Better Here:**\n",
    "- Keeps all multi-scale features separate\n",
    "- Next layer decides which scale is important\n",
    "- Network learns feature importance through weights\n",
    "\n",
    "**Like a buffet**: Keep all dishes separate so people can choose what they want! üçΩÔ∏è\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Parameter Calculation\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/inception_block_parameters.png\" alt=\"Inception Parameters\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "**Input:** 32√ó32√ó256\n",
    "\n",
    "**Branch Outputs:**\n",
    "- 1√ó1 path: 64 channels (üîé tiny details)\n",
    "- 3√ó3 path: 128 channels (üîç normal clues)\n",
    "- 5√ó5 path: 32 channels (üî≠ big picture)\n",
    "- Pool path: 32 channels (üì¶ best evidence)\n",
    "\n",
    "**Total Output:** 32√ó32√ó(64+128+32+32) = 32√ó32√ó256\n",
    "\n",
    "**Result:** Multi-scale features (1√ó1, 3√ó3, 5√ó5, pooling) captured in ~516K parameters‚Äîmuch more efficient than naive approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb83549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        \n",
    "        # 1x1 convolution branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_1x1, kernel_size=1), # reduce channels\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 1x1 convolution followed by 3x3 convolution branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_3x3, kernel_size=1), # reduce channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1), # 3x3 conv\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 1x1 convolution followed by 5x5 convolution branch\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_5x5, kernel_size=1), # reduce channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2), # 5x5 conv\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 3x3 max pooling followed by 1x1 convolution branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),   # 3x3 max pooling\n",
    "            nn.Conv2d(in_channels, out_pool, kernel_size=1), # 1x1 conv\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1_out = self.branch1(x)\n",
    "        branch2_out = self.branch2(x)\n",
    "        branch3_out = self.branch3(x)\n",
    "        branch4_out = self.branch4(x)\n",
    "\n",
    "        # Concatenate outputs along the channel dimension\n",
    "        outputs = [branch1_out, branch2_out, branch3_out, branch4_out]\n",
    "        return torch.cat(outputs, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649875e5",
   "metadata": {},
   "source": [
    "## The Heart of Inception: The Inception Module\n",
    "\n",
    "Now let's implement the core innovation - the Inception module that processes input through multiple parallel paths.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "Each Inception module has **4 parallel branches**:\n",
    "\n",
    "```\n",
    "                    Input [B, in_channels, H, W]\n",
    "                           ‚îÇ\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ                  ‚îÇ                  ‚îÇ             ‚îÇ\n",
    "    Branch 1           Branch 2           Branch 3      Branch 4\n",
    "    1√ó1 conv       1√ó1‚Üí3√ó3 convs      1√ó1‚Üí5√ó5 convs   Pool‚Üí1√ó1\n",
    "        ‚îÇ                  ‚îÇ                  ‚îÇ             ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                           ‚îÇ\n",
    "                    Concatenate\n",
    "                           ‚îÇ\n",
    "                Output [B, total_channels, H, W]\n",
    "```\n",
    "\n",
    "### Parameter Breakdown\n",
    "\n",
    "```python\n",
    "def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool):\n",
    "```\n",
    "\n",
    "**Branch 1 params:**\n",
    "- `out_1x1`: Output channels from 1√ó1 conv (e.g., 64)\n",
    "\n",
    "**Branch 2 params:**\n",
    "- `red_3x3`: Reduction channels before 3√ó3 conv (e.g., 96)\n",
    "- `out_3x3`: Output channels from 3√ó3 conv (e.g., 128)\n",
    "\n",
    "**Branch 3 params:**\n",
    "- `red_5x5`: Reduction channels before 5√ó5 conv (e.g., 16)\n",
    "- `out_5x5`: Output channels from 5√ó5 conv (e.g., 32)\n",
    "\n",
    "**Branch 4 params:**\n",
    "- `out_pool`: Output channels after pooling (e.g., 32)\n",
    "\n",
    "**Final output channels = out_1x1 + out_3x3 + out_5x5 + out_pool**\n",
    "\n",
    "Example: 64 + 128 + 32 + 32 = 256 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590fa0b",
   "metadata": {},
   "source": [
    "### Why Each Branch Matters\n",
    "\n",
    "Each branch captures features at different scales:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Branch 1: 1√ó1 Convolution**\n",
    "```python\n",
    "nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n",
    "```\n",
    "- **Purpose**: Captures point-wise channel relationships\n",
    "- **Benefit**: Adds non-linearity with minimal computation\n",
    "- **Think of it as**: A fully-connected layer per pixel\n",
    "\n",
    "---\n",
    "\n",
    "#### **Branch 2: 1√ó1 ‚Üí 3√ó3 Convolution**\n",
    "```python\n",
    "nn.Conv2d(in_channels, red_3x3, kernel_size=1)  # Reduce: 192‚Üí96\n",
    "nn.Conv2d(red_3x3, out_3x3, kernel_size=3)      # Process: 96‚Üí128\n",
    "```\n",
    "\n",
    "**Why two steps?**\n",
    "- **Naive approach**: Direct 3√ó3 on 192 channels = 221,184 params\n",
    "- **Inception approach**: Reduce first, then 3√ó3 = 129,024 params\n",
    "- **Savings**: 42% fewer parameters!\n",
    "\n",
    "**What it captures**: Local patterns (edges, textures, simple shapes)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Branch 3: 1√ó1 ‚Üí 5√ó5 Convolution**\n",
    "```python\n",
    "nn.Conv2d(in_channels, red_5x5, kernel_size=1)  # Reduce: 192‚Üí16\n",
    "nn.Conv2d(red_5x5, out_5x5, kernel_size=5)      # Process: 16‚Üí32\n",
    "```\n",
    "\n",
    "**Receptive field comparison:**\n",
    "- 3√ó3 sees 9 pixels ‚Üí Local context\n",
    "- 5√ó5 sees 25 pixels ‚Üí Wider context\n",
    "\n",
    "**Parameter savings**: 90% fewer params than direct 5√ó5!\n",
    "\n",
    "**What it captures**: Larger objects, extended patterns, broader shapes\n",
    "\n",
    "---\n",
    "\n",
    "#### **Branch 4: MaxPool ‚Üí 1√ó1**\n",
    "```python\n",
    "nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # Preserve strong features\n",
    "nn.Conv2d(in_channels, out_pool, kernel_size=1)   # Adjust channels\n",
    "```\n",
    "\n",
    "**Why pooling?**\n",
    "- Keeps strongest activations\n",
    "- Adds robustness to translations\n",
    "- Different from learned features (complementary)\n",
    "\n",
    "**Why stride=1?** All branches must have same spatial dimensions for concatenation\n",
    "\n",
    "---\n",
    "\n",
    "### Concatenation: The Key to Multi-Scale Learning\n",
    "\n",
    "```python\n",
    "outputs = [branch1_out, branch2_out, branch3_out, branch4_out]\n",
    "return torch.cat(outputs, 1)  # Stack along channel dimension\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input:    [B, 192, 28, 28]\n",
    "           ‚Üì\n",
    "Branch 1: [B,  64, 28, 28]  ‚Üê Point-wise features\n",
    "Branch 2: [B, 128, 28, 28]  ‚Üê Local features (3√ó3)\n",
    "Branch 3: [B,  32, 28, 28]  ‚Üê Regional features (5√ó5)\n",
    "Branch 4: [B,  32, 28, 28]  ‚Üê Pooled features\n",
    "           ‚Üì Concatenate\n",
    "Output:   [B, 256, 28, 28]  ‚Üê All scales combined!\n",
    "```\n",
    "\n",
    "**Why concatenate (not add)?**\n",
    "- **Preserves all information**: Each scale kept separate\n",
    "- **Network decides importance**: Next layer learns which features to use\n",
    "- **Multi-scale representation**: All receptive fields available simultaneously\n",
    "\n",
    "**The genius**: Instead of choosing one kernel size, use all of them! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4b9e7",
   "metadata": {},
   "source": [
    "## Building the Complete GoogleNet Architecture\n",
    "\n",
    "Now let's understand how these Inception modules are assembled into the full GoogleNet (Inception v1) architecture!\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/inception_net_chart.png\" alt=\"Full ResNet Architecture\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "**Architecture Overview:**\n",
    "```\n",
    "Input (224√ó224√ó3)\n",
    "    ‚Üì Initial Convolutions\n",
    "Stem Network\n",
    "    ‚Üì\n",
    "[Inception 3a, 3b] + MaxPool\n",
    "    ‚Üì\n",
    "[Inception 4a, 4b, 4c, 4d, 4e] + MaxPool\n",
    "    ‚Üì\n",
    "[Inception 5a, 5b]\n",
    "    ‚Üì\n",
    "Global Average Pool + FC\n",
    "    ‚Üì\n",
    "Output (1000 classes)\n",
    "```\n",
    "\n",
    "**Key Design Principles:**\n",
    "1. **Progressive channel expansion**: Start small (64), grow to large (1024)\n",
    "2. **Spatial reduction with MaxPool**: Reduce dimensions between Inception groups\n",
    "3. **Multiple Inception modules**: 9 total modules in 3 groups\n",
    "4. **No large FC layers**: Use Global Average Pooling to reduce parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc792521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1000):\n",
    "        super(GoogleNet,self).__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2= conv_block(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # format :  in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool\n",
    "        self.inception3a= InceptionModule(in_channels=192, out_1x1=64, red_3x3=96, out_3x3=128, red_5x5=16, out_5x5=32, out_pool=32)\n",
    "        self.inception3b= InceptionModule(in_channels=256, out_1x1=128, red_3x3=128, out_3x3=192, red_5x5=32, out_5x5=96, out_pool=64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception4a= InceptionModule(in_channels=480, out_1x1=192, red_3x3=96, out_3x3=208, red_5x5=16, out_5x5=48, out_pool=64)\n",
    "        self.inception4b= InceptionModule(in_channels=512, out_1x1=160, red_3x3=112, out_3x3=224, red_5x5=24, out_5x5=64, out_pool=64)\n",
    "        self.inception4c= InceptionModule(in_channels=512, out_1x1=128, red_3x3=128, out_3x3=256, red_5x5=24, out_5x5=64, out_pool=64)\n",
    "        self.inception4d= InceptionModule(in_channels=512, out_1x1=112, red_3x3=144, out_3x3=288, red_5x5=32, out_5x5=64, out_pool=64)\n",
    "        self.inception4e= InceptionModule(in_channels=528, out_1x1=256, red_3x3=160, out_3x3=320, red_5x5=32, out_5x5=128, out_pool=128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception5a= InceptionModule(in_channels=832, out_1x1=256, red_3x3=160, out_3x3=320, red_5x5=32, out_5x5=128, out_pool=128)\n",
    "        self.inception5b= InceptionModule(in_channels=832, out_1x1=384, red_3x3=192, out_3x3=384, red_5x5=48, out_5x5=128, out_pool=128)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e97ce",
   "metadata": {},
   "source": [
    "### Complete Architecture: Dimension Flow\n",
    "\n",
    "Here's how data transforms through GoogleNet:\n",
    "\n",
    "```\n",
    "Layer Group      | Output Shape          | Channels | Spatial | Notes\n",
    "-----------------|----------------------|----------|---------|------------------\n",
    "Input            | [B, 3, 224, 224]     | 3        | 224√ó224 | RGB images\n",
    "Conv1 + Pool1    | [B, 64, 56, 56]      | 64       | 56√ó56   | Rapid reduction\n",
    "Conv2 + Pool2    | [B, 192, 28, 28]     | 192      | 28√ó28   | Channel expansion\n",
    "Inception 3a     | [B, 256, 28, 28]     | 256      | 28√ó28   | Multi-scale start\n",
    "Inception 3b     | [B, 480, 28, 28]     | 480      | 28√ó28   | Growing capacity\n",
    "MaxPool3         | [B, 480, 14, 14]     | 480      | 14√ó14   | Spatial reduction\n",
    "Inception 4a-4e  | [B, 832, 14, 14]     | 512‚Üí832  | 14√ó14   | 5 modules (core)\n",
    "MaxPool4         | [B, 832, 7, 7]       | 832      | 7√ó7     | Final reduction\n",
    "Inception 5a-5b  | [B, 1024, 7, 7]      | 832‚Üí1024 | 7√ó7     | Maximum capacity\n",
    "Global AvgPool   | [B, 1024, 1, 1]      | 1024     | 1√ó1     | Spatial collapse\n",
    "FC               | [B, 1000]            | 1000     | -       | Classification\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Architecture Phases\n",
    "\n",
    "#### **Phase 1-2: Stem Network**\n",
    "```python\n",
    "Conv7√ó7(stride=2) ‚Üí Pool ‚Üí Conv3√ó3 ‚Üí Pool\n",
    "224√ó224 ‚Üí 56√ó56 ‚Üí 28√ó28\n",
    "```\n",
    "**Purpose**: Rapid spatial reduction, basic feature extraction\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 3: First Inception Group (3a, 3b)**\n",
    "```\n",
    "192 ‚Üí 256 ‚Üí 480 channels (at 28√ó28)\n",
    "```\n",
    "**Purpose**: Multi-scale learning begins, moderate channel growth\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 4: Core Inception Group (4a-4e)**\n",
    "```\n",
    "480 ‚Üí 512 ‚Üí 512 ‚Üí 512 ‚Üí 528 ‚Üí 832 channels (at 14√ó14)\n",
    "```\n",
    "**Why 5 modules?**\n",
    "- **4a-4c**: Stable channels (512) for feature refinement\n",
    "- **4d**: Slight increase (528) preparing for expansion  \n",
    "- **4e**: Large jump (832) for high-capacity features\n",
    "\n",
    "**This is the network's core** - most semantic learning happens here\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 5: Final Inception Group (5a, 5b)**\n",
    "```\n",
    "832 ‚Üí 832 ‚Üí 1024 channels (at 7√ó7)\n",
    "```\n",
    "**Why only 2 modules?**\n",
    "- Spatial dimensions already very small (7√ó7)\n",
    "- High channel count (1024) = high capacity\n",
    "- Diminishing returns for more modules\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 6: Classification Head**\n",
    "```python\n",
    "GlobalAvgPool(7√ó7 ‚Üí 1√ó1) ‚Üí Dropout(0.4) ‚Üí FC(1024 ‚Üí 1000)\n",
    "```\n",
    "\n",
    "**Why Global Average Pooling?**\n",
    "\n",
    "| Approach | Parameters | Efficiency |\n",
    "|----------|-----------|------------|\n",
    "| **Old (VGG)**: Flatten + FC | 50,176 √ó 4096 = 205M | ‚ùå Huge |\n",
    "| **GoogleNet**: AvgPool + FC | 1024 √ó 1000 = 1M | ‚úÖ 200√ó fewer! |\n",
    "\n",
    "**Benefits**: Fewer parameters, built-in regularization, spatial invariance\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Design Works\n",
    "\n",
    "**1. Progressive Abstraction**\n",
    "```\n",
    "Stem ‚Üí Low-level (edges, colors)\n",
    "Inception 3 ‚Üí Mid-level (textures)\n",
    "Inception 4 ‚Üí High-level (object parts)\n",
    "Inception 5 ‚Üí Abstract (full objects)\n",
    "```\n",
    "\n",
    "**2. Efficient Pyramid Structure**\n",
    "```\n",
    "Spatial:  224 ‚Üí 56 ‚Üí 28 ‚Üí 14 ‚Üí 7 ‚Üí 1  (aggressive reduction)\n",
    "Channels: 3 ‚Üí 64 ‚Üí 192 ‚Üí 480 ‚Üí 832 ‚Üí 1024  (gradual expansion)\n",
    "```\n",
    "As spatial ‚Üì, channels ‚Üë ‚Üí maintains information capacity\n",
    "\n",
    "**3. Computational Efficiency**\n",
    "- Only 7M parameters (vs VGG's 138M!)\n",
    "- 1√ó1 dimensionality reduction in every Inception module\n",
    "- Global Average Pooling instead of large FC layers\n",
    "\n",
    "**4. Multi-Scale at Every Stage**\n",
    "```\n",
    "Every Inception module learns:\n",
    "- Point-wise features (1√ó1)\n",
    "- Local patterns (3√ó3)\n",
    "- Regional patterns (5√ó5)\n",
    "- Robust features (pooling)\n",
    "```\n",
    "\n",
    "The network automatically learns which scale matters for each image! \udfaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e77b0a",
   "metadata": {},
   "source": [
    "### Forward Pass Implementation: The Journey of Data\n",
    "\n",
    "Let's break down the `forward` method to understand how data flows through GoogleNet:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    # Phase 1: Stem network\n",
    "    x = self.conv1(x)        # [B, 3, 224, 224] ‚Üí [B, 64, 112, 112]\n",
    "    x = self.maxpool1(x)     # ‚Üí [B, 64, 56, 56]\n",
    "    \n",
    "    # Phase 2: Second convolution\n",
    "    x = self.conv2(x)        # [B, 64, 56, 56] ‚Üí [B, 192, 56, 56]\n",
    "    x = self.maxpool2(x)     # ‚Üí [B, 192, 28, 28]\n",
    "    \n",
    "    # Phase 3: First Inception group\n",
    "    x = self.inception3a(x)  # [B, 192, 28, 28] ‚Üí [B, 256, 28, 28]\n",
    "    x = self.inception3b(x)  # [B, 256, 28, 28] ‚Üí [B, 480, 28, 28]\n",
    "    x = self.maxpool3(x)     # ‚Üí [B, 480, 14, 14]\n",
    "    \n",
    "    # Phase 4: Middle Inception group (the core)\n",
    "    x = self.inception4a(x)  # [B, 480, 14, 14] ‚Üí [B, 512, 14, 14]\n",
    "    x = self.inception4b(x)  # [B, 512, 14, 14] ‚Üí [B, 512, 14, 14]\n",
    "    x = self.inception4c(x)  # [B, 512, 14, 14] ‚Üí [B, 512, 14, 14]\n",
    "    x = self.inception4d(x)  # [B, 512, 14, 14] ‚Üí [B, 528, 14, 14]\n",
    "    x = self.inception4e(x)  # [B, 528, 14, 14] ‚Üí [B, 832, 14, 14]\n",
    "    x = self.maxpool4(x)     # ‚Üí [B, 832, 7, 7]\n",
    "    \n",
    "    # Phase 5: Final Inception group\n",
    "    x = self.inception5a(x)  # [B, 832, 7, 7] ‚Üí [B, 832, 7, 7]\n",
    "    x = self.inception5b(x)  # [B, 832, 7, 7] ‚Üí [B, 1024, 7, 7]\n",
    "    \n",
    "    # Phase 6: Classification head\n",
    "    x = self.avgpool(x)      # [B, 1024, 7, 7] ‚Üí [B, 1024, 1, 1]\n",
    "    x = torch.flatten(x, 1)  # ‚Üí [B, 1024]\n",
    "    x = self.dropout(x)      # Still [B, 1024], but 40% randomly zeroed\n",
    "    x = self.fc(x)           # ‚Üí [B, 1000] - final class logits\n",
    "    \n",
    "    return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Sequential Design?\n",
    "\n",
    "**1. Progressive abstraction:**\n",
    "```\n",
    "Input: Raw pixels\n",
    "‚Üì Conv layers\n",
    "Low-level features (edges, colors)\n",
    "‚Üì Inception 3a, 3b\n",
    "Mid-level features (textures, simple patterns)\n",
    "‚Üì Inception 4a-4e\n",
    "High-level features (object parts, complex patterns)\n",
    "‚Üì Inception 5a, 5b\n",
    "Abstract features (full objects, semantic concepts)\n",
    "‚Üì Classification\n",
    "Class predictions\n",
    "```\n",
    "\n",
    "**2. Spatial reduction strategy:**\n",
    "```\n",
    "224 ‚Üí 112 ‚Üí 56  (Conv1 + Pool1: Rapid initial reduction)\n",
    "56 ‚Üí 56 ‚Üí 28    (Conv2 + Pool2: Moderate reduction)\n",
    "28 ‚Üí 28 ‚Üí 14    (Inception 3 + Pool3: After multi-scale learning)\n",
    "14 ‚Üí 14 ‚Üí 7     (Inception 4 + Pool4: After deep processing)\n",
    "7 ‚Üí 7 ‚Üí 1       (Inception 5 + AvgPool: Final reduction)\n",
    "\n",
    "Each reduction happens AFTER extracting rich features at that scale!\n",
    "```\n",
    "\n",
    "**3. Channel expansion strategy:**\n",
    "```\n",
    "3 ‚Üí 64 ‚Üí 192    (Stem: Quick expansion)\n",
    "192 ‚Üí 256 ‚Üí 480 (Inception 3: Moderate growth)\n",
    "480 ‚Üí 512 ‚Üí 832 (Inception 4: Stable then grow)\n",
    "832 ‚Üí 1024      (Inception 5: Peak capacity)\n",
    "1024 ‚Üí 1000     (FC: Final mapping)\n",
    "\n",
    "Trade-off: As spatial ‚Üì, channels ‚Üë (maintain information capacity)\n",
    "```\n",
    "\n",
    "**4. Memory efficiency:**\n",
    "```\n",
    "Start of network: Large spatial (224√ó224), few channels (3)\n",
    "Middle: Balanced (14√ó14 with 512 channels)\n",
    "End: Small spatial (7√ó7), many channels (1024)\n",
    "\n",
    "This pyramid structure is memory-efficient!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What Makes This Forward Pass Special?\n",
    "\n",
    "**Compared to VGG (pure sequential convs):**\n",
    "```\n",
    "VGG: Conv ‚Üí Conv ‚Üí Conv ‚Üí ... ‚Üí Conv (single path)\n",
    "GoogleNet: Conv ‚Üí Inception ‚Üí Inception ‚Üí ... (multi-scale parallel)\n",
    "\n",
    "GoogleNet learns multiple scales simultaneously at each stage!\n",
    "```\n",
    "\n",
    "**Compared to ResNet (skip connections):**\n",
    "```\n",
    "ResNet: x_out = F(x) + x (addition for gradient flow)\n",
    "GoogleNet: x_out = [F1(x), F2(x), F3(x), F4(x)] (concatenation for multi-scale)\n",
    "\n",
    "Different solutions to different problems!\n",
    "```\n",
    "\n",
    "**The genius:**\n",
    "> \"Instead of choosing between small kernels (3√ó3) or large kernels (5√ó5), use both and let the network decide which is important for each specific image!\"\n",
    "\n",
    "This parallel processing at every stage is why Inception architectures are so powerful! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a282246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GoogleNet (Inception v1) - Model Test\n",
      "============================================================\n",
      "\n",
      "‚úì Input shape:  torch.Size([2, 3, 224, 224])\n",
      "‚úì Output shape: torch.Size([2, 1000])\n",
      "\n",
      "üìä Model Statistics:\n",
      "  Total parameters: 6,994,904\n",
      "  Model size: ~26.7 MB\n",
      "\n",
      "============================================================\n",
      "‚úÖ GoogleNet working correctly!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the GoogleNet implementation\n",
    "model = GoogleNet(in_channels=3, num_classes=1000)\n",
    "\n",
    "# Create sample batch (2 images)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GoogleNet (Inception v1) - Model Test\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "print(f\"\\n‚úì Input shape:  {x.shape}\")\n",
    "print(f\"‚úì Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ GoogleNet working correctly!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05802934",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways: GoogleNet vs ResNet\n",
    "\n",
    "### üçïüîç Remember Our Silly Examples!\n",
    "\n",
    "**ResNet = The Pizza Recipe Chain**\n",
    "- Problem: Information gets corrupted through 50 layers\n",
    "- Solution: Skip connections = Give original recipe directly + collect improvements\n",
    "- Formula: `Output = Original Recipe + Small Changes`\n",
    "\n",
    "**InceptionNet = The Detective Team**\n",
    "- Problem: One magnifying glass size might miss clues\n",
    "- Solution: 4 detectives with different tools working in parallel\n",
    "- Formula: `Output = [Tiny Details, Normal Clues, Big Patterns, Key Evidence]`\n",
    "\n",
    "---\n",
    "\n",
    "### Two Different Solutions to Deep Network Problems\n",
    "\n",
    "| Aspect | **GoogleNet (Inception)** üîç | **ResNet** üçï |\n",
    "|--------|---------------------------|------------|\n",
    "| **Core Problem** | Computational efficiency | Gradient degradation |\n",
    "| **Solution** | Multi-scale parallel paths | Skip connections |\n",
    "| **Philosophy** | \"Go wider\" (parallel) | \"Go deeper\" (sequential) |\n",
    "| **Feature Combination** | Concatenation | Addition |\n",
    "| **Parameters** | ~7M (very efficient) | ~25M (ResNet-50) |\n",
    "| **Depth** | 22 layers | 50-152 layers |\n",
    "| **Key Innovation** | 1√ó1 dimensionality reduction | Identity shortcuts |\n",
    "\n",
    "---\n",
    "\n",
    "### üé≠ Drama Club Analogy\n",
    "\n",
    "**GoogleNet**: Like a play where 4 actors perform different versions of the same scene simultaneously, then you pick the best parts from each!\n",
    "\n",
    "**ResNet**: Like writing a story where each chapter adds to the previous one, but you always keep the original chapter 1 as reference!\n",
    "\n",
    "---\n",
    "\n",
    "### What Makes Each Architecture Unique?\n",
    "\n",
    "**GoogleNet's Strength:**\n",
    "```python\n",
    "# Multi-scale in parallel (Detective team!)\n",
    "output = [1√ó1(x), 3√ó3(x), 5√ó5(x), pool(x)]  # Concatenate all scales\n",
    "```\n",
    "- **Best for:** Efficient inference, mobile deployment, multi-scale objects\n",
    "- **Trade-off:** More complex architecture, harder to modify\n",
    "- **Real analogy:** Swiss Army knife - multiple tools in one! üî™\n",
    "\n",
    "**ResNet's Strength:**\n",
    "```python\n",
    "# Residual learning (Pizza recipe with improvements!)\n",
    "output = F(x) + x  # Learn only the difference\n",
    "```\n",
    "- **Best for:** Very deep networks, transfer learning, gradient flow\n",
    "- **Trade-off:** More parameters, slightly slower\n",
    "- **Real analogy:** Building blocks - each layer stacks on previous! üß±\n",
    "\n",
    "---\n",
    "\n",
    "### üéÆ Video Game Analogy\n",
    "\n",
    "**GoogleNet:**\n",
    "```\n",
    "Like playing a game with 4 different characters at once:\n",
    "- Character 1: Speed specialist (1√ó1 - fast processing)\n",
    "- Character 2: Balanced fighter (3√ó3 - good all-around)\n",
    "- Character 3: Heavy hitter (5√ó5 - big moves)\n",
    "- Character 4: Support (pooling - assists others)\n",
    "\n",
    "You control all 4, then combine their powers!\n",
    "```\n",
    "\n",
    "**ResNet:**\n",
    "```\n",
    "Like a game with checkpoints:\n",
    "- Start level\n",
    "- Checkpoint 1: Save progress + improve skills\n",
    "- Checkpoint 2: Save progress + improve more\n",
    "- Checkpoint 3: Save progress + improve more\n",
    "...\n",
    "- End level with ALL improvements + original saves!\n",
    "\n",
    "You never lose your starting position!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "**Choose GoogleNet when:**\n",
    "- ‚úÖ Need efficient model (fewer parameters)\n",
    "- ‚úÖ Deploying to resource-constrained devices\n",
    "- ‚úÖ Objects appear at multiple scales\n",
    "- ‚úÖ Want faster inference\n",
    "- üîç Think: \"I need multiple perspectives!\"\n",
    "\n",
    "**Choose ResNet when:**\n",
    "- ‚úÖ Need maximum accuracy\n",
    "- ‚úÖ Doing transfer learning (more pre-trained models available)\n",
    "- ‚úÖ Need very deep network (100+ layers)\n",
    "- ‚úÖ Have sufficient computational resources\n",
    "- üçï Think: \"I need to go REALLY deep!\"\n",
    "\n",
    "---\n",
    "\n",
    "### üç™ Cookie Classification Example\n",
    "\n",
    "**Problem:** Identify cookie types from photos\n",
    "\n",
    "**GoogleNet approach:**\n",
    "```\n",
    "Detective 1 (1√ó1): \"Sees sugar crystals on top!\"\n",
    "Detective 2 (3√ó3): \"Sees chocolate chip pattern!\"\n",
    "Detective 3 (5√ó5): \"Sees overall round shape!\"\n",
    "Detective 4 (pool): \"Keeps texture information!\"\n",
    "‚Üí Combines all clues ‚Üí \"It's a chocolate chip cookie!\"\n",
    "```\n",
    "\n",
    "**ResNet approach:**\n",
    "```\n",
    "Layer 1: \"Looks like baked goods\" + [original image]\n",
    "Layer 2: \"Has dark spots\" + [layer 1 + original]\n",
    "Layer 3: \"Spots are chocolate\" + [layer 2 + layer 1 + original]\n",
    "...\n",
    "Layer 50: \"Definitely chocolate chip!\" + [all previous layers]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### The Lasting Impact\n",
    "\n",
    "**GoogleNet taught us:**\n",
    "- 1√ó1 convolutions are powerful (like having a zoom lens!)\n",
    "- Parallel processing captures multi-scale features (detective team!)\n",
    "- Bigger ‚â† better; smarter = better\n",
    "\n",
    "**ResNet taught us:**\n",
    "- Skip connections enable very deep networks (preserve the recipe!)\n",
    "- Learning differences is easier than learning transformations (small changes!)\n",
    "- Identity mappings preserve gradient flow (don't lose the original!)\n",
    "\n",
    "**Together, they revolutionized deep learning!** üöÄ\n",
    "\n",
    "---\n",
    "\n",
    "### üé™ Final Fun Fact\n",
    "\n",
    "**If neural networks were a circus:**\n",
    "\n",
    "**GoogleNet** = The trapeze act with 4 performers doing different tricks simultaneously, all catching each other perfectly! ü§π\n",
    "\n",
    "**ResNet** = The high wire act where each performer stands on the previous one's shoulders, but there's always a safety net (skip connection) back to the ground! üé™\n",
    "\n",
    "Modern architectures (EfficientNet, MobileNet, Vision Transformers) are like combining the trapeze act WITH the high wire act - best of both worlds! üé≠"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
