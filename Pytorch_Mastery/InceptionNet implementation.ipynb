{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74b2ca7",
   "metadata": {},
   "source": [
    "# InceptionNet (GoogLeNet) Implementation\n",
    "\n",
    "## Understanding the Innovation: \"Going Deeper with Convolutions\"\n",
    "\n",
    "InceptionNet, also known as GoogLeNet, won the ImageNet 2014 competition. While ResNet solved the depth problem with skip connections, Inception solved it differently: **going wider instead of just deeper**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Silly Example: The Detective Team\n",
    "\n",
    "**Imagine you're trying to identify who ate the cookies:**\n",
    "\n",
    "### Traditional CNN (Single Kernel Size):\n",
    "```\n",
    "You hire ONE detective with ONE magnifying glass size\n",
    "```\n",
    "\n",
    "**Detective:** \"I can only see clues at 3Ã—3 inch areas. I found crumbs!\"  \n",
    "**Problem:** What if the important clue was a large footprint? Or a tiny fingerprint? ğŸ¤”  \n",
    "**Result:** ğŸªâŒ Missed evidence!\n",
    "\n",
    "---\n",
    "\n",
    "### InceptionNet (Multiple Kernel Sizes):\n",
    "```\n",
    "You hire FOUR detectives working in parallel:\n",
    "1. Detective with Macro Lens (1Ã—1) â†’ Sees tiny details up close\n",
    "2. Detective with Normal Lens (3Ã—3) â†’ Sees regular clues  \n",
    "3. Detective with Wide Lens (5Ã—5) â†’ Sees big patterns\n",
    "4. Detective with Evidence Bag (pooling) â†’ Keeps important stuff\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- All four detectives examine the SAME crime scene simultaneously\n",
    "- Detective 1: \"Found tiny fingerprint!\"\n",
    "- Detective 2: \"Found crumb trail!\"\n",
    "- Detective 3: \"Found large footprint!\"\n",
    "- Detective 4: \"Bagged all important evidence!\"\n",
    "- They combine all findings â†’ Complete picture! ğŸ•µï¸\n",
    "\n",
    "**Result:** ğŸªâœ… Caught the cookie thief!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Real Lesson\n",
    "\n",
    "**Traditional CNN:** Choose one filter size (3Ã—3 or 5Ã—5). Hope it's right!\n",
    "\n",
    "**InceptionNet:** Use ALL sizes in parallel! Let the network decide what's important! ğŸ¯\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Problem\n",
    "\n",
    "Traditional CNNs face a dilemma:\n",
    "- **Large kernels (5Ã—5, 7Ã—7)**: Capture wide context but are computationally expensive\n",
    "- **Small kernels (3Ã—3)**: Efficient but have limited receptive field\n",
    "- **Which kernel size should we use?** ğŸ¤”\n",
    "\n",
    "### The Inception Solution: \"Why choose? Use them all!\"\n",
    "\n",
    "Instead of choosing one kernel size, **use multiple sizes in parallel** and let the network learn which is most useful!\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/inception_net.png\" alt=\"Full ResNet Architecture\" width=\"1500\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "```\n",
    "Input\n",
    "  â”œâ”€â†’ 1Ã—1 conv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "  â”œâ”€â†’ 1Ã—1 â†’ 3Ã—3 conv â”€â”€â”€â”€â”€â”€â”¤\n",
    "  â”œâ”€â†’ 1Ã—1 â†’ 5Ã—5 conv â”€â”€â”€â”€â”€â”€â”¤  â†’ Concatenate â†’ Output\n",
    "  â””â”€â†’ 3Ã—3 pool â†’ 1Ã—1 conv â”€â”˜\n",
    "```\n",
    "\n",
    "### Key Innovations\n",
    "\n",
    "1. **Multi-scale feature extraction**: Different kernel sizes capture different levels of detail\n",
    "2. **Dimensionality reduction**: 1Ã—1 convolutions reduce computational cost\n",
    "3. **Parallel processing**: All paths process simultaneously (efficient on GPUs)\n",
    "4. **Network in Network**: Using 1Ã—1 convs as \"mini neural networks\" between layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a7c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c04a9a",
   "metadata": {},
   "source": [
    "## Building Block: Conv Block\n",
    "\n",
    "Before building the Inception module, we need a reusable convolutional block that combines Conv â†’ ReLU â†’ BatchNorm.\n",
    "\n",
    "### Why This Pattern?\n",
    "\n",
    "**Standard order in modern CNNs:**\n",
    "```python\n",
    "Conv2d â†’ ReLU â†’ BatchNorm\n",
    "```\n",
    "\n",
    "**What each does:**\n",
    "1. **Conv2d**: Applies convolutional filters to extract features\n",
    "2. **ReLU**: Adds non-linearity (allows learning complex patterns)\n",
    "3. **BatchNorm**: Normalizes activations for stable training\n",
    "\n",
    "**Why bundle them together?**\n",
    "- These three operations almost always appear together\n",
    "- Reduces code duplication\n",
    "- Makes architecture easier to read\n",
    "- Ensures consistent ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bb997f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.batchnorm(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f0d2f",
   "metadata": {},
   "source": [
    "### Understanding the Conv Block Code\n",
    "\n",
    "```python\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `in_channels`: Number of input feature maps (e.g., 64)\n",
    "- `out_channels`: Number of output feature maps (e.g., 192)\n",
    "- `**kwargs`: Flexible parameters for Conv2d (kernel_size, stride, padding, etc.)\n",
    "\n",
    "**Why `**kwargs`?**\n",
    "- Allows passing different convolution configurations without changing the class\n",
    "- Examples: `kernel_size=3, padding=1` or `kernel_size=7, stride=2, padding=3`\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = self.conv(x)      # Apply convolution\n",
    "    x = self.relu(x)      # Add non-linearity\n",
    "    x = self.batchnorm(x) # Normalize\n",
    "    return x\n",
    "```\n",
    "\n",
    "**The flow:**\n",
    "```\n",
    "Input [B, in_channels, H, W]\n",
    "   â†“ Conv2d\n",
    "[B, out_channels, H', W']\n",
    "   â†“ ReLU (non-linearity)\n",
    "[B, out_channels, H', W']\n",
    "   â†“ BatchNorm (normalization)\n",
    "[B, out_channels, H', W']\n",
    "```\n",
    "\n",
    "This simple block will be reused throughout the network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9b4e8",
   "metadata": {},
   "source": [
    "## Understanding the Inception Module Architecture\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/inception_module.png\" alt=\"InceptioModule\" width=\"600\">\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ• Why Concatenation? The Pizza Toppings Analogy!\n",
    "\n",
    "**ResNet (Addition):**\n",
    "```\n",
    "Pizza base + [pepperoni OR mushrooms OR peppers]\n",
    "You blend all toppings into one mixed flavor\n",
    "```\n",
    "\n",
    "**InceptionNet (Concatenation):**\n",
    "```\n",
    "Pizza base + [pepperoni section | mushroom section | pepper section | olive section]\n",
    "Each topping stays separate, you taste each one distinctly!\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- **Addition**: Mixes everything together â†’ lose individual information\n",
    "- **Concatenation**: Keeps everything separate â†’ next layer decides what's important!\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Problem Inception Solves\n",
    "\n",
    "**The Dilemma:** Large kernels (5Ã—5, 7Ã—7) capture more context but are computationally expensive.\n",
    "\n",
    "**The Solution:** Use **dimensionality reduction** with 1Ã—1 convolutions before larger kernels.\n",
    "\n",
    "---\n",
    "\n",
    "### The Four Parallel Paths\n",
    "\n",
    "**Path 1: Direct 1Ã—1 Convolution**\n",
    "- Captures point-wise channel relationships\n",
    "- Provides a lightweight baseline path\n",
    "- ğŸ” Like checking fingerprints (tiny details)\n",
    "\n",
    "**Path 2: 1Ã—1 â†’ 3Ã—3 Convolution**\n",
    "- 1Ã—1 reduces channels (e.g., 192 â†’ 96)\n",
    "- 3Ã—3 captures local spatial patterns\n",
    "- **Efficiency**: 42% fewer parameters than direct 3Ã—3\n",
    "- ğŸ” Like checking footprints (normal clues)\n",
    "\n",
    "**Path 3: 1Ã—1 â†’ 5Ã—5 Convolution**\n",
    "- 1Ã—1 reduces channels even more (e.g., 192 â†’ 16)\n",
    "- 5Ã—5 captures larger spatial context\n",
    "- **Efficiency**: 90% fewer parameters than direct 5Ã—5\n",
    "- ğŸ”­ Like aerial view (big picture)\n",
    "\n",
    "**Path 4: 3Ã—3 MaxPool â†’ 1Ã—1 Convolution**\n",
    "- Pooling preserves important features\n",
    "- 1Ã—1 adjusts channel count for concatenation\n",
    "- Adds robustness to translations\n",
    "- ğŸ“¦ Like evidence bag (keeps best stuff)\n",
    "\n",
    "---\n",
    "\n",
    "### Why Concatenation?\n",
    "\n",
    "```python\n",
    "outputs = [branch1_out, branch2_out, branch3_out, branch4_out]\n",
    "return torch.cat(outputs, 1)  # Concatenate along channel dimension\n",
    "```\n",
    "\n",
    "**Concatenation vs Addition:**\n",
    "- **Addition (ResNet style)**: `output = F(x) + x` â†’ Mixes features\n",
    "- **Concatenation (Inception style)**: `output = [F1(x), F2(x), F3(x), F4(x)]` â†’ Preserves all\n",
    "\n",
    "**Why Concatenation is Better Here:**\n",
    "- Keeps all multi-scale features separate\n",
    "- Next layer decides which scale is important\n",
    "- Network learns feature importance through weights\n",
    "\n",
    "**Like a buffet**: Keep all dishes separate so people can choose what they want! ğŸ½ï¸\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Parameter Calculation\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/inception_block_parameters.png\" alt=\"Inception Parameters\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "**Input:** 32Ã—32Ã—256\n",
    "\n",
    "**Branch Outputs:**\n",
    "- 1Ã—1 path: 64 channels (ğŸ” tiny details)\n",
    "- 3Ã—3 path: 128 channels (ğŸ” normal clues)\n",
    "- 5Ã—5 path: 32 channels (ğŸ”­ big picture)\n",
    "- Pool path: 32 channels (ğŸ“¦ best evidence)\n",
    "\n",
    "**Total Output:** 32Ã—32Ã—(64+128+32+32) = 32Ã—32Ã—256\n",
    "\n",
    "**Result:** Multi-scale features (1Ã—1, 3Ã—3, 5Ã—5, pooling) captured in ~516K parametersâ€”much more efficient than naive approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb83549",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        \n",
    "        # 1x1 convolution branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_1x1, kernel_size=1), # reduce channels\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 1x1 convolution followed by 3x3 convolution branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_3x3, kernel_size=1), # reduce channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(red_3x3, out_3x3, kernel_size=3, padding=1), # 3x3 conv\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 1x1 convolution followed by 5x5 convolution branch\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red_5x5, kernel_size=1), # reduce channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(red_5x5, out_5x5, kernel_size=5, padding=2), # 5x5 conv\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # 3x3 max pooling followed by 1x1 convolution branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),   # 3x3 max pooling\n",
    "            nn.Conv2d(in_channels, out_pool, kernel_size=1), # 1x1 conv\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1_out = self.branch1(x)\n",
    "        branch2_out = self.branch2(x)\n",
    "        branch3_out = self.branch3(x)\n",
    "        branch4_out = self.branch4(x)\n",
    "\n",
    "        # Concatenate outputs along the channel dimension\n",
    "        outputs = [branch1_out, branch2_out, branch3_out, branch4_out]\n",
    "        return torch.cat(outputs, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649875e5",
   "metadata": {},
   "source": [
    "## The Heart of Inception: The Inception Module\n",
    "\n",
    "Now let's implement the core innovation - the Inception module that processes input through multiple parallel paths.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "Each Inception module has **4 parallel branches**:\n",
    "\n",
    "```\n",
    "                    Input [B, in_channels, H, W]\n",
    "                           â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                  â”‚                  â”‚             â”‚\n",
    "    Branch 1           Branch 2           Branch 3      Branch 4\n",
    "    1Ã—1 conv       1Ã—1â†’3Ã—3 convs      1Ã—1â†’5Ã—5 convs   Poolâ†’1Ã—1\n",
    "        â”‚                  â”‚                  â”‚             â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚\n",
    "                    Concatenate\n",
    "                           â”‚\n",
    "                Output [B, total_channels, H, W]\n",
    "```\n",
    "\n",
    "### Parameter Breakdown\n",
    "\n",
    "```python\n",
    "def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool):\n",
    "```\n",
    "\n",
    "**Branch 1 params:**\n",
    "- `out_1x1`: Output channels from 1Ã—1 conv (e.g., 64)\n",
    "\n",
    "**Branch 2 params:**\n",
    "- `red_3x3`: Reduction channels before 3Ã—3 conv (e.g., 96)\n",
    "- `out_3x3`: Output channels from 3Ã—3 conv (e.g., 128)\n",
    "\n",
    "**Branch 3 params:**\n",
    "- `red_5x5`: Reduction channels before 5Ã—5 conv (e.g., 16)\n",
    "- `out_5x5`: Output channels from 5Ã—5 conv (e.g., 32)\n",
    "\n",
    "**Branch 4 params:**\n",
    "- `out_pool`: Output channels after pooling (e.g., 32)\n",
    "\n",
    "**Final output channels = out_1x1 + out_3x3 + out_5x5 + out_pool**\n",
    "\n",
    "Example: 64 + 128 + 32 + 32 = 256 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590fa0b",
   "metadata": {},
   "source": [
    "### Why Each Branch Matters\n",
    "\n",
    "Each branch captures features at different scales:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Branch 1: 1Ã—1 Convolution**\n",
    "```python\n",
    "nn.Conv2d(in_channels, out_1x1, kernel_size=1)\n",
    "```\n",
    "- **Purpose**: Captures point-wise channel relationships\n",
    "- **Benefit**: Adds non-linearity with minimal computation\n",
    "- **Think of it as**: A fully-connected layer per pixel\n",
    "\n",
    "---\n",
    "\n",
    "#### **Branch 2: 1Ã—1 â†’ 3Ã—3 Convolution**\n",
    "```python\n",
    "nn.Conv2d(in_channels, red_3x3, kernel_size=1)  # Reduce: 192â†’96\n",
    "nn.Conv2d(red_3x3, out_3x3, kernel_size=3)      # Process: 96â†’128\n",
    "```\n",
    "\n",
    "**Why two steps?**\n",
    "- **Naive approach**: Direct 3Ã—3 on 192 channels = 221,184 params\n",
    "- **Inception approach**: Reduce first, then 3Ã—3 = 129,024 params\n",
    "- **Savings**: 42% fewer parameters!\n",
    "\n",
    "**What it captures**: Local patterns (edges, textures, simple shapes)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Branch 3: 1Ã—1 â†’ 5Ã—5 Convolution**\n",
    "```python\n",
    "nn.Conv2d(in_channels, red_5x5, kernel_size=1)  # Reduce: 192â†’16\n",
    "nn.Conv2d(red_5x5, out_5x5, kernel_size=5)      # Process: 16â†’32\n",
    "```\n",
    "\n",
    "**Receptive field comparison:**\n",
    "- 3Ã—3 sees 9 pixels â†’ Local context\n",
    "- 5Ã—5 sees 25 pixels â†’ Wider context\n",
    "\n",
    "**Parameter savings**: 90% fewer params than direct 5Ã—5!\n",
    "\n",
    "**What it captures**: Larger objects, extended patterns, broader shapes\n",
    "\n",
    "---\n",
    "\n",
    "#### **Branch 4: MaxPool â†’ 1Ã—1**\n",
    "```python\n",
    "nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # Preserve strong features\n",
    "nn.Conv2d(in_channels, out_pool, kernel_size=1)   # Adjust channels\n",
    "```\n",
    "\n",
    "**Why pooling?**\n",
    "- Keeps strongest activations\n",
    "- Adds robustness to translations\n",
    "- Different from learned features (complementary)\n",
    "\n",
    "**Why stride=1?** All branches must have same spatial dimensions for concatenation\n",
    "\n",
    "---\n",
    "\n",
    "### Concatenation: The Key to Multi-Scale Learning\n",
    "\n",
    "```python\n",
    "outputs = [branch1_out, branch2_out, branch3_out, branch4_out]\n",
    "return torch.cat(outputs, 1)  # Stack along channel dimension\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input:    [B, 192, 28, 28]\n",
    "           â†“\n",
    "Branch 1: [B,  64, 28, 28]  â† Point-wise features\n",
    "Branch 2: [B, 128, 28, 28]  â† Local features (3Ã—3)\n",
    "Branch 3: [B,  32, 28, 28]  â† Regional features (5Ã—5)\n",
    "Branch 4: [B,  32, 28, 28]  â† Pooled features\n",
    "           â†“ Concatenate\n",
    "Output:   [B, 256, 28, 28]  â† All scales combined!\n",
    "```\n",
    "\n",
    "**Why concatenate (not add)?**\n",
    "- **Preserves all information**: Each scale kept separate\n",
    "- **Network decides importance**: Next layer learns which features to use\n",
    "- **Multi-scale representation**: All receptive fields available simultaneously\n",
    "\n",
    "**The genius**: Instead of choosing one kernel size, use all of them! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4b9e7",
   "metadata": {},
   "source": [
    "## Building the Complete GoogleNet Architecture\n",
    "\n",
    "Now let's understand how these Inception modules are assembled into the full GoogleNet (Inception v1) architecture!\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/inception_net_chart.png\" alt=\"Full ResNet Architecture\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "**Architecture Overview:**\n",
    "```\n",
    "Input (224Ã—224Ã—3)\n",
    "    â†“ Initial Convolutions\n",
    "Stem Network\n",
    "    â†“\n",
    "[Inception 3a, 3b] + MaxPool\n",
    "    â†“\n",
    "[Inception 4a, 4b, 4c, 4d, 4e] + MaxPool\n",
    "    â†“\n",
    "[Inception 5a, 5b]\n",
    "    â†“\n",
    "Global Average Pool + FC\n",
    "    â†“\n",
    "Output (1000 classes)\n",
    "```\n",
    "\n",
    "**Key Design Principles:**\n",
    "1. **Progressive channel expansion**: Start small (64), grow to large (1024)\n",
    "2. **Spatial reduction with MaxPool**: Reduce dimensions between Inception groups\n",
    "3. **Multiple Inception modules**: 9 total modules in 3 groups\n",
    "4. **No large FC layers**: Use Global Average Pooling to reduce parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc792521",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1000):\n",
    "        super(GoogleNet,self).__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2= conv_block(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # format :  in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_pool\n",
    "        self.inception3a= InceptionModule(in_channels=192, out_1x1=64, red_3x3=96, out_3x3=128, red_5x5=16, out_5x5=32, out_pool=32)\n",
    "        self.inception3b= InceptionModule(in_channels=256, out_1x1=128, red_3x3=128, out_3x3=192, red_5x5=32, out_5x5=96, out_pool=64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception4a= InceptionModule(in_channels=480, out_1x1=192, red_3x3=96, out_3x3=208, red_5x5=16, out_5x5=48, out_pool=64)\n",
    "        self.inception4b= InceptionModule(in_channels=512, out_1x1=160, red_3x3=112, out_3x3=224, red_5x5=24, out_5x5=64, out_pool=64)\n",
    "        self.inception4c= InceptionModule(in_channels=512, out_1x1=128, red_3x3=128, out_3x3=256, red_5x5=24, out_5x5=64, out_pool=64)\n",
    "        self.inception4d= InceptionModule(in_channels=512, out_1x1=112, red_3x3=144, out_3x3=288, red_5x5=32, out_5x5=64, out_pool=64)\n",
    "        self.inception4e= InceptionModule(in_channels=528, out_1x1=256, red_3x3=160, out_3x3=320, red_5x5=32, out_5x5=128, out_pool=128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.inception5a= InceptionModule(in_channels=832, out_1x1=256, red_3x3=160, out_3x3=320, red_5x5=32, out_5x5=128, out_pool=128)\n",
    "        self.inception5b= InceptionModule(in_channels=832, out_1x1=384, red_3x3=192, out_3x3=384, red_5x5=48, out_5x5=128, out_pool=128)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc = nn.Linear(1024, num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080e97ce",
   "metadata": {},
   "source": [
    "### Complete Architecture: Dimension Flow\n",
    "\n",
    "Here's how data transforms through GoogleNet:\n",
    "\n",
    "```\n",
    "Layer Group      | Output Shape          | Channels | Spatial | Notes\n",
    "-----------------|----------------------|----------|---------|------------------\n",
    "Input            | [B, 3, 224, 224]     | 3        | 224Ã—224 | RGB images\n",
    "Conv1 + Pool1    | [B, 64, 56, 56]      | 64       | 56Ã—56   | Rapid reduction\n",
    "Conv2 + Pool2    | [B, 192, 28, 28]     | 192      | 28Ã—28   | Channel expansion\n",
    "Inception 3a     | [B, 256, 28, 28]     | 256      | 28Ã—28   | Multi-scale start\n",
    "Inception 3b     | [B, 480, 28, 28]     | 480      | 28Ã—28   | Growing capacity\n",
    "MaxPool3         | [B, 480, 14, 14]     | 480      | 14Ã—14   | Spatial reduction\n",
    "Inception 4a-4e  | [B, 832, 14, 14]     | 512â†’832  | 14Ã—14   | 5 modules (core)\n",
    "MaxPool4         | [B, 832, 7, 7]       | 832      | 7Ã—7     | Final reduction\n",
    "Inception 5a-5b  | [B, 1024, 7, 7]      | 832â†’1024 | 7Ã—7     | Maximum capacity\n",
    "Global AvgPool   | [B, 1024, 1, 1]      | 1024     | 1Ã—1     | Spatial collapse\n",
    "FC               | [B, 1000]            | 1000     | -       | Classification\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Key Architecture Phases\n",
    "\n",
    "#### **Phase 1-2: Stem Network**\n",
    "```python\n",
    "Conv7Ã—7(stride=2) â†’ Pool â†’ Conv3Ã—3 â†’ Pool\n",
    "224Ã—224 â†’ 56Ã—56 â†’ 28Ã—28\n",
    "```\n",
    "**Purpose**: Rapid spatial reduction, basic feature extraction\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 3: First Inception Group (3a, 3b)**\n",
    "```\n",
    "192 â†’ 256 â†’ 480 channels (at 28Ã—28)\n",
    "```\n",
    "**Purpose**: Multi-scale learning begins, moderate channel growth\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 4: Core Inception Group (4a-4e)**\n",
    "```\n",
    "480 â†’ 512 â†’ 512 â†’ 512 â†’ 528 â†’ 832 channels (at 14Ã—14)\n",
    "```\n",
    "**Why 5 modules?**\n",
    "- **4a-4c**: Stable channels (512) for feature refinement\n",
    "- **4d**: Slight increase (528) preparing for expansion  \n",
    "- **4e**: Large jump (832) for high-capacity features\n",
    "\n",
    "**This is the network's core** - most semantic learning happens here\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 5: Final Inception Group (5a, 5b)**\n",
    "```\n",
    "832 â†’ 832 â†’ 1024 channels (at 7Ã—7)\n",
    "```\n",
    "**Why only 2 modules?**\n",
    "- Spatial dimensions already very small (7Ã—7)\n",
    "- High channel count (1024) = high capacity\n",
    "- Diminishing returns for more modules\n",
    "\n",
    "---\n",
    "\n",
    "#### **Phase 6: Classification Head**\n",
    "```python\n",
    "GlobalAvgPool(7Ã—7 â†’ 1Ã—1) â†’ Dropout(0.4) â†’ FC(1024 â†’ 1000)\n",
    "```\n",
    "\n",
    "**Why Global Average Pooling?**\n",
    "\n",
    "| Approach | Parameters | Efficiency |\n",
    "|----------|-----------|------------|\n",
    "| **Old (VGG)**: Flatten + FC | 50,176 Ã— 4096 = 205M | âŒ Huge |\n",
    "| **GoogleNet**: AvgPool + FC | 1024 Ã— 1000 = 1M | âœ… 200Ã— fewer! |\n",
    "\n",
    "**Benefits**: Fewer parameters, built-in regularization, spatial invariance\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Design Works\n",
    "\n",
    "**1. Progressive Abstraction**\n",
    "```\n",
    "Stem â†’ Low-level (edges, colors)\n",
    "Inception 3 â†’ Mid-level (textures)\n",
    "Inception 4 â†’ High-level (object parts)\n",
    "Inception 5 â†’ Abstract (full objects)\n",
    "```\n",
    "\n",
    "**2. Efficient Pyramid Structure**\n",
    "```\n",
    "Spatial:  224 â†’ 56 â†’ 28 â†’ 14 â†’ 7 â†’ 1  (aggressive reduction)\n",
    "Channels: 3 â†’ 64 â†’ 192 â†’ 480 â†’ 832 â†’ 1024  (gradual expansion)\n",
    "```\n",
    "As spatial â†“, channels â†‘ â†’ maintains information capacity\n",
    "\n",
    "**3. Computational Efficiency**\n",
    "- Only 7M parameters (vs VGG's 138M!)\n",
    "- 1Ã—1 dimensionality reduction in every Inception module\n",
    "- Global Average Pooling instead of large FC layers\n",
    "\n",
    "**4. Multi-Scale at Every Stage**\n",
    "```\n",
    "Every Inception module learns:\n",
    "- Point-wise features (1Ã—1)\n",
    "- Local patterns (3Ã—3)\n",
    "- Regional patterns (5Ã—5)\n",
    "- Robust features (pooling)\n",
    "```\n",
    "\n",
    "The network automatically learns which scale matters for each image! \udfaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e77b0a",
   "metadata": {},
   "source": [
    "### Forward Pass Implementation: The Journey of Data\n",
    "\n",
    "Let's break down the `forward` method to understand how data flows through GoogleNet:\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    # Phase 1: Stem network\n",
    "    x = self.conv1(x)        # [B, 3, 224, 224] â†’ [B, 64, 112, 112]\n",
    "    x = self.maxpool1(x)     # â†’ [B, 64, 56, 56]\n",
    "    \n",
    "    # Phase 2: Second convolution\n",
    "    x = self.conv2(x)        # [B, 64, 56, 56] â†’ [B, 192, 56, 56]\n",
    "    x = self.maxpool2(x)     # â†’ [B, 192, 28, 28]\n",
    "    \n",
    "    # Phase 3: First Inception group\n",
    "    x = self.inception3a(x)  # [B, 192, 28, 28] â†’ [B, 256, 28, 28]\n",
    "    x = self.inception3b(x)  # [B, 256, 28, 28] â†’ [B, 480, 28, 28]\n",
    "    x = self.maxpool3(x)     # â†’ [B, 480, 14, 14]\n",
    "    \n",
    "    # Phase 4: Middle Inception group (the core)\n",
    "    x = self.inception4a(x)  # [B, 480, 14, 14] â†’ [B, 512, 14, 14]\n",
    "    x = self.inception4b(x)  # [B, 512, 14, 14] â†’ [B, 512, 14, 14]\n",
    "    x = self.inception4c(x)  # [B, 512, 14, 14] â†’ [B, 512, 14, 14]\n",
    "    x = self.inception4d(x)  # [B, 512, 14, 14] â†’ [B, 528, 14, 14]\n",
    "    x = self.inception4e(x)  # [B, 528, 14, 14] â†’ [B, 832, 14, 14]\n",
    "    x = self.maxpool4(x)     # â†’ [B, 832, 7, 7]\n",
    "    \n",
    "    # Phase 5: Final Inception group\n",
    "    x = self.inception5a(x)  # [B, 832, 7, 7] â†’ [B, 832, 7, 7]\n",
    "    x = self.inception5b(x)  # [B, 832, 7, 7] â†’ [B, 1024, 7, 7]\n",
    "    \n",
    "    # Phase 6: Classification head\n",
    "    x = self.avgpool(x)      # [B, 1024, 7, 7] â†’ [B, 1024, 1, 1]\n",
    "    x = torch.flatten(x, 1)  # â†’ [B, 1024]\n",
    "    x = self.dropout(x)      # Still [B, 1024], but 40% randomly zeroed\n",
    "    x = self.fc(x)           # â†’ [B, 1000] - final class logits\n",
    "    \n",
    "    return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Sequential Design?\n",
    "\n",
    "**1. Progressive abstraction:**\n",
    "```\n",
    "Input: Raw pixels\n",
    "â†“ Conv layers\n",
    "Low-level features (edges, colors)\n",
    "â†“ Inception 3a, 3b\n",
    "Mid-level features (textures, simple patterns)\n",
    "â†“ Inception 4a-4e\n",
    "High-level features (object parts, complex patterns)\n",
    "â†“ Inception 5a, 5b\n",
    "Abstract features (full objects, semantic concepts)\n",
    "â†“ Classification\n",
    "Class predictions\n",
    "```\n",
    "\n",
    "**2. Spatial reduction strategy:**\n",
    "```\n",
    "224 â†’ 112 â†’ 56  (Conv1 + Pool1: Rapid initial reduction)\n",
    "56 â†’ 56 â†’ 28    (Conv2 + Pool2: Moderate reduction)\n",
    "28 â†’ 28 â†’ 14    (Inception 3 + Pool3: After multi-scale learning)\n",
    "14 â†’ 14 â†’ 7     (Inception 4 + Pool4: After deep processing)\n",
    "7 â†’ 7 â†’ 1       (Inception 5 + AvgPool: Final reduction)\n",
    "\n",
    "Each reduction happens AFTER extracting rich features at that scale!\n",
    "```\n",
    "\n",
    "**3. Channel expansion strategy:**\n",
    "```\n",
    "3 â†’ 64 â†’ 192    (Stem: Quick expansion)\n",
    "192 â†’ 256 â†’ 480 (Inception 3: Moderate growth)\n",
    "480 â†’ 512 â†’ 832 (Inception 4: Stable then grow)\n",
    "832 â†’ 1024      (Inception 5: Peak capacity)\n",
    "1024 â†’ 1000     (FC: Final mapping)\n",
    "\n",
    "Trade-off: As spatial â†“, channels â†‘ (maintain information capacity)\n",
    "```\n",
    "\n",
    "**4. Memory efficiency:**\n",
    "```\n",
    "Start of network: Large spatial (224Ã—224), few channels (3)\n",
    "Middle: Balanced (14Ã—14 with 512 channels)\n",
    "End: Small spatial (7Ã—7), many channels (1024)\n",
    "\n",
    "This pyramid structure is memory-efficient!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### What Makes This Forward Pass Special?\n",
    "\n",
    "**Compared to VGG (pure sequential convs):**\n",
    "```\n",
    "VGG: Conv â†’ Conv â†’ Conv â†’ ... â†’ Conv (single path)\n",
    "GoogleNet: Conv â†’ Inception â†’ Inception â†’ ... (multi-scale parallel)\n",
    "\n",
    "GoogleNet learns multiple scales simultaneously at each stage!\n",
    "```\n",
    "\n",
    "**Compared to ResNet (skip connections):**\n",
    "```\n",
    "ResNet: x_out = F(x) + x (addition for gradient flow)\n",
    "GoogleNet: x_out = [F1(x), F2(x), F3(x), F4(x)] (concatenation for multi-scale)\n",
    "\n",
    "Different solutions to different problems!\n",
    "```\n",
    "\n",
    "**The genius:**\n",
    "> \"Instead of choosing between small kernels (3Ã—3) or large kernels (5Ã—5), use both and let the network decide which is important for each specific image!\"\n",
    "\n",
    "This parallel processing at every stage is why Inception architectures are so powerful! ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a282246c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GoogleNet (Inception v1) - Model Test\n",
      "============================================================\n",
      "\n",
      "âœ“ Input shape:  torch.Size([2, 3, 224, 224])\n",
      "âœ“ Output shape: torch.Size([2, 1000])\n",
      "\n",
      "ğŸ“Š Model Statistics:\n",
      "  Total parameters: 6,994,904\n",
      "  Model size: ~26.7 MB\n",
      "\n",
      "============================================================\n",
      "âœ… GoogleNet working correctly!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the GoogleNet implementation\n",
    "model = GoogleNet(in_channels=3, num_classes=1000)\n",
    "\n",
    "# Create sample batch (2 images)\n",
    "x = torch.randn(2, 3, 224, 224)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GoogleNet (Inception v1) - Model Test\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "print(f\"\\nâœ“ Input shape:  {x.shape}\")\n",
    "print(f\"âœ“ Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nğŸ“Š Model Statistics:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Model size: ~{total_params * 4 / (1024**2):.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… GoogleNet working correctly!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05802934",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways: GoogleNet vs ResNet\n",
    "\n",
    "### ğŸ•ğŸ” Remember Our Silly Examples!\n",
    "\n",
    "**ResNet = The Pizza Recipe Chain**\n",
    "- Problem: Information gets corrupted through 50 layers\n",
    "- Solution: Skip connections = Give original recipe directly + collect improvements\n",
    "- Formula: `Output = Original Recipe + Small Changes`\n",
    "\n",
    "**InceptionNet = The Detective Team**\n",
    "- Problem: One magnifying glass size might miss clues\n",
    "- Solution: 4 detectives with different tools working in parallel\n",
    "- Formula: `Output = [Tiny Details, Normal Clues, Big Patterns, Key Evidence]`\n",
    "\n",
    "---\n",
    "\n",
    "### Two Different Solutions to Deep Network Problems\n",
    "\n",
    "| Aspect | **GoogleNet (Inception)** ğŸ” | **ResNet** ğŸ• |\n",
    "|--------|---------------------------|------------|\n",
    "| **Core Problem** | Computational efficiency | Gradient degradation |\n",
    "| **Solution** | Multi-scale parallel paths | Skip connections |\n",
    "| **Philosophy** | \"Go wider\" (parallel) | \"Go deeper\" (sequential) |\n",
    "| **Feature Combination** | Concatenation | Addition |\n",
    "| **Parameters** | ~7M (very efficient) | ~25M (ResNet-50) |\n",
    "| **Depth** | 22 layers | 50-152 layers |\n",
    "| **Key Innovation** | 1Ã—1 dimensionality reduction | Identity shortcuts |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ­ Drama Club Analogy\n",
    "\n",
    "**GoogleNet**: Like a play where 4 actors perform different versions of the same scene simultaneously, then you pick the best parts from each!\n",
    "\n",
    "**ResNet**: Like writing a story where each chapter adds to the previous one, but you always keep the original chapter 1 as reference!\n",
    "\n",
    "---\n",
    "\n",
    "### What Makes Each Architecture Unique?\n",
    "\n",
    "**GoogleNet's Strength:**\n",
    "```python\n",
    "# Multi-scale in parallel (Detective team!)\n",
    "output = [1Ã—1(x), 3Ã—3(x), 5Ã—5(x), pool(x)]  # Concatenate all scales\n",
    "```\n",
    "- **Best for:** Efficient inference, mobile deployment, multi-scale objects\n",
    "- **Trade-off:** More complex architecture, harder to modify\n",
    "- **Real analogy:** Swiss Army knife - multiple tools in one! ğŸ”ª\n",
    "\n",
    "**ResNet's Strength:**\n",
    "```python\n",
    "# Residual learning (Pizza recipe with improvements!)\n",
    "output = F(x) + x  # Learn only the difference\n",
    "```\n",
    "- **Best for:** Very deep networks, transfer learning, gradient flow\n",
    "- **Trade-off:** More parameters, slightly slower\n",
    "- **Real analogy:** Building blocks - each layer stacks on previous! ğŸ§±\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ® Video Game Analogy\n",
    "\n",
    "**GoogleNet:**\n",
    "```\n",
    "Like playing a game with 4 different characters at once:\n",
    "- Character 1: Speed specialist (1Ã—1 - fast processing)\n",
    "- Character 2: Balanced fighter (3Ã—3 - good all-around)\n",
    "- Character 3: Heavy hitter (5Ã—5 - big moves)\n",
    "- Character 4: Support (pooling - assists others)\n",
    "\n",
    "You control all 4, then combine their powers!\n",
    "```\n",
    "\n",
    "**ResNet:**\n",
    "```\n",
    "Like a game with checkpoints:\n",
    "- Start level\n",
    "- Checkpoint 1: Save progress + improve skills\n",
    "- Checkpoint 2: Save progress + improve more\n",
    "- Checkpoint 3: Save progress + improve more\n",
    "...\n",
    "- End level with ALL improvements + original saves!\n",
    "\n",
    "You never lose your starting position!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Which?\n",
    "\n",
    "**Choose GoogleNet when:**\n",
    "- âœ… Need efficient model (fewer parameters)\n",
    "- âœ… Deploying to resource-constrained devices\n",
    "- âœ… Objects appear at multiple scales\n",
    "- âœ… Want faster inference\n",
    "- ğŸ” Think: \"I need multiple perspectives!\"\n",
    "\n",
    "**Choose ResNet when:**\n",
    "- âœ… Need maximum accuracy\n",
    "- âœ… Doing transfer learning (more pre-trained models available)\n",
    "- âœ… Need very deep network (100+ layers)\n",
    "- âœ… Have sufficient computational resources\n",
    "- ğŸ• Think: \"I need to go REALLY deep!\"\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸª Cookie Classification Example\n",
    "\n",
    "**Problem:** Identify cookie types from photos\n",
    "\n",
    "**GoogleNet approach:**\n",
    "```\n",
    "Detective 1 (1Ã—1): \"Sees sugar crystals on top!\"\n",
    "Detective 2 (3Ã—3): \"Sees chocolate chip pattern!\"\n",
    "Detective 3 (5Ã—5): \"Sees overall round shape!\"\n",
    "Detective 4 (pool): \"Keeps texture information!\"\n",
    "â†’ Combines all clues â†’ \"It's a chocolate chip cookie!\"\n",
    "```\n",
    "\n",
    "**ResNet approach:**\n",
    "```\n",
    "Layer 1: \"Looks like baked goods\" + [original image]\n",
    "Layer 2: \"Has dark spots\" + [layer 1 + original]\n",
    "Layer 3: \"Spots are chocolate\" + [layer 2 + layer 1 + original]\n",
    "...\n",
    "Layer 50: \"Definitely chocolate chip!\" + [all previous layers]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### The Lasting Impact\n",
    "\n",
    "**GoogleNet taught us:**\n",
    "- 1Ã—1 convolutions are powerful (like having a zoom lens!)\n",
    "- Parallel processing captures multi-scale features (detective team!)\n",
    "- Bigger â‰  better; smarter = better\n",
    "\n",
    "**ResNet taught us:**\n",
    "- Skip connections enable very deep networks (preserve the recipe!)\n",
    "- Learning differences is easier than learning transformations (small changes!)\n",
    "- Identity mappings preserve gradient flow (don't lose the original!)\n",
    "\n",
    "**Together, they revolutionized deep learning!** ğŸš€\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸª Final Fun Fact\n",
    "\n",
    "**If neural networks were a circus:**\n",
    "\n",
    "**GoogleNet** = The trapeze act with 4 performers doing different tricks simultaneously, all catching each other perfectly! ğŸ¤¹\n",
    "\n",
    "**ResNet** = The high wire act where each performer stands on the previous one's shoulders, but there's always a safety net (skip connection) back to the ground! ğŸª\n",
    "\n",
    "Modern architectures (EfficientNet, MobileNet, Vision Transformers) are like combining the trapeze act WITH the high wire act - best of both worlds! ğŸ­"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
