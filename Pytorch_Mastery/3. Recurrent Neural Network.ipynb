{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4742bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b06996",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN) Tutorial\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates two different approaches to building RNNs in PyTorch:\n",
    "1. **Method 1**: Manual RNN implementation from scratch\n",
    "2. **Method 2**: Using PyTorch's built-in `nn.RNN` module\n",
    "\n",
    "We'll solve two problems:\n",
    "- **Problem 1**: Name Classification (predicting nationality from names)\n",
    "- **Problem 2**: Image Classification (CIFAR-10 using RNNs)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What is an RNN?\n",
    "A Recurrent Neural Network processes sequential data by maintaining a \"hidden state\" that gets updated at each time step. This allows the network to have \"memory\" of previous inputs.\n",
    "\n",
    "### RNN Processing Flow\n",
    "For a sequence like \"Albert\":\n",
    "1. Process 'A' ‚Üí update hidden state\n",
    "2. Process 'l' with previous hidden state ‚Üí update hidden state\n",
    "3. Process 'b' with previous hidden state ‚Üí update hidden state\n",
    "4. Continue for 'e', 'r', 't'\n",
    "5. Use final hidden state for prediction\n",
    "\n",
    "### Input Shape for RNNs\n",
    "- **Method 1 (Manual)**: Process one time step at a time `(batch_size, features)`\n",
    "- **Method 2 (Built-in)**: Process entire sequence `(batch_size, sequence_length, features)`\n",
    "\n",
    "---\n",
    "\n",
    "## Method 1: Manual RNN Implementation\n",
    "\n",
    "**How it works:**\n",
    "- Manually loop through each character/time step\n",
    "- Concatenate input with previous hidden state\n",
    "- Apply linear transformations\n",
    "- Update hidden state at each step\n",
    "\n",
    "**Advantages:**\n",
    "- Full control over the RNN cell\n",
    "- Easy to understand the mechanics\n",
    "- Can customize each step\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires manual looping\n",
    "- Slower for long sequences\n",
    "- More code to write\n",
    "\n",
    "---\n",
    "\n",
    "## Method 2: PyTorch Built-in RNN\n",
    "\n",
    "**How it works:**\n",
    "- PyTorch's `nn.RNN` handles all the looping internally\n",
    "- Pass entire sequence at once\n",
    "- Automatically returns outputs for all time steps\n",
    "- We take the last time step for classification\n",
    "\n",
    "**Advantages:**\n",
    "- Highly optimized (faster)\n",
    "- Less code\n",
    "- Easier to scale\n",
    "\n",
    "**Disadvantages:**\n",
    "- Less control over internal mechanics\n",
    "- Slightly more abstract\n",
    "\n",
    "---\n",
    "\n",
    "## Data Representation\n",
    "\n",
    "### One-Hot Encoding (Text)\n",
    "Text is converted to one-hot vectors:\n",
    "- \"a\" ‚Üí [1, 0, 0, ..., 0] (58 dimensions)\n",
    "- Each character gets a unique position set to 1\n",
    "\n",
    "### Sequence Tensor Shape\n",
    "For a name like \"Albert\":\n",
    "- Shape: `(6, 1, 58)`\n",
    "  - 6 = sequence length (6 characters)\n",
    "  - 1 = batch size (processing one name)\n",
    "  - 58 = vocabulary size (one-hot encoding)\n",
    "\n",
    "### Image as Sequence (CIFAR-10)\n",
    "CIFAR-10 images (3√ó32√ó32) are treated as sequences:\n",
    "- Each row becomes a time step\n",
    "- Shape transformation: `(batch, 3, 32, 32)` ‚Üí `(batch, 32, 96)`\n",
    "  - 32 time steps (rows)\n",
    "  - 96 features (32 pixels √ó 3 colors)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697dbaa",
   "metadata": {},
   "source": [
    "# Problem 1: Classification of Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4a4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# We can use \"_\" to represent an out-of-vocabulary character, that is, any character we are not handling in our model\n",
    "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
    "n_letters = len(allowed_characters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in allowed_characters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fec989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting '≈ölus√†rski' to Slusarski\n"
     ]
    }
   ],
   "source": [
    "print (f\"converting '≈ölus√†rski' to {unicodeToAscii('≈ölus√†rski')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88dcae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    # return our out-of-vocabulary character if we encounter a letter unknown to our model\n",
    "    if letter not in allowed_characters:\n",
    "        return allowed_characters.find(\"_\")\n",
    "    else:\n",
    "        return allowed_characters.find(letter)\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1d3581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter 'a' becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]])\n",
      "The name 'anc' becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print (f\"The letter 'a' becomes {lineToTensor('a')}\") #notice that the first position in the tensor = 1\n",
    "print (f\"The name 'anc' becomes {lineToTensor('anc')}\") #notice 'A' sets the 27th index to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02be812",
   "metadata": {},
   "source": [
    "# Problem 2: Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d3f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "# Transform: Convert image to tensor and normalize (0‚Äì1 range)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))   # normalize to mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "# Download MNIST training & test datasets\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1440f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8d7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir #for provenance of the dataset\n",
    "        self.load_time = time.localtime #for provenance of the dataset\n",
    "        labels_set = set() #set of all classes\n",
    "\n",
    "        self.data = []\n",
    "        self.data_tensors = []\n",
    "        self.labels = []\n",
    "        self.labels_tensors = []\n",
    "\n",
    "        #read all the ``.txt`` files in the specified directory\n",
    "        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in text_files:\n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            labels_set.add(label)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for name in lines:\n",
    "                self.data.append(name)\n",
    "                self.data_tensors.append(lineToTensor(name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "        #Cache the tensor representation of the labels\n",
    "        self.labels_uniq = list(labels_set)\n",
    "        for idx in range(len(self.labels)):\n",
    "            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
    "            self.labels_tensors.append(temp_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        data_tensor = self.data_tensors[idx]\n",
    "        label_tensor = self.labels_tensors[idx]\n",
    "\n",
    "        return label_tensor, data_tensor, data_label, data_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "285c4cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 20074 items of data\n",
      "example = (tensor([2]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]]), 'Arabic', 'Khoury')\n"
     ]
    }
   ],
   "source": [
    "alldata = NamesDataset(\"data/names\")\n",
    "print(f\"loaded {len(alldata)} items of data\")\n",
    "print(f\"example = {alldata[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d384845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = cpu\n",
      "train examples = 17063, validation examples = 3011\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using device = {torch.get_default_device()}\")\n",
    "train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\n",
    "\n",
    "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89997f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"asset/rnn_workflow.png\" alt=\"Rnn Flow\" width=\"300\" height=\"300\">\n",
    "  <figcaption>How data is processed inside of RNN</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdc0f6",
   "metadata": {},
   "source": [
    "# Method 1: Manual RNN Implementation\n",
    "\n",
    "## Architecture Explanation\n",
    "\n",
    "This RNN processes sequences **one time step at a time**:\n",
    "\n",
    "### Components:\n",
    "1. **`input_to_hidden`**: Combines current input with previous hidden state\n",
    "   - Input size: `input_size + hidden_size`\n",
    "   - Output size: `hidden_size`\n",
    "   \n",
    "2. **`input_to_output`**: Produces output predictions\n",
    "   - Input size: `input_size + hidden_size`\n",
    "   - Output size: `output_size` (number of classes)\n",
    "   \n",
    "3. **`softmax`**: Converts outputs to log probabilities\n",
    "\n",
    "### Forward Pass:\n",
    "```python\n",
    "# For each character in the sequence:\n",
    "combined = concat(input_char, hidden_state)  # Combine input with memory\n",
    "hidden_state = linear(combined)              # Update memory\n",
    "output = linear(combined)                     # Compute output\n",
    "output = log_softmax(output)                  # Convert to probabilities\n",
    "```\n",
    "\n",
    "### Usage Pattern:\n",
    "```python\n",
    "hidden = model.initHidden()\n",
    "for char in sequence:\n",
    "    output, hidden = model(char, hidden)\n",
    "# Use final output for prediction\n",
    "```\n",
    "\n",
    "**Key Point**: You must manually loop through the sequence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fbefa2",
   "metadata": {},
   "source": [
    "\n",
    "## Why Combine Input + Hidden State?\n",
    "\n",
    "The core idea of an RNN is to use **both current information AND past information** to make decisions. Here's the intuition:\n",
    "\n",
    "### The Concept\n",
    "\n",
    "When reading a sequence like \"Albert\":\n",
    "- At time step 1 ('A'): We have no history, just the letter 'A'\n",
    "- At time step 2 ('l'): We need to remember we saw 'A' before\n",
    "- At time step 3 ('b'): We need to remember 'Al' came before\n",
    "- And so on...\n",
    "\n",
    "### How Concatenation Achieves This\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = torch.cat((input_tensor, hidden_tensor), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4b713",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This creates a **single vector** containing:\n",
    "1. **Current input** - What we're seeing NOW (e.g., current character)\n",
    "2. **Hidden state** - What we've seen BEFORE (memory from previous time steps)\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "Let's say:\n",
    "- `input_size = 58` (one-hot encoded character)\n",
    "- `hidden_size = 128` (memory size)\n",
    "\n",
    "At each time step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16922626",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Current character 'l': [0, 0, 1, 0, ..., 0]  (58 dimensions)\n",
    "                       ‚Üì\n",
    "              Concatenate with\n",
    "                       ‚Üì\n",
    "Previous memory:       [0.3, -0.5, 0.8, ...]  (128 dimensions)\n",
    "                       ‚Üì\n",
    "              Results in:\n",
    "                       ‚Üì\n",
    "Combined vector:       [0, 0, 1, ..., 0, 0.3, -0.5, 0.8, ...]  (186 dimensions)\n",
    "                       ‚Üë____________‚Üë  ‚Üë___________________‚Üë\n",
    "                         input(58)      hidden_state(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6952dfc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Why This Design?\n",
    "\n",
    "**The linear layer needs BOTH pieces of information** to decide:\n",
    "1. **What to remember next** (update hidden state)\n",
    "2. **What to output** (prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83492025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network learns to say things like:\n",
    "# \"If I see 'e' AND the previous letters were 'Alb', \n",
    "#  then this is probably a name, update memory accordingly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab440bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Mathematical Perspective\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_new = f(input_current + hidden_previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1956c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is the fundamental RNN equation. The concatenation is just how we implement the \"+\" operation - we stack the vectors and let a linear layer learn how to combine them.\n",
    "\n",
    "### Alternative: Why Not Add Them?\n",
    "\n",
    "You might wonder: \"Why concatenate instead of adding?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e325670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not this?\n",
    "combined = input_tensor + hidden_tensor  # ‚ùå Won't work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa118cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Problem**: They have different dimensions! (58 vs 128)\n",
    "\n",
    "Even if we made them the same size, **concatenation is better** because:\n",
    "- ‚úÖ Preserves all information from both sources\n",
    "- ‚úÖ Lets the network learn how much weight to give each\n",
    "- ‚úÖ More flexible - the network decides how to combine them\n",
    "\n",
    "### In PyTorch's Built-in RNN\n",
    "\n",
    "PyTorch's `nn.RNN` does the same thing internally, but you don't see it because it's hidden inside the optimized implementation. It still combines input + hidden state behind the scenes!\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway**: Concatenating input with hidden state is how RNNs achieve their \"memory\" - they always consider both what they're seeing NOW and what they've seen BEFORE to make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32cc72",
   "metadata": {},
   "source": [
    "## Code-Level Comparison: Manual vs Built-in RNN\n",
    "\n",
    "| Aspect | Method 1: Manual RNN | Method 2: Built-in RNN |\n",
    "|--------|---------------------|------------------------|\n",
    "| **Class Definition** | `class RNNModel(nn.Module)` | `class RNN(nn.Module)` |\n",
    "| **Layers Defined** | ‚Ä¢ `nn.Linear(input+hidden, hidden)`<br>‚Ä¢ `nn.Linear(input+hidden, output)`<br>‚Ä¢ `nn.LogSoftmax(dim=1)` | ‚Ä¢ `nn.RNN(input, hidden, layers, batch_first=True)`<br>‚Ä¢ `nn.Linear(hidden, output)` |\n",
    "| **Initialization** | `self.input_to_hidden`<br>`self.input_to_hidden`<br>`self.softmax` | `self.rnn`<br>`self.fc`<br>`self.hidden_size`, `self.num_layers` |\n",
    "| **Forward Input** | `forward(input_tensor, hidden_tensor)` | `forward(x)` |\n",
    "| **Hidden State Init** | `initHidden()` returns `torch.zeros(1, hidden_size)` | `h0 = torch.zeros(num_layers, batch_size, hidden_size)` inside forward |\n",
    "| **Input Shape** | `(1, input_size)` - single time step | `(batch, seq_len, input_size)` - entire sequence |\n",
    "| **Processing** | ```python<br>combined = cat(input, hidden)<br>hidden = linear(combined)<br>output = linear(combined)<br>output = log_softmax(output)<br>``` | ```python<br>h0 = zeros(...)<br>out, _ = self.rnn(x, h0)<br>out = out[:, -1, :]<br>out = self.fc(out)<br>``` |\n",
    "| **Output Shape** | `(1, output_size)` | `(batch, output_size)` |\n",
    "| **Returns** | `output, hidden_state` (tuple) | `output` (single tensor) |\n",
    "| **Manual Looping** | ‚úÖ **Required** - Must loop externally | ‚ùå **Not Required** - Handles internally |\n",
    "| **Usage Example** | ```python<br>hidden = model.initHidden()<br>for i in range(seq_len):<br>    out, hidden = model(x[i], hidden)<br>``` | ```python<br>output = model(sequence)<br>``` |\n",
    "| **Concatenation** | `torch.cat((input, hidden), 1)` - **Explicit** | Hidden inside `nn.RNN` - **Implicit** |\n",
    "| **Activation** | `LogSoftmax` (for NLLLoss) | None (for CrossEntropyLoss) |\n",
    "| **Time Step Output** | Only final output available | All time steps available (`out[:, -1, :]` takes last) |\n",
    "| **Hidden State Access** | ‚úÖ After each step | ‚úÖ Final state returned (ignored with `_`) |\n",
    "| **Flexibility** | Can customize each step | Standard RNN behavior only |\n",
    "| **Code Lines** | ~18 lines | ~16 lines |\n",
    "| **Memory Efficient** | Process one at a time | Batch processing |\n",
    "| **GPU Optimization** | Standard operations | Highly optimized CUDA kernels |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Architectural Differences\n",
    "\n",
    "#### **1. Layer Construction**\n",
    "\n",
    "**Method 1 (Manual):**\n",
    "```python\n",
    "self.input_to_hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "self.input_to_output = nn.Linear(input_size + hidden_size, output_size)\n",
    "self.softmax = nn.LogSoftmax(dim=1)\n",
    "```\n",
    "\n",
    "**Method 2 (Built-in):**\n",
    "```python\n",
    "self.rnn = nn.RNN(input_size, hidden_size, num_rnn_layers, batch_first=True)\n",
    "self.fc = nn.Linear(hidden_size, output_size)\n",
    "```\n",
    "\n",
    "#### **2. Forward Pass Logic**\n",
    "\n",
    "**Method 1 (Manual):**\n",
    "```python\n",
    "def forward(self, input_tensor, hidden_tensor):\n",
    "    combined = torch.cat((input_tensor, hidden_tensor), 1)  # Manually combine\n",
    "    hidden_state = self.input_to_hidden(combined)           # Update hidden\n",
    "    output = self.input_to_output(combined)                 # Compute output\n",
    "    output = self.softmax(output)                           # Apply softmax\n",
    "    return output, hidden_state                             # Return both\n",
    "```\n",
    "\n",
    "**Method 2 (Built-in):**\n",
    "```python\n",
    "def forward(self, x):\n",
    "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "    out, _ = self.rnn(x, h0)        # RNN handles all time steps\n",
    "    out = out[:, -1, :]             # Extract last time step\n",
    "    out = self.fc(out)              # Classify\n",
    "    return out                      # Return only output\n",
    "```\n",
    "\n",
    "#### **3. Training Loop Differences**\n",
    "\n",
    "**Method 1 Usage:**\n",
    "```python\n",
    "for i in batch:\n",
    "    hidden = rnn.initHidden()                    # Initialize hidden\n",
    "    for j in range(text_tensor.size()[0]):       # Loop through sequence\n",
    "        output, hidden = rnn(text_tensor[j], hidden)  # One step at a time\n",
    "    loss = criterion(output, label)              # Use final output\n",
    "```\n",
    "\n",
    "**Method 2 Usage:**\n",
    "```python\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs = inputs.reshape(-1, seq_len, input_size)  # Reshape to sequence\n",
    "    outputs = rnn(inputs)                              # Process entire batch\n",
    "    loss = criterion(outputs, labels)                  # Compute loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Metric | Method 1 | Method 2 |\n",
    "|--------|----------|----------|\n",
    "| **Speed** | ~10-50x slower | Baseline (fastest) |\n",
    "| **Memory** | Lower (one step) | Higher (full sequence) |\n",
    "| **Batch Size** | Effectively 1 | Full batch (64, 128, etc.) |\n",
    "| **Parallelization** | Limited | Excellent |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use Method 1 (Manual) when:**\n",
    "- üéì Learning RNN internals\n",
    "- üîß Need custom RNN cell behavior\n",
    "- üìä Want step-by-step inspection\n",
    "- üß™ Prototyping new architectures\n",
    "\n",
    "**Use Method 2 (Built-in) when:**\n",
    "- üöÄ Production code\n",
    "- ‚ö° Speed matters\n",
    "- üì¶ Standard RNN behavior is sufficient\n",
    "- üíº Working with large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c372e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_to_hidden=nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_output=nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax=nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        combined=torch.cat((input_tensor, hidden_tensor), 1)\n",
    "        \n",
    "        hidden_state=self.input_to_hidden(combined)\n",
    "        output=self.input_to_output(combined)\n",
    "        output=self.softmax(output)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3511f",
   "metadata": {},
   "source": [
    "# Method 2: PyTorch Built-in RNN\n",
    "\n",
    "## Architecture Explanation\n",
    "\n",
    "This RNN uses PyTorch's optimized `nn.RNN` module that processes **entire sequences at once**:\n",
    "\n",
    "### Components:\n",
    "1. **`self.rnn`**: PyTorch's RNN layer\n",
    "   - `input_size`: Features per time step\n",
    "   - `hidden_size`: Size of hidden state\n",
    "   - `num_layers`: Number of stacked RNN layers\n",
    "   - `batch_first=True`: Input shape is `(batch, sequence, features)`\n",
    "   \n",
    "2. **`self.fc`**: Fully connected layer for classification\n",
    "   - Input size: `hidden_size`\n",
    "   - Output size: `output_size` (number of classes)\n",
    "\n",
    "### Forward Pass:\n",
    "```python\n",
    "# Initialize hidden state\n",
    "h0 = zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "# Process entire sequence at once\n",
    "out, hidden = self.rnn(x, h0)\n",
    "# out shape: (batch, sequence_length, hidden_size)\n",
    "\n",
    "# Take only the last time step\n",
    "out = out[:, -1, :]  # Shape: (batch, hidden_size)\n",
    "\n",
    "# Classify\n",
    "out = self.fc(out)   # Shape: (batch, num_classes)\n",
    "```\n",
    "\n",
    "### Key Differences from Method 1:\n",
    "- ‚úÖ No manual looping required\n",
    "- ‚úÖ Much faster (optimized CUDA kernels)\n",
    "- ‚úÖ Returns outputs for ALL time steps\n",
    "- ‚úÖ We extract the last time step `out[:, -1, :]` for classification\n",
    "\n",
    "### Usage Pattern:\n",
    "```python\n",
    "# Just pass the entire sequence!\n",
    "output = model(sequence)  # sequence shape: (batch, seq_len, features)\n",
    "```\n",
    "\n",
    "**Why take the last time step?**\n",
    "The last time step's output contains information from the entire sequence (all previous time steps), making it ideal for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa132d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_rnn_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_rnn_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_rnn_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # out shape: (batch_size, sequence_length, hidden_size)\n",
    "        # We only need the output from the last time step\n",
    "        out = out[:, -1, :]  # Take the last time step: (batch_size, hidden_size)\n",
    "        out = self.fc(out)  # (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74116d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(96, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 images are 3x32x32 (3 color channels, 32x32 pixels)\n",
    "# We'll treat each row as a sequence: 32 rows, each with 32*3=96 features\n",
    "input_size = 32 * 3  # 96 features per time step (32 pixels x 3 color channels)\n",
    "sequence_length = 32  # 32 rows (time steps)\n",
    "num_rnn_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "rnn = RNN(input_size, hidden_size, num_rnn_layers, num_classes)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "508a5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0b0c1",
   "metadata": {},
   "source": [
    "## Why Permute and Reshape for Images?\n",
    "\n",
    "### The Problem: Image Format vs Sequence Format\n",
    "\n",
    "**CIFAR-10 images come in this shape:**\n",
    "- `(batch, channels, height, width)` = `(64, 3, 32, 32)`\n",
    "- This is the **standard PyTorch image format**\n",
    "- Channels first: RGB values are separated\n",
    "\n",
    "**But RNN expects sequences in this shape:**\n",
    "- `(batch, sequence_length, features)` = `(64, 32, 96)`\n",
    "- Each time step should have ALL features for that step\n",
    "\n",
    "### Visual Explanation\n",
    "\n",
    "**Original Image Shape: (3, 32, 32)**\n",
    "```\n",
    "Channel dimension:\n",
    "‚îú‚îÄ Red channel:    32√ó32 pixels\n",
    "‚îú‚îÄ Green channel:  32√ó32 pixels  \n",
    "‚îî‚îÄ Blue channel:   32√ó32 pixels\n",
    "```\n",
    "\n",
    "**We want to treat each ROW as a time step:**\n",
    "- Row 1: All RGB values from the first row (32 pixels √ó 3 colors = 96 features)\n",
    "- Row 2: All RGB values from the second row (96 features)\n",
    "- Row 3: All RGB values from the third row (96 features)\n",
    "- ... up to Row 32\n",
    "\n",
    "### Why Two Steps?\n",
    "\n",
    "#### Step 1: **Permute** `(batch, 3, 32, 32)` ‚Üí `(batch, 32, 32, 3)`\n",
    "```python\n",
    "inputs = inputs.permute(0, 2, 3, 1)\n",
    "```\n",
    "- **What it does**: Moves channels from dimension 1 to dimension 3\n",
    "- **Why**: We need RGB values together for each pixel\n",
    "- **Result**: Now each pixel has its 3 color values grouped together\n",
    "\n",
    "#### Step 2: **Reshape** `(batch, 32, 32, 3)` ‚Üí `(batch, 32, 96)`\n",
    "```python\n",
    "inputs = inputs.reshape(-1, sequence_length, input_size)\n",
    "```\n",
    "- **What it does**: Flattens each row into a single feature vector\n",
    "- **Why**: RNN needs `(batch, seq_len, features)` format\n",
    "- **Result**: 32 rows (time steps), each with 96 features (32 pixels √ó 3 colors)\n",
    "\n",
    "### Comparison with Text Sequences\n",
    "\n",
    "**Your earlier image (text sequences):**\n",
    "```python\n",
    "# Text already comes as sequences!\n",
    "text_tensor = lineToTensor(\"Albert\")  # Shape: (6, 1, 58)\n",
    "# 6 time steps (characters), already in sequence format\n",
    "# NO permute/reshape needed! ‚úì\n",
    "```\n",
    "\n",
    "**CIFAR-10 images (need conversion):**\n",
    "```python\n",
    "# Images come as (batch, C, H, W), NOT as sequences\n",
    "images = load_cifar()  # Shape: (64, 3, 32, 32)\n",
    "# Must convert to sequence format\n",
    "images = images.permute(0, 2, 3, 1)  # ‚Üê Reorder dimensions\n",
    "images = images.reshape(-1, 32, 96)   # ‚Üê Flatten into sequences\n",
    "# NOW it's (64, 32, 96) - ready for RNN! ‚úì\n",
    "```\n",
    "\n",
    "### Key Difference\n",
    "\n",
    "| Data Type | Original Format | Needs Conversion? | Why? |\n",
    "|-----------|----------------|-------------------|------|\n",
    "| **Text** | Already sequential (character-by-character) | ‚ùå No | Text is naturally a sequence |\n",
    "| **Images** | Grid format (height √ó width √ó channels) | ‚úÖ Yes | Images are 2D grids, not sequences |\n",
    "\n",
    "### What if we skip permute?\n",
    "\n",
    "```python\n",
    "# ‚ùå WRONG: Skip permute, just reshape\n",
    "images.reshape(-1, 32, 96)  # (64, 3, 32, 32) ‚Üí (64, 32, 96)\n",
    "# Problem: This would group pixels incorrectly!\n",
    "# Row 1 would have: all reds from first 10 pixels, then some greens...\n",
    "# The RGB values wouldn't be together!\n",
    "```\n",
    "\n",
    "**Correct way with permute:**\n",
    "```python\n",
    "# ‚úì RIGHT: Permute then reshape  \n",
    "images.permute(0, 2, 3, 1).reshape(-1, 32, 96)\n",
    "# Row 1: [R‚ÇÅ, G‚ÇÅ, B‚ÇÅ, R‚ÇÇ, G‚ÇÇ, B‚ÇÇ, ..., R‚ÇÉ‚ÇÇ, G‚ÇÉ‚ÇÇ, B‚ÇÉ‚ÇÇ]\n",
    "# Each pixel's RGB values stay together!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54871b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Understanding Image to Sequence Conversion\n",
      "======================================================================\n",
      "\n",
      "1. Original image shape: torch.Size([1, 3, 4, 4])\n",
      "   Format: (batch, channels, height, width)\n",
      "   This is how PyTorch stores images: RGB separated\n",
      "\n",
      "2. After permute(0, 2, 3, 1): torch.Size([1, 4, 4, 3])\n",
      "   Format: (batch, height, width, channels)\n",
      "   Now RGB values are together for each pixel\n",
      "\n",
      "3. After reshape(1, 4, 12): torch.Size([1, 4, 12])\n",
      "   Format: (batch, sequence_length, features)\n",
      "   Each row is now a time step with all pixel RGB values\n",
      "\n",
      "======================================================================\n",
      "For CIFAR-10:\n",
      "  (64, 3, 32, 32) ‚Üí permute ‚Üí (64, 32, 32, 3) ‚Üí reshape ‚Üí (64, 32, 96)\n",
      "  64 images, 32 time steps, 96 features per step\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Why we need permute + reshape\n",
    "print(\"=\" * 70)\n",
    "print(\"Understanding Image to Sequence Conversion\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a small example image (1 batch, 3 channels, 4x4 pixels)\n",
    "example_image = torch.randn(1, 3, 4, 4)\n",
    "print(f\"\\n1. Original image shape: {example_image.shape}\")\n",
    "print(f\"   Format: (batch, channels, height, width)\")\n",
    "print(f\"   This is how PyTorch stores images: RGB separated\")\n",
    "\n",
    "# Step 1: Permute\n",
    "permuted = example_image.permute(0, 2, 3, 1)\n",
    "print(f\"\\n2. After permute(0, 2, 3, 1): {permuted.shape}\")\n",
    "print(f\"   Format: (batch, height, width, channels)\")\n",
    "print(f\"   Now RGB values are together for each pixel\")\n",
    "\n",
    "# Step 2: Reshape\n",
    "sequence = permuted.reshape(1, 4, 12)  # 4 rows, 12 features (4 pixels √ó 3 colors)\n",
    "print(f\"\\n3. After reshape(1, 4, 12): {sequence.shape}\")\n",
    "print(f\"   Format: (batch, sequence_length, features)\")\n",
    "print(f\"   Each row is now a time step with all pixel RGB values\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"For CIFAR-10:\")\n",
    "print(f\"  (64, 3, 32, 32) ‚Üí permute ‚Üí (64, 32, 32, 3) ‚Üí reshape ‚Üí (64, 32, 96)\")\n",
    "print(f\"  64 images, 32 time steps, 96 features per step\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc52d23",
   "metadata": {},
   "source": [
    "## Direct Comparison: Text vs Images\n",
    "\n",
    "### Text Sequences (Your earlier example) ‚úÖ No preprocessing needed\n",
    "\n",
    "```python\n",
    "# Text is already sequential!\n",
    "name = \"Albert\"\n",
    "text_tensor = lineToTensor(name)\n",
    "print(text_tensor.shape)  # (6, 1, 58)\n",
    "                          # ‚Üë  ‚Üë  ‚Üë\n",
    "                          # |  |  ‚îî‚îÄ Features (58 one-hot characters)\n",
    "                          # |  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ Batch size (1 name)\n",
    "                          # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Sequence length (6 characters)\n",
    "\n",
    "# Feed directly to RNN - it's already in sequence format!\n",
    "for i in range(text_tensor.size()[0]):\n",
    "    output, hidden = rnn(text_tensor[i], hidden)\n",
    "```\n",
    "\n",
    "**Why no preprocessing?** Text is inherently sequential. Each character is already a time step.\n",
    "\n",
    "---\n",
    "\n",
    "### Image Sequences (CIFAR-10) ‚ùå Needs preprocessing\n",
    "\n",
    "```python\n",
    "# Images come in grid format, NOT sequential!\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  # (64, 3, 32, 32)\n",
    "                     # ‚Üë   ‚Üë  ‚Üë   ‚Üë\n",
    "                     # |   |  |   ‚îî‚îÄ Width (32 pixels)\n",
    "                     # |   |  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Height (32 pixels)\n",
    "                     # |   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Channels (R, G, B)\n",
    "                     # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Batch size (64 images)\n",
    "\n",
    "# ‚ùå Can't feed directly to RNN - wrong format!\n",
    "# ‚úÖ Must convert to sequence format first:\n",
    "\n",
    "# Step 1: Move channels to last dimension\n",
    "images = images.permute(0, 2, 3, 1)  # (64, 32, 32, 3)\n",
    "\n",
    "# Step 2: Flatten each row into features\n",
    "images = images.reshape(-1, 32, 96)  # (64, 32, 96)\n",
    "                                     # ‚Üë   ‚Üë   ‚Üë\n",
    "                                     # |   |   ‚îî‚îÄ Features (32 pixels √ó 3 colors)\n",
    "                                     # |   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Sequence length (32 rows)\n",
    "                                     # ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄBatch size (64 images)\n",
    "\n",
    "# NOW it's in sequence format - ready for RNN!\n",
    "output = rnn(images)\n",
    "```\n",
    "\n",
    "**Why preprocessing?** Images are 2D grids, not sequences. We artificially treat rows as time steps.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Aspect | Text | Images |\n",
    "|--------|------|--------|\n",
    "| **Natural format** | Sequential (character-by-character) | Grid (2D array) |\n",
    "| **As received** | Already sequence-like | Grid format `(B, C, H, W)` |\n",
    "| **Preprocessing** | None needed | `permute()` + `reshape()` |\n",
    "| **Why different?** | Text IS a sequence | Images BECOME sequences |\n",
    "| **What's a time step?** | One character | One row of pixels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d662ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [0/782], Loss: 2.3007\n",
      "Epoch [1/5], Step [100/782], Loss: 2.3150\n",
      "Epoch [1/5], Step [100/782], Loss: 2.3150\n",
      "Epoch [1/5], Step [200/782], Loss: 2.2998\n",
      "Epoch [1/5], Step [200/782], Loss: 2.2998\n",
      "Epoch [1/5], Step [300/782], Loss: 2.2700\n",
      "Epoch [1/5], Step [300/782], Loss: 2.2700\n",
      "Epoch [1/5], Step [400/782], Loss: 2.2978\n",
      "Epoch [1/5], Step [400/782], Loss: 2.2978\n",
      "Epoch [1/5], Step [500/782], Loss: 2.3258\n",
      "Epoch [1/5], Step [500/782], Loss: 2.3258\n",
      "Epoch [1/5], Step [600/782], Loss: 2.2951\n",
      "Epoch [1/5], Step [600/782], Loss: 2.2951\n",
      "Epoch [1/5], Step [700/782], Loss: 2.3128\n",
      "Epoch [1/5], Step [700/782], Loss: 2.3128\n",
      "Epoch [1/5] completed, Loss: 2.2672\n",
      "Epoch [2/5], Step [0/782], Loss: 2.3111\n",
      "Epoch [1/5] completed, Loss: 2.2672\n",
      "Epoch [2/5], Step [0/782], Loss: 2.3111\n",
      "Epoch [2/5], Step [100/782], Loss: 2.3248\n",
      "Epoch [2/5], Step [100/782], Loss: 2.3248\n",
      "Epoch [2/5], Step [200/782], Loss: 2.2859\n",
      "Epoch [2/5], Step [200/782], Loss: 2.2859\n",
      "Epoch [2/5], Step [300/782], Loss: 2.3401\n",
      "Epoch [2/5], Step [300/782], Loss: 2.3401\n",
      "Epoch [2/5], Step [400/782], Loss: 2.2917\n",
      "Epoch [2/5], Step [400/782], Loss: 2.2917\n",
      "Epoch [2/5], Step [500/782], Loss: 2.3066\n",
      "Epoch [2/5], Step [500/782], Loss: 2.3066\n",
      "Epoch [2/5], Step [600/782], Loss: 2.3214\n",
      "Epoch [2/5], Step [600/782], Loss: 2.3214\n",
      "Epoch [2/5], Step [700/782], Loss: 2.3225\n",
      "Epoch [2/5], Step [700/782], Loss: 2.3225\n",
      "Epoch [2/5] completed, Loss: 2.2788\n",
      "Epoch [3/5], Step [0/782], Loss: 2.3112\n",
      "Epoch [2/5] completed, Loss: 2.2788\n",
      "Epoch [3/5], Step [0/782], Loss: 2.3112\n",
      "Epoch [3/5], Step [100/782], Loss: 2.3111\n",
      "Epoch [3/5], Step [100/782], Loss: 2.3111\n",
      "Epoch [3/5], Step [200/782], Loss: 2.3152\n",
      "Epoch [3/5], Step [200/782], Loss: 2.3152\n",
      "Epoch [3/5], Step [300/782], Loss: 2.3290\n",
      "Epoch [3/5], Step [300/782], Loss: 2.3290\n",
      "Epoch [3/5], Step [400/782], Loss: 2.3084\n",
      "Epoch [3/5], Step [400/782], Loss: 2.3084\n",
      "Epoch [3/5], Step [500/782], Loss: 2.2967\n",
      "Epoch [3/5], Step [500/782], Loss: 2.2967\n",
      "Epoch [3/5], Step [600/782], Loss: 2.3195\n",
      "Epoch [3/5], Step [600/782], Loss: 2.3195\n",
      "Epoch [3/5], Step [700/782], Loss: 2.3007\n",
      "Epoch [3/5], Step [700/782], Loss: 2.3007\n",
      "Epoch [3/5] completed, Loss: 2.3033\n",
      "Epoch [4/5], Step [0/782], Loss: 2.2763\n",
      "Epoch [3/5] completed, Loss: 2.3033\n",
      "Epoch [4/5], Step [0/782], Loss: 2.2763\n",
      "Epoch [4/5], Step [100/782], Loss: 2.3133\n",
      "Epoch [4/5], Step [100/782], Loss: 2.3133\n",
      "Epoch [4/5], Step [200/782], Loss: 2.3087\n",
      "Epoch [4/5], Step [200/782], Loss: 2.3087\n",
      "Epoch [4/5], Step [300/782], Loss: 2.3105\n",
      "Epoch [4/5], Step [300/782], Loss: 2.3105\n",
      "Epoch [4/5], Step [400/782], Loss: 2.3081\n",
      "Epoch [4/5], Step [400/782], Loss: 2.3081\n",
      "Epoch [4/5], Step [500/782], Loss: 2.3145\n",
      "Epoch [4/5], Step [500/782], Loss: 2.3145\n",
      "Epoch [4/5], Step [600/782], Loss: 2.3093\n",
      "Epoch [4/5], Step [600/782], Loss: 2.3093\n",
      "Epoch [4/5], Step [700/782], Loss: 2.2995\n",
      "Epoch [4/5], Step [700/782], Loss: 2.2995\n",
      "Epoch [4/5] completed, Loss: 2.3405\n",
      "Epoch [5/5], Step [0/782], Loss: 2.3136\n",
      "Epoch [4/5] completed, Loss: 2.3405\n",
      "Epoch [5/5], Step [0/782], Loss: 2.3136\n",
      "Epoch [5/5], Step [100/782], Loss: 2.3180\n",
      "Epoch [5/5], Step [100/782], Loss: 2.3180\n",
      "Epoch [5/5], Step [200/782], Loss: 2.3177\n",
      "Epoch [5/5], Step [200/782], Loss: 2.3177\n",
      "Epoch [5/5], Step [300/782], Loss: 2.3261\n",
      "Epoch [5/5], Step [300/782], Loss: 2.3261\n",
      "Epoch [5/5], Step [400/782], Loss: 2.3217\n",
      "Epoch [5/5], Step [400/782], Loss: 2.3217\n",
      "Epoch [5/5], Step [500/782], Loss: 2.2951\n",
      "Epoch [5/5], Step [500/782], Loss: 2.2951\n",
      "Epoch [5/5], Step [600/782], Loss: 2.3087\n",
      "Epoch [5/5], Step [600/782], Loss: 2.3087\n",
      "Epoch [5/5], Step [700/782], Loss: 2.3087\n",
      "Epoch [5/5], Step [700/782], Loss: 2.3087\n",
      "Epoch [5/5] completed, Loss: 2.3293\n",
      "Finished Training\n",
      "Epoch [5/5] completed, Loss: 2.3293\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # CIFAR-10: (batch, 3, 32, 32) -> (batch, 32, 96)\n",
    "        # Reshape: batch_size x 3 x 32 x 32 -> batch_size x 32 x 96\n",
    "        inputs = inputs.permute(0, 2, 3, 1)  # (batch, 32, 32, 3)\n",
    "        inputs = inputs.reshape(-1, sequence_length, input_size)  # (batch, 32, 96)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = rnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{n_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}] completed, Loss: {loss.item():.4f}')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93362fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.09%\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        # Reshape the same way as in training\n",
    "        X_batch = X_batch.permute(0, 2, 3, 1)  # (batch, 32, 32, 3)\n",
    "        X_batch = X_batch.reshape(-1, sequence_length, input_size)  # (batch, 32, 96)\n",
    "        \n",
    "        outputs = rnn(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b9782",
   "metadata": {},
   "source": [
    "# Method 1 Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3c40e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (input_to_hidden): Linear(in_features=186, out_features=128, bias=True)\n",
      "  (input_to_output): Linear(in_features=186, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 128 # start small\n",
    "model = RNNModel(n_letters, n_hidden, len(alldata.labels_uniq)).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02ae55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8762, -2.9271, -2.8852, -2.9765, -2.8594, -2.8838, -2.7746, -2.8978,\n",
      "         -2.9350, -2.9942, -2.8139, -2.9554, -2.9709, -2.8508, -2.8357, -2.8770,\n",
      "         -2.9392, -2.8074]], grad_fn=<LogSoftmaxBackward0>)\n",
      "('Russian', 6)\n"
     ]
    }
   ],
   "source": [
    "def label_from_output(output, output_labels):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    label_i = top_i[0].item()\n",
    "    return output_labels[label_i], label_i\n",
    "\n",
    "hidden_state = model.initHidden()\n",
    "input = lineToTensor('Albert')\n",
    "\n",
    "# Loop through each character in the sequence\n",
    "for i in range(input.size()[0]):\n",
    "    output, hidden_state = model(input[i], hidden_state)\n",
    "    \n",
    "print(output)\n",
    "print(label_from_output(output, alldata.labels_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa5d3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "n_batch_size = 64\n",
    "report_every = 50\n",
    "learning_rate = 0.2\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9386d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def train(rnn, training_data):\n",
    "    \"\"\"\n",
    "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
    "    \"\"\"\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    model.train()\n",
    "    print(f\"training on data set with n = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        model.zero_grad() # clear the gradients\n",
    "\n",
    "        # create some minibatches\n",
    "        # we cannot use dataloaders because each of our names is a different length\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, len(batches) //n_batch_size )\n",
    "\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch: #for each example in this batch\n",
    "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
    "                \n",
    "                # Initialize hidden state for this example\n",
    "                hidden = rnn.initHidden()\n",
    "                \n",
    "                # Loop through each character in the sequence\n",
    "                for j in range(text_tensor.size()[0]):\n",
    "                    output, hidden = rnn(text_tensor[j], hidden)\n",
    "                \n",
    "                loss = criterion(output, label_tensor)\n",
    "                batch_loss += loss\n",
    "\n",
    "            # optimize parameters\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "\n",
    "        all_losses.append(current_loss / len(batches) )\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbe5935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on data set with n = 17063\n",
      "training took 578.5927290916443s\n",
      "training took 578.5927290916443s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_losses = train(model, train_set)\n",
    "end = time.time()\n",
    "print(f\"training took {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa01340e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAALqpJREFUeJzt3Qd01fX9//F39t4hISEJgQCCAiFMkaFUHIhUWltXFUSx9ldUlJ6qVGv1XyvVVsUqap24R6tA3VUsBBBBkCB7SCCBDJJA9h7/8/lkkAAJGTf53Hu/z8c533MH98obA7mvfMb741JXV1cnAAAAhria+o0BAAAUwggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAo9zFAdTW1kpGRoYEBASIi4uL6XIAAEA7qL6qRUVFEh0dLa6uro4dRlQQiY2NNV0GAADohPT0dImJiXHsMKJGRBr/MIGBgabLAQAA7VBYWKgHExo/xx06jDROzaggQhgBAMCxnGmJBQtYAQCAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARlk6jHy2LVMWvJciOzIKTJcCAIBlWTqMLNtyRD7cckRW7ckxXQoAAJZl6TAyaVAvfZu8lzACAIAplg4jkweG69vv045LcUW16XIAALAkS4eRvmF+EhvqI1U1dbLhQJ7pcgAAsCRLhxFl0sD6qZo1+3JNlwIAgCVZPow0TtWs2ce6EQAATLB8GBmfEC6uLiI/5pTIkfwy0+UAAGA5lg8jQT4eMiI2WN9fy+gIAAA9zvJhRJnYsG4kmXUjAAD0OMJIs3Uj6/bnSk1tnelyAACwFMKIiCTGBkuAl7vkl1bRGh4AgB5GGBERDzdXGZ8Qpu+zxRcAADsPI8nJyTJjxgyJjo4WFxcXWb58eZuvX7t2rUyYMEHCwsLEx8dHBg8eLE8++aTYm0kNUzW0hgcAoGe5d/QNJSUlkpiYKDfddJP8/Oc/P+Pr/fz85LbbbpPhw4fr+yqc3Hrrrfr+r3/9a7G35meqNXxJRbX4eXX4fw0AAOiEDn/iTps2TV/tlZSUpK9G8fHx8uGHH8qaNWvsKoz0DfPVreHTj5XJhtQ8+cngSNMlAQBgCT2+ZmTLli3yzTffyPnnn9/qayoqKqSwsLDF1d3UlFPj6EjyXtaNAADgdGEkJiZGvLy8ZPTo0TJv3jyZO3duq69dtGiRBAUFNV2xsbE9UiOt4QEAcOIwoqZlNm3aJM8//7wsXrxY3nnnnVZfu3DhQikoKGi60tPTe7w1fAat4QEA6BE9tkqzX79++nbYsGGSnZ0tDz74oFx77bWnfa0aQVGXidbwqufIlrR8WbsvV64a0zMjMgAAWJmRPiO1tbV6XYg9alo3wlQNAAD2OTJSXFws+/fvb3qcmpoqKSkpEhoaKnFxcXqK5ciRI/L666/rX1+yZIl+XvUXaexT8ve//13uuOMOsUdq3cg/Vu6TtQ2t4d3UvA0AALCfMKLWfUyZMqXp8YIFC/Tt7NmzZenSpZKZmSlpaWktRkFUQFGhxd3dXRISEuTRRx/VvUYcoTX88Jj6E30BAED3cKmrq7P7k+HU1l61q0YtZg0MDOz23++W1zfJlzuz5feXnCXzpgzo9t8PAABn1N7Pb86mOQ22+AIA0HMII20sYt18qL41PAAA6D6EkTZaw1fV1OnW8AAAoPsQRlppDT9xAK3hAQDoCYSRVrBuBACAnkEYacV5tIYHAKBHEEZaEeRb3xpeUa3hAQBA9yCMtIHW8AAAdD/CSBsmNawbWbc/V2pr7b43HAAADokw0oYRscHi7+Uux3Vr+ELT5QAA4JQII23wcHOV8Qlh+j5TNQAAdA/CyBmwxRcAgO5FGDkDWsMDANC9CCPtaA0fE0JreAAAugthpB2t4RtHR9bQbwQAAJsjjHRo3QhhBAAAWyOMdKA1/P6jxbSGBwDAxggj7WwNPzyG1vAAAHQHwkhHp2r2E0YAALAlwkg7TRpUv4h17b4cWsMDAGBDhJF2ojU8AADdgzDSTrSGBwCgexBGOnGKL63hAQCwHcJIJ1vDl1bSGh4AAFsgjHRAfPPW8AeOmS4HAACnQBjpZGt41o0AAGAbhJEOojU8AAC2RRjpQmv4zAJawwMA0FWEkS60hmd0BACAriOMdAJTNQAA2A5hpBNoDQ8AgO0QRjqB1vAAANgOYaSTreHP7V/fGn7Nfrb4AgDQFYSRTpo8qGHdyF7WjQAA0BWEkU5qbH626dAxWsMDANAFhJFOojU8AAC2QRjpUmv4+qkaWsMDANB5hBEbTNWspd8IAACdRhjpgvMSwnRr+H20hgcAoNMII10Q7OtJa3gAALqIMNJFtIYHAKBrCCNdNLFh3ci6/bm0hgcAoBMII12UFBcsfp5ucqykUnZm0hoeAICOIozYoDX8+AS2+AIA0FmEERugNTwAAJ1HGLGBiQPqwwit4QEA6DjCiA30C/eTPsENreFTaQ0PAEBHEEZs1BqeqRoAADqHMGLj1vBrWMQKAECHEEZshNbwAAB0DmHEhq3hh9EaHgCADiOMdENreE7xBQCg/Qgj3bBuZC2t4QEAaDfCiA3RGh4AgI4jjNgQreEBAOg4woiNTWLdCAAAHUIY6aYwsungcVrDAwDQDoSRbmoNX1lTS2t4AADagTBiY7SGBwCgYwgj3WDiAFrDAwDQXoSRbjBhQJi4NLSGzyooN10OAAB2jTDSTa3hhze1hmd0BACAthBGurk1POfUAADQNsJIN6E1PAAA7UMY6Sa0hgcAoH0II93aGj5M32eqBgAAG4aR5ORkmTFjhkRHR+ueGsuXL2/z9R9++KFcdNFF0qtXLwkMDJTx48fLF198IVaaqmERKwAANgwjJSUlkpiYKEuWLGl3eFFh5NNPP5XNmzfLlClTdJjZsmWLODtawwMAcGbu0kHTpk3TV3stXry4xeNHHnlEVqxYIR999JEkJSWJFVrDH8kv063hp5wVYbokAADsTo+vGamtrZWioiIJDQ1t9TUVFRVSWFjY4nJEahqLU3wBALCzMPL3v/9diouL5aqrrmr1NYsWLZKgoKCmKzY2VhwV60YAALCjMPL222/LQw89JO+//75ERLQ+ZbFw4UIpKChoutLT08XRW8PvzaY1PAAARsPIu+++K3PnztVBZOrUqW2+1svLS++8aX45KlrDAwBgB2HknXfekTlz5ujb6dOni9VMGkBreAAAbBZG1HqPlJQUfSmpqan6flpaWtMUy6xZs1pMzajHjz/+uIwbN06ysrL0paZfrKJxEes6WsMDAND1MLJp0ya9JbdxW+6CBQv0/QceeEA/zszMbAomygsvvCDV1dUyb948iYqKarrmz58vVpEUF6Jbw+fRGh4AgK73Gbngggukrq71n+6XLl3a4vGqVavE6jzd61vDf7XrqJ6qGdonyHRJAADYDc6m6SFs8QUA4PQIIz1kYrPW8GWVNabLAQDAbhBGekj/htbwlTW1siE1z3Q5AADYDcKIgdbwbPEFAOAEwkgPYt0IAACnIoz0IFrDAwBwKsJIT7eGb9jWu3Y/UzUAACiEkR7GVA0AAC0RRnpY4yLWtftoDQ8AgEIY6WG0hgcAoCXCiIHW8Of2D9P32eILAABhxOxUzX7WjQAAQBgxYNKg+kWs36XSGh4AAMKIAbSGBwDgBMKIAbSGBwDgBMKI4VN81RZfAACsjDBiyISEcN0afk92kWQX0hoeAGBdhBFDQvxOtIZnqgYAYGWEEYNoDQ8AAGHEKFrDAwBAGDHeGt63oTX8rixawwMArIkwYrg1/HhawwMALI4wYtiJfiOsGwEAWBNhxDBawwMArI4wYget4aODvHVr+I0Hj5kuBwCAHkcYsYvW8A1bfPcyVQMAsB7CiB2YNIhzagAA1kUYsQO0hgcAWBlhxA7QGh4AYGWEEbs7xZd1IwAAayGM2InGRaxr99MaHgBgLYQROzGyoTV8bjGt4QEA1kIYsRO0hgcAWBVhxI7QGh4AYEWEETsysWHdyHcHaQ0PALAOwogdSejV0Bq+mtbwAADrIIzYEVrDAwCsiDBiZ2gNDwCwGsKIHbeGP0preACABRBG7LA1/DBawwMALIQwYofY4gsAsBLCiB2iNTwAwEoII3aI1vAAACshjNhpa/hzG1rDr2XdCADAyRFG7H7dCGEEAODcCCN2vm5EdWKlNTwAwJkRRuwUreEBAFZBGLFTtIYHAFgFYcSOTWxYN6K2+AIA4KwII3ZswoD61vC7s2gNDwBwXoQROxZKa3gAgAUQRuwcreEBAM6OMOIwreHzaA0PAHBKhBGHaQ1fodeOAADgbAgjDtQanqkaAIAzIow4AFrDAwCcGWHEwVrDl1fRGh4A4FwIIw7SGj6qsTV8Kq3hAQDOhTDiMK3h2eILAHBOhBEH0XRODetGAABOhjDiIGgNDwBwVoQRB2wNz8F5AABnQhhxIBMHsMUXAOB8CCMOum6E1vAAAGdBGHEgI/sG0xoeAOB0OhxGkpOTZcaMGRIdHa23nC5fvrzN12dmZsp1110ngwYNEldXV7nzzju7Uq+lebm70RoeAOB0OhxGSkpKJDExUZYsWdKu11dUVEivXr3k/vvv1+9D1zT2G2ERKwDAWbh39A3Tpk3TV3vFx8fLU089pe+/8sorHf3t0EoY2ZBa3xre28PNdEkAADjfmhE1mlJYWNjiQr2EXv60hgcAOBW7DCOLFi2SoKCgpis2NtZ0SXaD1vAAAGdjl2Fk4cKFUlBQ0HSlp6ebLsmu0BoeAGDpNSM9wcvLS19oX2v4iEBv0yUBAOBcIyM4c2v4odG0hgcAWDSMFBcXS0pKir6U1NRUfT8tLa1pimXWrFkt3tP4evXenJwcfX/nzp22+jNY0ol1I4QRAIDFpmk2bdokU6ZMaXq8YMECfTt79mxZunSpbnLWGEwaJSUlNd3fvHmzvP3229K3b185ePBg16q3+LqRZ1f9qMNIXV2dXtgKAIAlwsgFF1ygP/xaowLJydp6PWzTGn5IVKDpkgAA6BTWjDhwa/hx/UL1fbb4AgAcGWHEgbHFFwDgDAgjDmzyoJat4QEAcESEEQdGa3gAgDMgjDhJa3j6jQAAHBVhxMFNbFg3kryXRawAAMdEGHFwE09qDQ8AgKMhjDhRa/j/7TlquhwAADqMMOIEpg6J1Ld//niXbDtcYLocAAA6hDDiBG49v79ugFZcUS2zX90o+48Wmy4JAIB2I4w4AW8PN3lp9mgZHhMkx0oq5YaXN8jh46WmywIAoF0II04iwNtDls4ZKwMj/CWzoFyuf2mD5BRVmC4LAIAzIow42WLWN24eJzEhPnIwr1SPkBSUVpkuCwCANhFGnEzvIG958+Zx0ivAS2/3nbN0o5RWVpsuCwCAVhFGnFB8uJ+8cfNYCfLxkO/T8uXWNzZLRTVn1wAA7BNhxEkN7h0or84ZI76ebvpU3zvfTZHqmlrTZQEAcArCiBMbGRciL9wwWjzdXOWz7Vnyh2XbpK6uznRZAAC0QBhxchMHhss/rk0SVxeR9zcdloc/2UUgAQDYFcKIBVw6tLc89otEff/ltany9Nf7TZcEAEATwohF/GJUjDxw+dn6/hNf7pWl61JNlwQAgEYYsZCbJvaTO6cO1Pcf/GinfLD5sOmSAAAgjFjN/AsHypwJ8fr+3R/8IP/dkWW6JACAxRFGLMbFxUX+OP1sPW1TU1snt729Rb7Zn2u6LACAhRFGLMjV1UX++vNhcuk5vaWyplbmvr5JtqQdN10WAMCiCCMW5e7mKk9dO0ImDgiX0soaufHV72RPVpHpsgAAFkQYsTAvdzf55w2jJCkuWArKqvTBeml5pabLAgBYDGHE4vy83GXpjWNlcO8AOVpUIb96+VvJLiw3XRYAwEIII5AgXw95/eax0jfMV9KPlcn1L22Q4yWVpssCAFgEYQRaRIC3vHnzOIkM9JJ9R4vlxlc3SnFFtemyAAAWQBhBk9hQXx1IQnw9ZOvhArnltU1SXlVjuiwAgJMjjKCFgZEB8tpNY8Xfy13WH8jTfUiqampNlwUAcGKEEZxieEywvDhrtHi6u8pXu7Ll7n//ILW1nPQLAOgehBGc1viEMHnuVyPF3dVFlm05Ig99tEPq6ggkAADbI4ygVRcOiZTHr0oUFxeR19Yf0qf9AgBga4QRtOmKEX3k/10xVN9/+uv98tKaA6ZLAgA4GcIIzuiGc/vK7y85S99/+JNd8t53aaZLAgA4EcII2uW3FyTIrZP76/sLP9wmn27LNF0SAMBJEEbQLi4uLnLvtMFy7dhYURtr5r+7RVbvzTFdFgDACRBG0KFA8vDMYTJ9eJRU1dTJrW9skk0Hj5kuCwDg4Agj6BA3Vxd58qoRcsFZvaS8qlbmLP1OdmQUmC4LAODACCPoMNUM7blfjZIx8SFSVF4ts1/ZKAdyik2XBQBwUIQRdIqPp5u8fOMYOSc6UHKLK+WGlzdKRn6Z6bIAAA6IMIJOC/T20OfY9A/3kyP5ZXL9yxskt7jCdFkAAAdDGEGXhPt7yRtzx0l0kLccyCnRUzaF5VWmywIAOBDCCLqsT7CPvDl3nIT5ecqOjEKZu3STlFXWmC4LAOAgCCOwif69/OX1m8dKgLe7bDx4TP7vrc1SWV1ruiwAgAMgjMBmzokOkldvHCPeHq6yak+OLHg/RWpUhzQAANpAGIFNjY4PleevHyUebi7y8Q+Zcv/ybVJXRyABALSOMAKbu+CsCFl8dZK4uoi8szFd/vr5btMlAQDsGGEE3UK1jH/kZ8P0/X+uPiDPrtpvuiQAgJ0ijKDbXDM2Tv5w2WB9/7HP98gb3x4yXRIAwA4RRtCtfj05QW6bMkDff2DFdlmRcsR0SQAAO0MYQbf73cWDZNb4vqLWsf7u/a2ycle26ZIAAHaEMIJu5+LiIg/OOEdmjoiW6to6+e1b38u3B/JMlwUAsBOEEfQIV1cX+dsvE2XqkAipqK6Vua9tkh8O55suCwBgBwgj6DEebq7yzHUj5dz+oVJcUa3PsdmXXWS6LACAYYQR9ChvDzd5afYYGR4TJMdLq+SGlzdK+rFS02UBAAwijKDH+Xu5y9I5Y2VghL9kFZbL9S9vkKNF5abLAgAYQhiBEaF+nvLGzeMkJsRHDuWVyqyXN0pBaZXpsgAABhBGYEzvIG95a+446RXgJbuziuTGpRulpKLadFkAgB5GGIFRfcP85I2bx0qQj4dsScuX37y5WSqqa0yXBQDoQYQRGDe4d6AsnTNGfD3dZM2+XPnFc+tlR0aB6bIAAD2EMAK7kBQXIi/NGi0B3u6y7UiB/PSZdbLos11SVskoCQA4O8II7MZ5A8Jl5YLz9Ym/NbV1+rTfixevluS9OaZLAwB0I8II7EpEoLcsuW6kHiWJCvKW9GNlMuuVjXLXeymSV1xhujwAgD2EkeTkZJkxY4ZER0frM0eWL19+xvesWrVKRo4cKV5eXjJgwABZunRpZ+uFRUw9O1K+XHC+3HhevLi4iCzbckSmPrFaPth8WOrUiXsAAOuGkZKSEklMTJQlS5a06/Wpqakyffp0mTJliqSkpMidd94pc+fOlS+++KIz9cJizdEe/Ok5suy3E2Rw7wDdsfV3/9qqm6QdzC0xXR4AwEZc6rrwY6YaGVm2bJnMnDmz1dfcc8898sknn8j27dubnrvmmmskPz9fPv/883b9PoWFhRIUFCQFBQUSGBjY2XLhwKpqauWlNamy+Ku9+qA9L3dXmT91oNwyqb8+8wYAYH/a+/nd7d/F169fL1OnTm3x3CWXXKKfb01FRYX+AzS/YG0qcPzfBQnyxZ2TZcKAMB1IHvt8j8x4eq2kpHP6LwA4sm4PI1lZWRIZGdniOfVYBYyysrLTvmfRokU6STVesbGx3V0mHER8uJ+8efM4efyXiRLi66E7t/7s2XXy0Ec79EnAAADHY5fj2wsXLtRDOo1Xenq66ZJgR9T04JWjYuSrBefLz5L6iJpofHXdQbn4idWycle26fIAAPYWRnr37i3Z2S0/INRjNXfk4+Nz2veoXTfq15tfwMnC/L3kyatHyOs3jZXYUB/JKCiXm1/bJPPe+l6OFnIKMAA4im4PI+PHj5eVK1e2eO7LL7/UzwO2MHlQL72W5NbJ/cXN1UU+2ZYpFz6xWt7ZmCa1tWwDBgCnCyPFxcV6i666GrfuqvtpaWlNUyyzZs1qev1vfvMbOXDggNx9992ye/duefbZZ+X999+Xu+66y5Z/Dlicr6e7LLxsiKyYN0GG9QmSovJqWfjhNrnmhW9l/9Fi0+UBAGy5tVc1MFM9Q042e/Zs3czsxhtvlIMHD+rXNX+PCh87d+6UmJgY+eMf/6hf115s7UVHVNfUymvrD8nj/90jpZU14unmKr+dkqB343i5u5kuDwAso7Cdn99d6jPSUwgj6IzDx0vl/uXbZdWe+rNtBkT4y6KfD5Mx8aGmSwMASyi0lz4jgCkxIb7y6o1j5B/XJkm4v6eervnl8+vlvmXbpKCsynR5AIAGhBE4/TbgnyZG623AV4+u71fz1oY0ueiJ1fLZtkzOuQEAO0AYgSUE+3rKo78YLu/ccq70D/eTo0UV8n9vfS+3vL5ZMvJP33wPANAzCCOwlPEJYfLp/Ely+08GiLuri3y1K1uPkrz2zUGpYRswABhBGIHleHu4ye8uPks+uWOSjIwLlpLKGvnTf3bIlc99I7uzOAcJAHoaYQSWdVbvAPn3b86TP19xjvh7uesD9y7/x1p57PPdUl5VY7o8ALAMwggszdXVRW4YH68XuF58dqRU19bJs6t+lEsXJ8s3+3NNlwcAlkAYAdQZSkHe8sKs0fL89aMkMtBLDuaVynUvbZDf/2urHC+pNF0eADg1wgjQzKVDe8uXC86XG87tKy4uIv/afFimPrFaVqQcYRswAHQTwghwkkBvD/nzzKHy79+Ml0GR/pJXUinz302R2a9+J+nHSk2XBwBOhzACtGJU31D5+PZJ8ruLBunzbZL35sjFTybLi8kH9Pk3AADbIIwAbfB0d5XbLxwon905Scb2C5Wyqhr5y6e75Iol62Tb4QLT5QGAUyCMAO2Q0Mtf3r3lXHn0ymES6O0uOzIK5Yola+Xhj3dKaWW16fIAwKERRoAObAO+ekycfPW78+Xy4VGiGra+tDZVLnoiWVbtOWq6PABwWIQRoIMiArzlmetGyis3jpY+wT5yJL9Mbnz1O5n/7hbJLa4wXR4AOBzCCNBJPxkcKf+9a7LcPLGfuLqIrEjJkAsfXy3vb0pnGzAAdIBLnQN81ywsLJSgoCApKCiQwMBA0+UAp9iani/3frhNdmXWn20zvn+Y3DNtsCTGBImLalgCABZU2M7Pb8IIYCNVNbXyytpUefKrvVJeVb/1t3+4n8xM6iMzR/SRuDBf0yUCQI8ijACGpOWVyuNf7pEvdmQ1hRJlVN8QHUwuHxYlIX6eRmsEgJ5AGAEMK66oli+2Z8nylCOybn+u3n2juLu6yAVnRcjPkvrIhUMixNvDzXSpANAtCCOAHckuLJePtmbIsi1HdI+SRgFe7jJtWG89YnJuvzC9fRgAnAVhBLBTe7OLZPmWI3r3jdoW3CgqyFt+OiJaj5gM7s3fcwCOjzAC2Lna2jr57uAxPY3zyQ+ZUlh+opPr4N4BOpSocBIV5GO0TgDoLMII4EAqqmvkf7tz9IjJ17uPSmXDQXxqV7DaJqymcS4d2lufKAwAjoIwAjiogtIq+XR7pl5fsjH1WItD+y4aEqmDyfmDeunHAGDPCCOAEzh8vFSvLVHBZP/R4qbng3099Pk4aipnZFwIjdUA2CXCCOBE1D9TtQtHL3zdmiE5RSfOwIkL9ZWZI6LliqQ++nRhALAXhBHASdXU1sk3P+bq0ZLPt2dJaWVN06+p9vNqGmdGYrSE+3sZrRMACgkjgPMrrayWL3dm6xGT5H25Oqgobq4uMmlguJ7GuejsSPH1dDddKgALKiSMANaSW1whH6vGaikZ+uC+Rr6ebnLpOfWN1c5LCBN3Nxa+AugZhBHAwlJzS/RoiephciivtOn5XgFe8tPE+sZq50QHsvAVQLcijADQC1+3pOfrYKLa0R8vrWr6tQER/vWN1RKjJTaUE4UB2B5hBEALldW1smZfjl74qtaZVFSfOFF4bHyoXJEULdOHRUmwLycKA7ANwgiAVhWVV+mdOGoa55sf86Txu4CHm4tMOStCfj6yjz5ZmBOFAXQFYQRAu2QVlMt/th6RZVsyZFdmsxOFvd31SMl5A8IlPsxX+ob6SZAv7egBtB9hBECH7c5SjdUyZEXKEcksKD/l11Xn175hfvXhpOm2/n6YnycLYgG0QBgB0KUThTekHpNPtmXI3qxiOZhXIkebdX09HX8vdx1M4sP8mm7jGm4jArzE1ZWgAlhNIWEEgK0brKltwvVXiRxsuFWPMwrKmtadnI63h6ue5tEhJdxPt7BvDC3RwT66SRsA59Pez2/aMgJoF9XFdUhUoL5OVl5VI4ePl7UIKY236vnyqlrZk12kr5OpRbNqa7EeSdG3vtI3XE0B+UlMiI940KQNcHqEEQBdpnbdqL4l6jpZVU2tZOSXnQgpuQ0jKsdKJS2vVCprauVATom+TqZGTPoE+zSsS2kcTalfq6ICDLt9AOfANA0AY9RZOlmF5XIot+W0z8GG27KqE4cAnkytlY0K9G5al9J8Ya0KLn5e/KwFmMaaEQAOTX1ryimq0CFFhZO0ZiHlYG6JFFVUt/l+dWpx810/ibHBMj4hjGkfoAcRRgA4LfVtS7W2rw8n9QGl+YjKsZLK074vxNdDLh3aW6YPi5Zz+4dyaCDQzQgjACyroKxKj6QcOlYfTn48Wiyr9+ZIXrOQEurnqYPJ5cOjZFy/MHb0AN2AMAIAzVTX1OreKR//kCmfb89scWhguL+nTBsaJdOHR8mY+FCCCWAjhBEAaCOYrD+QJ5+oYLIjS/KbBZNeAV5ymZrKGR4to/uG0KwN6ALCCAC0g9p6rA4L/OSHDH14YGH5iYWxkYFeesRkRmKUJMUSTICOIowAQAdVVtfKuv25eirnvzuzpKhZMIkK8pbLhtVP5STFBnMOD9AOhBEA6IKK6hpZuy9XT+X8d2e2FDfbSqwasV02rH4qJzEmiGACtIIwAgA2otrdr9HBJEO+3JktJZUnmrGplvVqtOTyYdEytE8gwQRohjACAN0UTFbtyZFPtmXKyl3ZUtosmKizdVQwmT4sSs6JJpgAhYQRAOheZZUqmByVj7dlyte7jrZoX6+6vtYHk2gZEhVAMIElFRJGAKDnlFZWy/92qxGTDPl691F9UnGj/uF+9cFkeJScFUkwgXUUEkYAwIySimpZufuoXmPyvz05epdOI3WysZrGUZ1fB0YGGK0T6G6EEQCwA2oXjlpborYLr1bBpOZEMBkUqYJJtB4xUSEFcDaEEQCwM4XlVfXBZGumJO/LkaqaE99+B/cO0CMmKpj070UwgXMgjACAnR/mp7YJq6kctW24uvbEt+KzowKbduXEh/sZrRPoCsIIADiI/NJK3VhNNVhTHWCbBxPVu0RP5QyLkrgwX6N1Ah1FGAEAB3S8RAWTLL3GRJ2ZU9MsmAyPCZKLhkTKqL4hkhgbLH5e7kZrBc6EMAIADi6vuEK+2JGttwuv/zFPmuUSUWf2De4dqIPJyL7BMiouVGJDfdg2DLtCGAEAJ5JbXKFPFV5/IE+2HDouGQXlp7wm3N9TRsapcBKiQ8qwPkHi7eFmpF5AIYwAgBPLLCiT7w/ly+ZDx+X7tOOyI6Ogxe4cxd3VRbelV+FEhRQVUKKDfYzVDOspJIwAgLXOzNl+pEAHExVQNh/K16MpJ+sd6K1DSVJcsL49JzpIPN1djdQM51dIGAEA61Lf2g8fL2sKJ+p2V2ZRiwWxigoiw/sENYyeBOsRlIhAb2N1w7kQRgAAp5yfszW9fvTk+4aAcry06pTXxYT41C+MbZjaUQ3Z3N0YPYGdhZElS5bI3/72N8nKypLExER5+umnZezYsad9bVVVlSxatEhee+01OXLkiJx11lny6KOPyqWXXmrzPwwAoP3Ut//U3BL5Pq1+7cmWtOOyJ7tITv5U8PFwk8TYoKZwkhQXIqF+nqbKhgPptjDy3nvvyaxZs+T555+XcePGyeLFi+Vf//qX7NmzRyIiIk55/T333CNvvvmmvPjiizJ48GD54osvZMGCBfLNN99IUlKSTf8wAICut6zfmp5fvzg2rT6gFJVXn/I6dRKxCiWNW4sHRgSIm9pvDPREGFEBZMyYMfLMM8/ox7W1tRIbGyu333673Hvvvae8Pjo6Wu677z6ZN29e03NXXnml+Pj46JDSHoQRADCjtrZO9ucU62mdxrUnP+aUnPI6fy93vSi2MaCMiA2WIB8PIzXDfrT387tD7fsqKytl8+bNsnDhwqbnXF1dZerUqbJ+/frTvqeiokK8vVsuhlJBZO3atR35rQEABri6usigyAB9XTM2rql9/Za0/KbFsSnp+fp0YnXGjroU1XttYIR/U98TdZvQy4+mbOh6GMnNzZWamhqJjIxs8bx6vHv37tO+55JLLpEnnnhCJk+eLAkJCbJy5Ur58MMP9X+nNSrAqKt5sgIA2IdgX0+ZMjhCX0p1Ta1ea6LWnjQujD2UVyp7s4v19e536Q3v85Ck2PodO0NjgiQu1Ff6BPvQmA0dCyOd8dRTT8ktt9yi14uoRKwCyZw5c+SVV15p9T1qwetDDz3U3aUBAGxA7bRR/UrUdcO5ffVzqsdJfTCpDyhbD+dLfmmV/G9Pjr6aiwz0ktgQX72LJzbUt/5+qI++jQryZiePBXRozYiapvH19ZV///vfMnPmzKbnZ8+eLfn5+bJixYpW31teXi55eXl6DYlaW/Lxxx/Ljh072j0yotalsGYEABxTZXWt7MosbJra2X+0WNKPlUpJZeuj5IpaFKsCiQom6uyd+tsTwaWXv5eeSoKF1ox4enrKqFGj9FRLYxhRC1jV49tuu63N96p1I3369NFbfT/44AO56qqrWn2tl5eXvgAAzkE1V1MnDatrzoR++jn1s7Dqc6JCSfrxUt2krf5+mRw+Vv+4sqZW36pr/YHT/3d1MDlpZKUxuKipIdapOOE0jdqWq0ZCRo8erXuLqK29JSUleupFUdt+VehQUy3Khg0bdH+RESNG6NsHH3xQB5i7777b9n8aAIDDUCFB9StRlwopp9vJc7SoQg4frw8r6ccaw0r9fXU+jxpxOZBToq/TUbt8VEiJaWVkRf06zOvwV+Hqq6+WnJwceeCBB3TTMxUyPv/886ZFrWlpaXqHTfPpmfvvv18OHDgg/v7+ctlll8kbb7whwcGn/sUDAKCRmn7pHeStr9Hxoaf8elVNrWQVlLcIKM1HWFSQUbt8dmcV6et0Qnw9WqxT0aGlIaiwuLbn0A4eAOC0hwfqYNIQUA6fFFrUgtozYXGtHa4ZAQDAUahRjQER/vo6naLyqhbrVNStmhJqfE4trs0urNDXpkPHT7u4NjrYW/qF+0u/MF+JD/eTfg2XGlUhqLQfYQQAYEkB3h4yJEpdp/7E3t7FtfXrWMok+aT3e7i56JEU1TY/PsxP+vXyk34Nt5EB3uwAOglhBACATi6uPZRXIgfzSiQ1t1RSc4vloLrNK2lzYa23h2t9QGkYRYkP96sPLeF+EubnacndP6wZAQDAhlRQySwsl9ScEh1M1G19YCnRoyvVta1/7AZ4uevRk+ZhpTGwOOJZP912UJ4JhBEAgDNQO4DUFM/B3BI5kFuib1MbroyCMmnrE1mNmjRfl6JDSpgKKr7i62mfEx2EEQAAHGz3T9qx0qZw0jywqCmhtvQO9NahRC+mbXar1q14uZvbnkwYAQDASRRXVDeNojSNpjRM/bS1RVmtk+0T4tO04+fEGhV//bzaEdSdCCMAAFjA8ZJKHUyaT/k0hpa2zv45ecfPT0dEy/AY2zYkpc8IAAAWEOLnqa+RcSEtnldjDTlFFfXBJK/lGpWDeaWn7PgZFhNk8zDSXoQRAACckIuLi0QEeutrXP+wU3b8qAWzeitybrHemjysT5CxWgkjAABYjKuriz6HR10TB4abLkfoVQsAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIxyiFN76+rq9G1hYaHpUgAAQDs1fm43fo47dBgpKirSt7GxsaZLAQAAnfgcDwoKavXXXerOFFfsQG1trWRkZEhAQIC4uLjYNLGpgJOeni6BgYE2+++i8/ia2Be+HvaFr4d94etxZipiqCASHR0trq6ujj0yov4AMTEx3fbfV3+J+ItkX/ia2Be+HvaFr4d94evRtrZGRBqxgBUAABhFGAEAAEZZOox4eXnJn/70J30L+8DXxL7w9bAvfD3sC18P23GIBawAAMB5WXpkBAAAmEcYAQAARhFGAACAUYQRAABglKXDyJIlSyQ+Pl68vb1l3LhxsnHjRtMlWdKiRYtkzJgxusNuRESEzJw5U/bs2WO6LDT461//qjsf33nnnaZLsbQjR47I9ddfL2FhYeLj4yPDhg2TTZs2mS7LkmpqauSPf/yj9OvXT38tEhIS5M9//vMZz19B6ywbRt577z1ZsGCB3pb1/fffS2JiolxyySVy9OhR06VZzurVq2XevHny7bffypdffilVVVVy8cUXS0lJienSLO+7776Tf/7znzJ8+HDTpVja8ePHZcKECeLh4SGfffaZ7Ny5Ux5//HEJCQkxXZolPfroo/Lcc8/JM888I7t27dKPH3vsMXn66adNl+awLLu1V42EqJ/G1V+mxvNv1BkDt99+u9x7772my7O0nJwcPUKiQsrkyZNNl2NZxcXFMnLkSHn22Wfl4YcflhEjRsjixYtNl2VJ6nvSunXrZM2aNaZLgYhcfvnlEhkZKS+//HLTc1deeaUeJXnzzTeN1uaoLDkyUllZKZs3b5apU6e2OP9GPV6/fr3R2iBSUFCgb0NDQ02XYmlqtGr69Okt/p3AjP/85z8yevRo+eUvf6mDelJSkrz44oumy7Ks8847T1auXCl79+7Vj7du3Spr166VadOmmS7NYTnEQXm2lpubq+f8VLJtTj3evXu3sbpQP0Kl1iaoIemhQ4eaLsey3n33XT19qaZpYN6BAwf0tICaWv7DH/6gvy533HGHeHp6yuzZs02XZ8mRKnVi7+DBg8XNzU1/nvzlL3+RX/3qV6ZLc1iWDCOw75/Gt2/frn/KgBnqOPT58+fr9TtqcTfsI6SrkZFHHnlEP1YjI+rfyfPPP08YMeD999+Xt956S95++20555xzJCUlRf8QFR0dzdejkywZRsLDw3Wazc7ObvG8ety7d29jdVndbbfdJh9//LEkJydLTEyM6XIsS01hqoXcar1II/WTn/q6qDVWFRUV+t8Pek5UVJScffbZLZ4bMmSIfPDBB8ZqsrLf//73enTkmmuu0Y/VzqZDhw7pnYGEkc6x5JoRNbQ5atQoPefX/CcP9Xj8+PFGa7MitYZaBZFly5bJ119/rbfLwZwLL7xQtm3bpn/aa7zUT+VqCFrdJ4j0PDVtefJ2d7VeoW/fvsZqsrLS0lK9zrA59e9CfY6gcyw5MqKouVeVYNU32bFjx+pdAmor6Zw5c0yXZsmpGTXcuWLFCt1rJCsrSz8fFBSkV6ejZ6mvwcnrdfz8/HR/C9bxmHHXXXfpRZNqmuaqq67SPZFeeOEFfaHnzZgxQ68RiYuL09M0W7ZskSeeeEJuuukm06U5rjoLe/rpp+vi4uLqPD0968aOHVv37bffmi7JktRfw9Ndr776qunS0OD888+vmz9/vukyLO2jjz6qGzp0aJ2Xl1fd4MGD61544QXTJVlWYWGh/vegPj+8vb3r+vfvX3fffffVVVRUmC7NYVm2zwgAALAPllwzAgAA7AdhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgJj0/wFBNrbyzEL9VQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2863f",
   "metadata": {},
   "source": [
    "# Summary & Key Takeaways\n",
    "\n",
    "## Method Comparison\n",
    "\n",
    "| Aspect | Method 1 (Manual) | Method 2 (Built-in) |\n",
    "|--------|-------------------|---------------------|\n",
    "| **Code Complexity** | More complex (manual loops) | Simpler (automatic) |\n",
    "| **Speed** | Slower | Faster (optimized) |\n",
    "| **Control** | Full control over each step | Less control |\n",
    "| **Input Shape** | `(batch, features)` per step | `(batch, seq_len, features)` |\n",
    "| **Best For** | Learning, custom RNN cells | Production, standard RNNs |\n",
    "\n",
    "---\n",
    "\n",
    "## Important Concepts Covered\n",
    "\n",
    "### 1. **Sequential Processing**\n",
    "RNNs process data one step at a time, maintaining a hidden state that acts as \"memory\"\n",
    "\n",
    "### 2. **Hidden State**\n",
    "- Carries information from previous time steps\n",
    "- Gets updated at each step\n",
    "- Final hidden state contains information about the entire sequence\n",
    "\n",
    "### 3. **One-Hot Encoding**\n",
    "- Converts categorical data (characters) into numerical vectors\n",
    "- Each character gets a unique position in a vector\n",
    "- Essential for neural network input\n",
    "\n",
    "### 4. **Tensor Shapes Matter!**\n",
    "- **Method 1**: `torch.zeros(seq_len, 1, features)` - process one at a time\n",
    "- **Method 2**: `(batch, seq_len, features)` - process entire batch\n",
    "- Always verify tensor shapes match model expectations\n",
    "\n",
    "### 5. **Image as Sequence**\n",
    "- Images can be treated as sequences (rows as time steps)\n",
    "- CIFAR-10: `(3, 32, 32)` ‚Üí `(32, 96)` where 32 = rows, 96 = pixels√óchannels\n",
    "- Reshape: `permute(0,2,3,1)` then `reshape(-1, seq_len, features)`\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls & Solutions\n",
    "\n",
    "### ‚ùå **Problem**: Shape mismatch errors\n",
    "**Solution**: Always check tensor shapes with `.shape` before passing to model\n",
    "\n",
    "### ‚ùå **Problem**: Forgetting to loop in Method 1\n",
    "**Solution**: Must manually loop through sequence for manual RNN\n",
    "\n",
    "### ‚ùå **Problem**: Using all outputs instead of last one (Method 2)\n",
    "**Solution**: Use `out[:, -1, :]` to get last time step\n",
    "\n",
    "### ‚ùå **Problem**: Forgetting `.to(device)` when using GPU\n",
    "**Solution**: Always move model and data to same device\n",
    "\n",
    "### ‚ùå **Problem**: Not reshaping test data the same as training data\n",
    "**Solution**: Apply identical preprocessing to both train and test data\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use RNNs?\n",
    "\n",
    "‚úÖ **Good for:**\n",
    "- Text classification\n",
    "- Sequence prediction\n",
    "- Time series analysis\n",
    "- Variable-length inputs\n",
    "\n",
    "‚ùå **Consider alternatives for:**\n",
    "- Very long sequences (use LSTM/GRU instead)\n",
    "- Parallel processing (use Transformers)\n",
    "- Image classification (CNNs are better)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Try LSTM/GRU**: Better at capturing long-term dependencies\n",
    "2. **Experiment with bidirectional RNNs**: Process sequences forward and backward\n",
    "3. **Stack multiple RNN layers**: Increase model capacity\n",
    "4. **Try different optimizers**: Adam vs SGD vs RMSprop\n",
    "5. **Tune hyperparameters**: hidden_size, learning_rate, num_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a3768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
