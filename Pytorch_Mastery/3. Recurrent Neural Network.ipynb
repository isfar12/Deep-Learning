{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4742bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b06996",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNN) Tutorial\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates two different approaches to building RNNs in PyTorch:\n",
    "1. **Method 1**: Manual RNN implementation from scratch\n",
    "2. **Method 2**: Using PyTorch's built-in `nn.RNN` module\n",
    "\n",
    "We'll solve two problems:\n",
    "- **Problem 1**: Name Classification (predicting nationality from names)\n",
    "- **Problem 2**: Image Classification (CIFAR-10 using RNNs)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### What is an RNN?\n",
    "A Recurrent Neural Network processes sequential data by maintaining a \"hidden state\" that gets updated at each time step. This allows the network to have \"memory\" of previous inputs.\n",
    "\n",
    "### RNN Processing Flow\n",
    "For a sequence like \"Albert\":\n",
    "1. Process 'A' → update hidden state\n",
    "2. Process 'l' with previous hidden state → update hidden state\n",
    "3. Process 'b' with previous hidden state → update hidden state\n",
    "4. Continue for 'e', 'r', 't'\n",
    "5. Use final hidden state for prediction\n",
    "\n",
    "### Input Shape for RNNs\n",
    "- **Method 1 (Manual)**: Process one time step at a time `(batch_size, features)`\n",
    "- **Method 2 (Built-in)**: Process entire sequence `(batch_size, sequence_length, features)`\n",
    "\n",
    "---\n",
    "\n",
    "## Method 1: Manual RNN Implementation\n",
    "\n",
    "**How it works:**\n",
    "- Manually loop through each character/time step\n",
    "- Concatenate input with previous hidden state\n",
    "- Apply linear transformations\n",
    "- Update hidden state at each step\n",
    "\n",
    "**Advantages:**\n",
    "- Full control over the RNN cell\n",
    "- Easy to understand the mechanics\n",
    "- Can customize each step\n",
    "\n",
    "**Disadvantages:**\n",
    "- Requires manual looping\n",
    "- Slower for long sequences\n",
    "- More code to write\n",
    "\n",
    "---\n",
    "\n",
    "## Method 2: PyTorch Built-in RNN\n",
    "\n",
    "**How it works:**\n",
    "- PyTorch's `nn.RNN` handles all the looping internally\n",
    "- Pass entire sequence at once\n",
    "- Automatically returns outputs for all time steps\n",
    "- We take the last time step for classification\n",
    "\n",
    "**Advantages:**\n",
    "- Highly optimized (faster)\n",
    "- Less code\n",
    "- Easier to scale\n",
    "\n",
    "**Disadvantages:**\n",
    "- Less control over internal mechanics\n",
    "- Slightly more abstract\n",
    "\n",
    "---\n",
    "\n",
    "## Data Representation\n",
    "\n",
    "### One-Hot Encoding (Text)\n",
    "Text is converted to one-hot vectors:\n",
    "- \"a\" → [1, 0, 0, ..., 0] (58 dimensions)\n",
    "- Each character gets a unique position set to 1\n",
    "\n",
    "### Sequence Tensor Shape\n",
    "For a name like \"Albert\":\n",
    "- Shape: `(6, 1, 58)`\n",
    "  - 6 = sequence length (6 characters)\n",
    "  - 1 = batch size (processing one name)\n",
    "  - 58 = vocabulary size (one-hot encoding)\n",
    "\n",
    "### Image as Sequence (CIFAR-10)\n",
    "CIFAR-10 images (3×32×32) are treated as sequences:\n",
    "- Each row becomes a time step\n",
    "- Shape transformation: `(batch, 3, 32, 32)` → `(batch, 32, 96)`\n",
    "  - 32 time steps (rows)\n",
    "  - 96 features (32 pixels × 3 colors)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697dbaa",
   "metadata": {},
   "source": [
    "# Problem 1: Classification of Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4a4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "# We can use \"_\" to represent an out-of-vocabulary character, that is, any character we are not handling in our model\n",
    "allowed_characters = string.ascii_letters + \" .,;'\" + \"_\"\n",
    "n_letters = len(allowed_characters)\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in allowed_characters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11fec989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting 'Ślusàrski' to Slusarski\n"
     ]
    }
   ],
   "source": [
    "print (f\"converting 'Ślusàrski' to {unicodeToAscii('Ślusàrski')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88dcae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    # return our out-of-vocabulary character if we encounter a letter unknown to our model\n",
    "    if letter not in allowed_characters:\n",
    "        return allowed_characters.find(\"_\")\n",
    "    else:\n",
    "        return allowed_characters.find(letter)\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1d3581e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter 'a' becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]])\n",
      "The name 'anc' becomes tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print (f\"The letter 'a' becomes {lineToTensor('a')}\") #notice that the first position in the tensor = 1\n",
    "print (f\"The name 'anc' becomes {lineToTensor('anc')}\") #notice 'A' sets the 27th index to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02be812",
   "metadata": {},
   "source": [
    "# Problem 2: Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5d3f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "# Transform: Convert image to tensor and normalize (0–1 range)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))   # normalize to mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "# Download MNIST training & test datasets\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1440f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8d7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir #for provenance of the dataset\n",
    "        self.load_time = time.localtime #for provenance of the dataset\n",
    "        labels_set = set() #set of all classes\n",
    "\n",
    "        self.data = []\n",
    "        self.data_tensors = []\n",
    "        self.labels = []\n",
    "        self.labels_tensors = []\n",
    "\n",
    "        #read all the ``.txt`` files in the specified directory\n",
    "        text_files = glob.glob(os.path.join(data_dir, '*.txt'))\n",
    "        for filename in text_files:\n",
    "            label = os.path.splitext(os.path.basename(filename))[0]\n",
    "            labels_set.add(label)\n",
    "            lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "            for name in lines:\n",
    "                self.data.append(name)\n",
    "                self.data_tensors.append(lineToTensor(name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "        #Cache the tensor representation of the labels\n",
    "        self.labels_uniq = list(labels_set)\n",
    "        for idx in range(len(self.labels)):\n",
    "            temp_tensor = torch.tensor([self.labels_uniq.index(self.labels[idx])], dtype=torch.long)\n",
    "            self.labels_tensors.append(temp_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_item = self.data[idx]\n",
    "        data_label = self.labels[idx]\n",
    "        data_tensor = self.data_tensors[idx]\n",
    "        label_tensor = self.labels_tensors[idx]\n",
    "\n",
    "        return label_tensor, data_tensor, data_label, data_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "285c4cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 20074 items of data\n",
      "example = (tensor([2]), tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0.]]]), 'Arabic', 'Khoury')\n"
     ]
    }
   ],
   "source": [
    "alldata = NamesDataset(\"data/names\")\n",
    "print(f\"loaded {len(alldata)} items of data\")\n",
    "print(f\"example = {alldata[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d384845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device = cpu\n",
      "train examples = 17063, validation examples = 3011\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using device = {torch.get_default_device()}\")\n",
    "train_set, test_set = torch.utils.data.random_split(alldata, [.85, .15], generator=torch.Generator(device=device).manual_seed(2024))\n",
    "\n",
    "print(f\"train examples = {len(train_set)}, validation examples = {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89997f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"asset/rnn_workflow.png\" alt=\"Rnn Flow\" width=\"300\" height=\"300\">\n",
    "  <figcaption>How data is processed inside of RNN</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fdc0f6",
   "metadata": {},
   "source": [
    "# Method 1: Manual RNN Implementation\n",
    "\n",
    "## Architecture Explanation\n",
    "\n",
    "This RNN processes sequences **one time step at a time**:\n",
    "\n",
    "### Components:\n",
    "1. **`input_to_hidden`**: Combines current input with previous hidden state\n",
    "   - Input size: `input_size + hidden_size`\n",
    "   - Output size: `hidden_size`\n",
    "   \n",
    "2. **`input_to_output`**: Produces output predictions\n",
    "   - Input size: `input_size + hidden_size`\n",
    "   - Output size: `output_size` (number of classes)\n",
    "   \n",
    "3. **`softmax`**: Converts outputs to log probabilities\n",
    "\n",
    "### Forward Pass:\n",
    "```python\n",
    "# For each character in the sequence:\n",
    "combined = concat(input_char, hidden_state)  # Combine input with memory\n",
    "hidden_state = linear(combined)              # Update memory\n",
    "output = linear(combined)                     # Compute output\n",
    "output = log_softmax(output)                  # Convert to probabilities\n",
    "```\n",
    "\n",
    "### Usage Pattern:\n",
    "```python\n",
    "hidden = model.initHidden()\n",
    "for char in sequence:\n",
    "    output, hidden = model(char, hidden)\n",
    "# Use final output for prediction\n",
    "```\n",
    "\n",
    "**Key Point**: You must manually loop through the sequence!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fbefa2",
   "metadata": {},
   "source": [
    "\n",
    "## Why Combine Input + Hidden State?\n",
    "\n",
    "The core idea of an RNN is to use **both current information AND past information** to make decisions. Here's the intuition:\n",
    "\n",
    "### The Concept\n",
    "\n",
    "When reading a sequence like \"Albert\":\n",
    "- At time step 1 ('A'): We have no history, just the letter 'A'\n",
    "- At time step 2 ('l'): We need to remember we saw 'A' before\n",
    "- At time step 3 ('b'): We need to remember 'Al' came before\n",
    "- And so on...\n",
    "\n",
    "### How Concatenation Achieves This\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea3aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = torch.cat((input_tensor, hidden_tensor), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4b713",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This creates a **single vector** containing:\n",
    "1. **Current input** - What we're seeing NOW (e.g., current character)\n",
    "2. **Hidden state** - What we've seen BEFORE (memory from previous time steps)\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "Let's say:\n",
    "- `input_size = 58` (one-hot encoded character)\n",
    "- `hidden_size = 128` (memory size)\n",
    "\n",
    "At each time step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16922626",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "Current character 'l': [0, 0, 1, 0, ..., 0]  (58 dimensions)\n",
    "                       ↓\n",
    "              Concatenate with\n",
    "                       ↓\n",
    "Previous memory:       [0.3, -0.5, 0.8, ...]  (128 dimensions)\n",
    "                       ↓\n",
    "              Results in:\n",
    "                       ↓\n",
    "Combined vector:       [0, 0, 1, ..., 0, 0.3, -0.5, 0.8, ...]  (186 dimensions)\n",
    "                       ↑____________↑  ↑___________________↑\n",
    "                         input(58)      hidden_state(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6952dfc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Why This Design?\n",
    "\n",
    "**The linear layer needs BOTH pieces of information** to decide:\n",
    "1. **What to remember next** (update hidden state)\n",
    "2. **What to output** (prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83492025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network learns to say things like:\n",
    "# \"If I see 'e' AND the previous letters were 'Alb', \n",
    "#  then this is probably a name, update memory accordingly\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab440bb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Mathematical Perspective\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117f654",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_new = f(input_current + hidden_previous)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1956c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This is the fundamental RNN equation. The concatenation is just how we implement the \"+\" operation - we stack the vectors and let a linear layer learn how to combine them.\n",
    "\n",
    "### Alternative: Why Not Add Them?\n",
    "\n",
    "You might wonder: \"Why concatenate instead of adding?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e325670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why not this?\n",
    "combined = input_tensor + hidden_tensor  # ❌ Won't work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa118cb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Problem**: They have different dimensions! (58 vs 128)\n",
    "\n",
    "Even if we made them the same size, **concatenation is better** because:\n",
    "- ✅ Preserves all information from both sources\n",
    "- ✅ Lets the network learn how much weight to give each\n",
    "- ✅ More flexible - the network decides how to combine them\n",
    "\n",
    "### In PyTorch's Built-in RNN\n",
    "\n",
    "PyTorch's `nn.RNN` does the same thing internally, but you don't see it because it's hidden inside the optimized implementation. It still combines input + hidden state behind the scenes!\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaway**: Concatenating input with hidden state is how RNNs achieve their \"memory\" - they always consider both what they're seeing NOW and what they've seen BEFORE to make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32cc72",
   "metadata": {},
   "source": [
    "## Code-Level Comparison: Manual vs Built-in RNN\n",
    "\n",
    "| Aspect | Method 1: Manual RNN | Method 2: Built-in RNN |\n",
    "|--------|---------------------|------------------------|\n",
    "| **Class Definition** | `class RNNModel(nn.Module)` | `class RNN(nn.Module)` |\n",
    "| **Layers Defined** | • `nn.Linear(input+hidden, hidden)`<br>• `nn.Linear(input+hidden, output)`<br>• `nn.LogSoftmax(dim=1)` | • `nn.RNN(input, hidden, layers, batch_first=True)`<br>• `nn.Linear(hidden, output)` |\n",
    "| **Initialization** | `self.input_to_hidden`<br>`self.input_to_hidden`<br>`self.softmax` | `self.rnn`<br>`self.fc`<br>`self.hidden_size`, `self.num_layers` |\n",
    "| **Forward Input** | `forward(input_tensor, hidden_tensor)` | `forward(x)` |\n",
    "| **Hidden State Init** | `initHidden()` returns `torch.zeros(1, hidden_size)` | `h0 = torch.zeros(num_layers, batch_size, hidden_size)` inside forward |\n",
    "| **Input Shape** | `(1, input_size)` - single time step | `(batch, seq_len, input_size)` - entire sequence |\n",
    "| **Processing** | ```python<br>combined = cat(input, hidden)<br>hidden = linear(combined)<br>output = linear(combined)<br>output = log_softmax(output)<br>``` | ```python<br>h0 = zeros(...)<br>out, _ = self.rnn(x, h0)<br>out = out[:, -1, :]<br>out = self.fc(out)<br>``` |\n",
    "| **Output Shape** | `(1, output_size)` | `(batch, output_size)` |\n",
    "| **Returns** | `output, hidden_state` (tuple) | `output` (single tensor) |\n",
    "| **Manual Looping** | ✅ **Required** - Must loop externally | ❌ **Not Required** - Handles internally |\n",
    "| **Usage Example** | ```python<br>hidden = model.initHidden()<br>for i in range(seq_len):<br>    out, hidden = model(x[i], hidden)<br>``` | ```python<br>output = model(sequence)<br>``` |\n",
    "| **Concatenation** | `torch.cat((input, hidden), 1)` - **Explicit** | Hidden inside `nn.RNN` - **Implicit** |\n",
    "| **Activation** | `LogSoftmax` (for NLLLoss) | None (for CrossEntropyLoss) |\n",
    "| **Time Step Output** | Only final output available | All time steps available (`out[:, -1, :]` takes last) |\n",
    "| **Hidden State Access** | ✅ After each step | ✅ Final state returned (ignored with `_`) |\n",
    "| **Flexibility** | Can customize each step | Standard RNN behavior only |\n",
    "| **Code Lines** | ~18 lines | ~16 lines |\n",
    "| **Memory Efficient** | Process one at a time | Batch processing |\n",
    "| **GPU Optimization** | Standard operations | Highly optimized CUDA kernels |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Architectural Differences\n",
    "\n",
    "#### **1. Layer Construction**\n",
    "\n",
    "**Method 1 (Manual):**\n",
    "```python\n",
    "self.input_to_hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "self.input_to_output = nn.Linear(input_size + hidden_size, output_size)\n",
    "self.softmax = nn.LogSoftmax(dim=1)\n",
    "```\n",
    "\n",
    "**Method 2 (Built-in):**\n",
    "```python\n",
    "self.rnn = nn.RNN(input_size, hidden_size, num_rnn_layers, batch_first=True)\n",
    "self.fc = nn.Linear(hidden_size, output_size)\n",
    "```\n",
    "\n",
    "#### **2. Forward Pass Logic**\n",
    "\n",
    "**Method 1 (Manual):**\n",
    "```python\n",
    "def forward(self, input_tensor, hidden_tensor):\n",
    "    combined = torch.cat((input_tensor, hidden_tensor), 1)  # Manually combine\n",
    "    hidden_state = self.input_to_hidden(combined)           # Update hidden\n",
    "    output = self.input_to_output(combined)                 # Compute output\n",
    "    output = self.softmax(output)                           # Apply softmax\n",
    "    return output, hidden_state                             # Return both\n",
    "```\n",
    "\n",
    "**Method 2 (Built-in):**\n",
    "```python\n",
    "def forward(self, x):\n",
    "    h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "    out, _ = self.rnn(x, h0)        # RNN handles all time steps\n",
    "    out = out[:, -1, :]             # Extract last time step\n",
    "    out = self.fc(out)              # Classify\n",
    "    return out                      # Return only output\n",
    "```\n",
    "\n",
    "#### **3. Training Loop Differences**\n",
    "\n",
    "**Method 1 Usage:**\n",
    "```python\n",
    "for i in batch:\n",
    "    hidden = rnn.initHidden()                    # Initialize hidden\n",
    "    for j in range(text_tensor.size()[0]):       # Loop through sequence\n",
    "        output, hidden = rnn(text_tensor[j], hidden)  # One step at a time\n",
    "    loss = criterion(output, label)              # Use final output\n",
    "```\n",
    "\n",
    "**Method 2 Usage:**\n",
    "```python\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs = inputs.reshape(-1, seq_len, input_size)  # Reshape to sequence\n",
    "    outputs = rnn(inputs)                              # Process entire batch\n",
    "    loss = criterion(outputs, labels)                  # Compute loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Metric | Method 1 | Method 2 |\n",
    "|--------|----------|----------|\n",
    "| **Speed** | ~10-50x slower | Baseline (fastest) |\n",
    "| **Memory** | Lower (one step) | Higher (full sequence) |\n",
    "| **Batch Size** | Effectively 1 | Full batch (64, 128, etc.) |\n",
    "| **Parallelization** | Limited | Excellent |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use Method 1 (Manual) when:**\n",
    "- 🎓 Learning RNN internals\n",
    "- 🔧 Need custom RNN cell behavior\n",
    "- 📊 Want step-by-step inspection\n",
    "- 🧪 Prototyping new architectures\n",
    "\n",
    "**Use Method 2 (Built-in) when:**\n",
    "- 🚀 Production code\n",
    "- ⚡ Speed matters\n",
    "- 📦 Standard RNN behavior is sufficient\n",
    "- 💼 Working with large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c372e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_to_hidden=nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.input_to_output=nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax=nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        combined=torch.cat((input_tensor, hidden_tensor), 1)\n",
    "        \n",
    "        hidden_state=self.input_to_hidden(combined)\n",
    "        output=self.input_to_output(combined)\n",
    "        output=self.softmax(output)\n",
    "        return output, hidden_state\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3511f",
   "metadata": {},
   "source": [
    "# Method 2: PyTorch Built-in RNN\n",
    "\n",
    "## Architecture Explanation\n",
    "\n",
    "This RNN uses PyTorch's optimized `nn.RNN` module that processes **entire sequences at once**:\n",
    "\n",
    "### Components:\n",
    "1. **`self.rnn`**: PyTorch's RNN layer\n",
    "   - `input_size`: Features per time step\n",
    "   - `hidden_size`: Size of hidden state\n",
    "   - `num_layers`: Number of stacked RNN layers\n",
    "   - `batch_first=True`: Input shape is `(batch, sequence, features)`\n",
    "   \n",
    "2. **`self.fc`**: Fully connected layer for classification\n",
    "   - Input size: `hidden_size`\n",
    "   - Output size: `output_size` (number of classes)\n",
    "\n",
    "### Forward Pass:\n",
    "```python\n",
    "# Initialize hidden state\n",
    "h0 = zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "# Process entire sequence at once\n",
    "out, hidden = self.rnn(x, h0)\n",
    "# out shape: (batch, sequence_length, hidden_size)\n",
    "\n",
    "# Take only the last time step\n",
    "out = out[:, -1, :]  # Shape: (batch, hidden_size)\n",
    "\n",
    "# Classify\n",
    "out = self.fc(out)   # Shape: (batch, num_classes)\n",
    "```\n",
    "\n",
    "### Key Differences from Method 1:\n",
    "- ✅ No manual looping required\n",
    "- ✅ Much faster (optimized CUDA kernels)\n",
    "- ✅ Returns outputs for ALL time steps\n",
    "- ✅ We extract the last time step `out[:, -1, :]` for classification\n",
    "\n",
    "### Usage Pattern:\n",
    "```python\n",
    "# Just pass the entire sequence!\n",
    "output = model(sequence)  # sequence shape: (batch, seq_len, features)\n",
    "```\n",
    "\n",
    "**Why take the last time step?**\n",
    "The last time step's output contains information from the entire sequence (all previous time steps), making it ideal for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa132d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_rnn_layers, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_rnn_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_rnn_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # Initial hidden state\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # out shape: (batch_size, sequence_length, hidden_size)\n",
    "        # We only need the output from the last time step\n",
    "        out = out[:, -1, :]  # Take the last time step: (batch_size, hidden_size)\n",
    "        out = self.fc(out)  # (batch_size, output_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74116d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(96, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 images are 3x32x32 (3 color channels, 32x32 pixels)\n",
    "# We'll treat each row as a sequence: 32 rows, each with 32*3=96 features\n",
    "input_size = 32 * 3  # 96 features per time step (32 pixels x 3 color channels)\n",
    "sequence_length = 32  # 32 rows (time steps)\n",
    "num_rnn_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "rnn = RNN(input_size, hidden_size, num_rnn_layers, num_classes)\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "508a5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0b0c1",
   "metadata": {},
   "source": [
    "## Why Permute and Reshape for Images?\n",
    "\n",
    "### The Problem: Image Format vs Sequence Format\n",
    "\n",
    "**CIFAR-10 images come in this shape:**\n",
    "- `(batch, channels, height, width)` = `(64, 3, 32, 32)`\n",
    "- This is the **standard PyTorch image format**\n",
    "- Channels first: RGB values are separated\n",
    "\n",
    "**But RNN expects sequences in this shape:**\n",
    "- `(batch, sequence_length, features)` = `(64, 32, 96)`\n",
    "- Each time step should have ALL features for that step\n",
    "\n",
    "### Visual Explanation\n",
    "\n",
    "**Original Image Shape: (3, 32, 32)**\n",
    "```\n",
    "Channel dimension:\n",
    "├─ Red channel:    32×32 pixels\n",
    "├─ Green channel:  32×32 pixels  \n",
    "└─ Blue channel:   32×32 pixels\n",
    "```\n",
    "\n",
    "**We want to treat each ROW as a time step:**\n",
    "- Row 1: All RGB values from the first row (32 pixels × 3 colors = 96 features)\n",
    "- Row 2: All RGB values from the second row (96 features)\n",
    "- Row 3: All RGB values from the third row (96 features)\n",
    "- ... up to Row 32\n",
    "\n",
    "### Why Two Steps?\n",
    "\n",
    "#### Step 1: **Permute** `(batch, 3, 32, 32)` → `(batch, 32, 32, 3)`\n",
    "```python\n",
    "inputs = inputs.permute(0, 2, 3, 1)\n",
    "```\n",
    "- **What it does**: Moves channels from dimension 1 to dimension 3\n",
    "- **Why**: We need RGB values together for each pixel\n",
    "- **Result**: Now each pixel has its 3 color values grouped together\n",
    "\n",
    "#### Step 2: **Reshape** `(batch, 32, 32, 3)` → `(batch, 32, 96)`\n",
    "```python\n",
    "inputs = inputs.reshape(-1, sequence_length, input_size)\n",
    "```\n",
    "- **What it does**: Flattens each row into a single feature vector\n",
    "- **Why**: RNN needs `(batch, seq_len, features)` format\n",
    "- **Result**: 32 rows (time steps), each with 96 features (32 pixels × 3 colors)\n",
    "\n",
    "### Comparison with Text Sequences\n",
    "\n",
    "**Your earlier image (text sequences):**\n",
    "```python\n",
    "# Text already comes as sequences!\n",
    "text_tensor = lineToTensor(\"Albert\")  # Shape: (6, 1, 58)\n",
    "# 6 time steps (characters), already in sequence format\n",
    "# NO permute/reshape needed! ✓\n",
    "```\n",
    "\n",
    "**CIFAR-10 images (need conversion):**\n",
    "```python\n",
    "# Images come as (batch, C, H, W), NOT as sequences\n",
    "images = load_cifar()  # Shape: (64, 3, 32, 32)\n",
    "# Must convert to sequence format\n",
    "images = images.permute(0, 2, 3, 1)  # ← Reorder dimensions\n",
    "images = images.reshape(-1, 32, 96)   # ← Flatten into sequences\n",
    "# NOW it's (64, 32, 96) - ready for RNN! ✓\n",
    "```\n",
    "\n",
    "### Key Difference\n",
    "\n",
    "| Data Type | Original Format | Needs Conversion? | Why? |\n",
    "|-----------|----------------|-------------------|------|\n",
    "| **Text** | Already sequential (character-by-character) | ❌ No | Text is naturally a sequence |\n",
    "| **Images** | Grid format (height × width × channels) | ✅ Yes | Images are 2D grids, not sequences |\n",
    "\n",
    "### What if we skip permute?\n",
    "\n",
    "```python\n",
    "# ❌ WRONG: Skip permute, just reshape\n",
    "images.reshape(-1, 32, 96)  # (64, 3, 32, 32) → (64, 32, 96)\n",
    "# Problem: This would group pixels incorrectly!\n",
    "# Row 1 would have: all reds from first 10 pixels, then some greens...\n",
    "# The RGB values wouldn't be together!\n",
    "```\n",
    "\n",
    "**Correct way with permute:**\n",
    "```python\n",
    "# ✓ RIGHT: Permute then reshape  \n",
    "images.permute(0, 2, 3, 1).reshape(-1, 32, 96)\n",
    "# Row 1: [R₁, G₁, B₁, R₂, G₂, B₂, ..., R₃₂, G₃₂, B₃₂]\n",
    "# Each pixel's RGB values stay together!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54871b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Understanding Image to Sequence Conversion\n",
      "======================================================================\n",
      "\n",
      "1. Original image shape: torch.Size([1, 3, 4, 4])\n",
      "   Format: (batch, channels, height, width)\n",
      "   This is how PyTorch stores images: RGB separated\n",
      "\n",
      "2. After permute(0, 2, 3, 1): torch.Size([1, 4, 4, 3])\n",
      "   Format: (batch, height, width, channels)\n",
      "   Now RGB values are together for each pixel\n",
      "\n",
      "3. After reshape(1, 4, 12): torch.Size([1, 4, 12])\n",
      "   Format: (batch, sequence_length, features)\n",
      "   Each row is now a time step with all pixel RGB values\n",
      "\n",
      "======================================================================\n",
      "For CIFAR-10:\n",
      "  (64, 3, 32, 32) → permute → (64, 32, 32, 3) → reshape → (64, 32, 96)\n",
      "  64 images, 32 time steps, 96 features per step\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Demo: Why we need permute + reshape\n",
    "print(\"=\" * 70)\n",
    "print(\"Understanding Image to Sequence Conversion\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a small example image (1 batch, 3 channels, 4x4 pixels)\n",
    "example_image = torch.randn(1, 3, 4, 4)\n",
    "print(f\"\\n1. Original image shape: {example_image.shape}\")\n",
    "print(f\"   Format: (batch, channels, height, width)\")\n",
    "print(f\"   This is how PyTorch stores images: RGB separated\")\n",
    "\n",
    "# Step 1: Permute\n",
    "permuted = example_image.permute(0, 2, 3, 1)\n",
    "print(f\"\\n2. After permute(0, 2, 3, 1): {permuted.shape}\")\n",
    "print(f\"   Format: (batch, height, width, channels)\")\n",
    "print(f\"   Now RGB values are together for each pixel\")\n",
    "\n",
    "# Step 2: Reshape\n",
    "sequence = permuted.reshape(1, 4, 12)  # 4 rows, 12 features (4 pixels × 3 colors)\n",
    "print(f\"\\n3. After reshape(1, 4, 12): {sequence.shape}\")\n",
    "print(f\"   Format: (batch, sequence_length, features)\")\n",
    "print(f\"   Each row is now a time step with all pixel RGB values\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"For CIFAR-10:\")\n",
    "print(f\"  (64, 3, 32, 32) → permute → (64, 32, 32, 3) → reshape → (64, 32, 96)\")\n",
    "print(f\"  64 images, 32 time steps, 96 features per step\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc52d23",
   "metadata": {},
   "source": [
    "## Direct Comparison: Text vs Images\n",
    "\n",
    "### Text Sequences (Your earlier example) ✅ No preprocessing needed\n",
    "\n",
    "```python\n",
    "# Text is already sequential!\n",
    "name = \"Albert\"\n",
    "text_tensor = lineToTensor(name)\n",
    "print(text_tensor.shape)  # (6, 1, 58)\n",
    "                          # ↑  ↑  ↑\n",
    "                          # |  |  └─ Features (58 one-hot characters)\n",
    "                          # |  └──── Batch size (1 name)\n",
    "                          # └─────── Sequence length (6 characters)\n",
    "\n",
    "# Feed directly to RNN - it's already in sequence format!\n",
    "for i in range(text_tensor.size()[0]):\n",
    "    output, hidden = rnn(text_tensor[i], hidden)\n",
    "```\n",
    "\n",
    "**Why no preprocessing?** Text is inherently sequential. Each character is already a time step.\n",
    "\n",
    "---\n",
    "\n",
    "### Image Sequences (CIFAR-10) ❌ Needs preprocessing\n",
    "\n",
    "```python\n",
    "# Images come in grid format, NOT sequential!\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape)  # (64, 3, 32, 32)\n",
    "                     # ↑   ↑  ↑   ↑\n",
    "                     # |   |  |   └─ Width (32 pixels)\n",
    "                     # |   |  └───── Height (32 pixels)\n",
    "                     # |   └──────── Channels (R, G, B)\n",
    "                     # └──────────── Batch size (64 images)\n",
    "\n",
    "# ❌ Can't feed directly to RNN - wrong format!\n",
    "# ✅ Must convert to sequence format first:\n",
    "\n",
    "# Step 1: Move channels to last dimension\n",
    "images = images.permute(0, 2, 3, 1)  # (64, 32, 32, 3)\n",
    "\n",
    "# Step 2: Flatten each row into features\n",
    "images = images.reshape(-1, 32, 96)  # (64, 32, 96)\n",
    "                                     # ↑   ↑   ↑\n",
    "                                     # |   |   └─ Features (32 pixels × 3 colors)\n",
    "                                     # |   └───── Sequence length (32 rows)\n",
    "                                     # └─────────Batch size (64 images)\n",
    "\n",
    "# NOW it's in sequence format - ready for RNN!\n",
    "output = rnn(images)\n",
    "```\n",
    "\n",
    "**Why preprocessing?** Images are 2D grids, not sequences. We artificially treat rows as time steps.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Aspect | Text | Images |\n",
    "|--------|------|--------|\n",
    "| **Natural format** | Sequential (character-by-character) | Grid (2D array) |\n",
    "| **As received** | Already sequence-like | Grid format `(B, C, H, W)` |\n",
    "| **Preprocessing** | None needed | `permute()` + `reshape()` |\n",
    "| **Why different?** | Text IS a sequence | Images BECOME sequences |\n",
    "| **What's a time step?** | One character | One row of pixels |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d662ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [0/782], Loss: 2.3007\n",
      "Epoch [1/5], Step [100/782], Loss: 2.3150\n",
      "Epoch [1/5], Step [100/782], Loss: 2.3150\n",
      "Epoch [1/5], Step [200/782], Loss: 2.2998\n",
      "Epoch [1/5], Step [200/782], Loss: 2.2998\n",
      "Epoch [1/5], Step [300/782], Loss: 2.2700\n",
      "Epoch [1/5], Step [300/782], Loss: 2.2700\n",
      "Epoch [1/5], Step [400/782], Loss: 2.2978\n",
      "Epoch [1/5], Step [400/782], Loss: 2.2978\n",
      "Epoch [1/5], Step [500/782], Loss: 2.3258\n",
      "Epoch [1/5], Step [500/782], Loss: 2.3258\n",
      "Epoch [1/5], Step [600/782], Loss: 2.2951\n",
      "Epoch [1/5], Step [600/782], Loss: 2.2951\n",
      "Epoch [1/5], Step [700/782], Loss: 2.3128\n",
      "Epoch [1/5], Step [700/782], Loss: 2.3128\n",
      "Epoch [1/5] completed, Loss: 2.2672\n",
      "Epoch [2/5], Step [0/782], Loss: 2.3111\n",
      "Epoch [1/5] completed, Loss: 2.2672\n",
      "Epoch [2/5], Step [0/782], Loss: 2.3111\n",
      "Epoch [2/5], Step [100/782], Loss: 2.3248\n",
      "Epoch [2/5], Step [100/782], Loss: 2.3248\n",
      "Epoch [2/5], Step [200/782], Loss: 2.2859\n",
      "Epoch [2/5], Step [200/782], Loss: 2.2859\n",
      "Epoch [2/5], Step [300/782], Loss: 2.3401\n",
      "Epoch [2/5], Step [300/782], Loss: 2.3401\n",
      "Epoch [2/5], Step [400/782], Loss: 2.2917\n",
      "Epoch [2/5], Step [400/782], Loss: 2.2917\n",
      "Epoch [2/5], Step [500/782], Loss: 2.3066\n",
      "Epoch [2/5], Step [500/782], Loss: 2.3066\n",
      "Epoch [2/5], Step [600/782], Loss: 2.3214\n",
      "Epoch [2/5], Step [600/782], Loss: 2.3214\n",
      "Epoch [2/5], Step [700/782], Loss: 2.3225\n",
      "Epoch [2/5], Step [700/782], Loss: 2.3225\n",
      "Epoch [2/5] completed, Loss: 2.2788\n",
      "Epoch [3/5], Step [0/782], Loss: 2.3112\n",
      "Epoch [2/5] completed, Loss: 2.2788\n",
      "Epoch [3/5], Step [0/782], Loss: 2.3112\n",
      "Epoch [3/5], Step [100/782], Loss: 2.3111\n",
      "Epoch [3/5], Step [100/782], Loss: 2.3111\n",
      "Epoch [3/5], Step [200/782], Loss: 2.3152\n",
      "Epoch [3/5], Step [200/782], Loss: 2.3152\n",
      "Epoch [3/5], Step [300/782], Loss: 2.3290\n",
      "Epoch [3/5], Step [300/782], Loss: 2.3290\n",
      "Epoch [3/5], Step [400/782], Loss: 2.3084\n",
      "Epoch [3/5], Step [400/782], Loss: 2.3084\n",
      "Epoch [3/5], Step [500/782], Loss: 2.2967\n",
      "Epoch [3/5], Step [500/782], Loss: 2.2967\n",
      "Epoch [3/5], Step [600/782], Loss: 2.3195\n",
      "Epoch [3/5], Step [600/782], Loss: 2.3195\n",
      "Epoch [3/5], Step [700/782], Loss: 2.3007\n",
      "Epoch [3/5], Step [700/782], Loss: 2.3007\n",
      "Epoch [3/5] completed, Loss: 2.3033\n",
      "Epoch [4/5], Step [0/782], Loss: 2.2763\n",
      "Epoch [3/5] completed, Loss: 2.3033\n",
      "Epoch [4/5], Step [0/782], Loss: 2.2763\n",
      "Epoch [4/5], Step [100/782], Loss: 2.3133\n",
      "Epoch [4/5], Step [100/782], Loss: 2.3133\n",
      "Epoch [4/5], Step [200/782], Loss: 2.3087\n",
      "Epoch [4/5], Step [200/782], Loss: 2.3087\n",
      "Epoch [4/5], Step [300/782], Loss: 2.3105\n",
      "Epoch [4/5], Step [300/782], Loss: 2.3105\n",
      "Epoch [4/5], Step [400/782], Loss: 2.3081\n",
      "Epoch [4/5], Step [400/782], Loss: 2.3081\n",
      "Epoch [4/5], Step [500/782], Loss: 2.3145\n",
      "Epoch [4/5], Step [500/782], Loss: 2.3145\n",
      "Epoch [4/5], Step [600/782], Loss: 2.3093\n",
      "Epoch [4/5], Step [600/782], Loss: 2.3093\n",
      "Epoch [4/5], Step [700/782], Loss: 2.2995\n",
      "Epoch [4/5], Step [700/782], Loss: 2.2995\n",
      "Epoch [4/5] completed, Loss: 2.3405\n",
      "Epoch [5/5], Step [0/782], Loss: 2.3136\n",
      "Epoch [4/5] completed, Loss: 2.3405\n",
      "Epoch [5/5], Step [0/782], Loss: 2.3136\n",
      "Epoch [5/5], Step [100/782], Loss: 2.3180\n",
      "Epoch [5/5], Step [100/782], Loss: 2.3180\n",
      "Epoch [5/5], Step [200/782], Loss: 2.3177\n",
      "Epoch [5/5], Step [200/782], Loss: 2.3177\n",
      "Epoch [5/5], Step [300/782], Loss: 2.3261\n",
      "Epoch [5/5], Step [300/782], Loss: 2.3261\n",
      "Epoch [5/5], Step [400/782], Loss: 2.3217\n",
      "Epoch [5/5], Step [400/782], Loss: 2.3217\n",
      "Epoch [5/5], Step [500/782], Loss: 2.2951\n",
      "Epoch [5/5], Step [500/782], Loss: 2.2951\n",
      "Epoch [5/5], Step [600/782], Loss: 2.3087\n",
      "Epoch [5/5], Step [600/782], Loss: 2.3087\n",
      "Epoch [5/5], Step [700/782], Loss: 2.3087\n",
      "Epoch [5/5], Step [700/782], Loss: 2.3087\n",
      "Epoch [5/5] completed, Loss: 2.3293\n",
      "Finished Training\n",
      "Epoch [5/5] completed, Loss: 2.3293\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # CIFAR-10: (batch, 3, 32, 32) -> (batch, 32, 96)\n",
    "        # Reshape: batch_size x 3 x 32 x 32 -> batch_size x 32 x 96\n",
    "        inputs = inputs.permute(0, 2, 3, 1)  # (batch, 32, 32, 3)\n",
    "        inputs = inputs.reshape(-1, sequence_length, input_size)  # (batch, 32, 96)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = rnn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{n_epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    print(f'Epoch [{epoch + 1}/{n_epochs}] completed, Loss: {loss.item():.4f}')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93362fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.09%\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        # Reshape the same way as in training\n",
    "        X_batch = X_batch.permute(0, 2, 3, 1)  # (batch, 32, 32, 3)\n",
    "        X_batch = X_batch.reshape(-1, sequence_length, input_size)  # (batch, 32, 96)\n",
    "        \n",
    "        outputs = rnn(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b9782",
   "metadata": {},
   "source": [
    "# Method 1 Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3c40e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (input_to_hidden): Linear(in_features=186, out_features=128, bias=True)\n",
      "  (input_to_output): Linear(in_features=186, out_features=18, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 128 # start small\n",
    "model = RNNModel(n_letters, n_hidden, len(alldata.labels_uniq)).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02ae55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8762, -2.9271, -2.8852, -2.9765, -2.8594, -2.8838, -2.7746, -2.8978,\n",
      "         -2.9350, -2.9942, -2.8139, -2.9554, -2.9709, -2.8508, -2.8357, -2.8770,\n",
      "         -2.9392, -2.8074]], grad_fn=<LogSoftmaxBackward0>)\n",
      "('Russian', 6)\n"
     ]
    }
   ],
   "source": [
    "def label_from_output(output, output_labels):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    label_i = top_i[0].item()\n",
    "    return output_labels[label_i], label_i\n",
    "\n",
    "hidden_state = model.initHidden()\n",
    "input = lineToTensor('Albert')\n",
    "\n",
    "# Loop through each character in the sequence\n",
    "for i in range(input.size()[0]):\n",
    "    output, hidden_state = model(input[i], hidden_state)\n",
    "    \n",
    "print(output)\n",
    "print(label_from_output(output, alldata.labels_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa5d3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "n_batch_size = 64\n",
    "report_every = 50\n",
    "learning_rate = 0.2\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9386d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def train(rnn, training_data):\n",
    "    \"\"\"\n",
    "    Learn on a batch of training_data for a specified number of iterations and reporting thresholds\n",
    "    \"\"\"\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "    model.train()\n",
    "    print(f\"training on data set with n = {len(training_data)}\")\n",
    "\n",
    "    for iter in range(1, n_epoch + 1):\n",
    "        model.zero_grad() # clear the gradients\n",
    "\n",
    "        # create some minibatches\n",
    "        # we cannot use dataloaders because each of our names is a different length\n",
    "        batches = list(range(len(training_data)))\n",
    "        random.shuffle(batches)\n",
    "        batches = np.array_split(batches, len(batches) //n_batch_size )\n",
    "\n",
    "        for idx, batch in enumerate(batches):\n",
    "            batch_loss = 0\n",
    "            for i in batch: #for each example in this batch\n",
    "                (label_tensor, text_tensor, label, text) = training_data[i]\n",
    "                \n",
    "                # Initialize hidden state for this example\n",
    "                hidden = rnn.initHidden()\n",
    "                \n",
    "                # Loop through each character in the sequence\n",
    "                for j in range(text_tensor.size()[0]):\n",
    "                    output, hidden = rnn(text_tensor[j], hidden)\n",
    "                \n",
    "                loss = criterion(output, label_tensor)\n",
    "                batch_loss += loss\n",
    "\n",
    "            # optimize parameters\n",
    "            batch_loss.backward()\n",
    "            nn.utils.clip_grad_norm_(rnn.parameters(), 3)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            current_loss += batch_loss.item() / len(batch)\n",
    "\n",
    "        all_losses.append(current_loss / len(batches) )\n",
    "        if iter % report_every == 0:\n",
    "            print(f\"{iter} ({iter / n_epoch:.0%}): \\t average batch loss = {all_losses[-1]}\")\n",
    "        current_loss = 0\n",
    "\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cbe5935b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on data set with n = 17063\n",
      "training took 578.5927290916443s\n",
      "training took 578.5927290916443s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "all_losses = train(model, train_set)\n",
    "end = time.time()\n",
    "print(f\"training took {end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa01340e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAALqpJREFUeJzt3Qd01fX9//F39t4hISEJgQCCAiFMkaFUHIhUWltXFUSx9ldUlJ6qVGv1XyvVVsUqap24R6tA3VUsBBBBkCB7SCCBDJJA9h7/8/lkkAAJGTf53Hu/z8c533MH98obA7mvfMb741JXV1cnAAAAhria+o0BAAAUwggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAo9zFAdTW1kpGRoYEBASIi4uL6XIAAEA7qL6qRUVFEh0dLa6uro4dRlQQiY2NNV0GAADohPT0dImJiXHsMKJGRBr/MIGBgabLAQAA7VBYWKgHExo/xx06jDROzaggQhgBAMCxnGmJBQtYAQCAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARlk6jHy2LVMWvJciOzIKTJcCAIBlWTqMLNtyRD7cckRW7ckxXQoAAJZl6TAyaVAvfZu8lzACAIAplg4jkweG69vv045LcUW16XIAALAkS4eRvmF+EhvqI1U1dbLhQJ7pcgAAsCRLhxFl0sD6qZo1+3JNlwIAgCVZPow0TtWs2ce6EQAATLB8GBmfEC6uLiI/5pTIkfwy0+UAAGA5lg8jQT4eMiI2WN9fy+gIAAA9zvJhRJnYsG4kmXUjAAD0OMJIs3Uj6/bnSk1tnelyAACwFMKIiCTGBkuAl7vkl1bRGh4AgB5GGBERDzdXGZ8Qpu+zxRcAADsPI8nJyTJjxgyJjo4WFxcXWb58eZuvX7t2rUyYMEHCwsLEx8dHBg8eLE8++aTYm0kNUzW0hgcAoGe5d/QNJSUlkpiYKDfddJP8/Oc/P+Pr/fz85LbbbpPhw4fr+yqc3Hrrrfr+r3/9a7G35meqNXxJRbX4eXX4fw0AAOiEDn/iTps2TV/tlZSUpK9G8fHx8uGHH8qaNWvsKoz0DfPVreHTj5XJhtQ8+cngSNMlAQBgCT2+ZmTLli3yzTffyPnnn9/qayoqKqSwsLDF1d3UlFPj6EjyXtaNAADgdGEkJiZGvLy8ZPTo0TJv3jyZO3duq69dtGiRBAUFNV2xsbE9UiOt4QEAcOIwoqZlNm3aJM8//7wsXrxY3nnnnVZfu3DhQikoKGi60tPTe7w1fAat4QEA6BE9tkqzX79++nbYsGGSnZ0tDz74oFx77bWnfa0aQVGXidbwqufIlrR8WbsvV64a0zMjMgAAWJmRPiO1tbV6XYg9alo3wlQNAAD2OTJSXFws+/fvb3qcmpoqKSkpEhoaKnFxcXqK5ciRI/L666/rX1+yZIl+XvUXaexT8ve//13uuOMOsUdq3cg/Vu6TtQ2t4d3UvA0AALCfMKLWfUyZMqXp8YIFC/Tt7NmzZenSpZKZmSlpaWktRkFUQFGhxd3dXRISEuTRRx/VvUYcoTX88Jj6E30BAED3cKmrq7P7k+HU1l61q0YtZg0MDOz23++W1zfJlzuz5feXnCXzpgzo9t8PAABn1N7Pb86mOQ22+AIA0HMII20sYt18qL41PAAA6D6EkTZaw1fV1OnW8AAAoPsQRlppDT9xAK3hAQDoCYSRVrBuBACAnkEYacV5tIYHAKBHEEZaEeRb3xpeUa3hAQBA9yCMtIHW8AAAdD/CSBsmNawbWbc/V2pr7b43HAAADokw0oYRscHi7+Uux3Vr+ELT5QAA4JQII23wcHOV8Qlh+j5TNQAAdA/CyBmwxRcAgO5FGDkDWsMDANC9CCPtaA0fE0JreAAAugthpB2t4RtHR9bQbwQAAJsjjHRo3QhhBAAAWyOMdKA1/P6jxbSGBwDAxggj7WwNPzyG1vAAAHQHwkhHp2r2E0YAALAlwkg7TRpUv4h17b4cWsMDAGBDhJF2ojU8AADdgzDSTrSGBwCgexBGOnGKL63hAQCwHcJIJ1vDl1bSGh4AAFsgjHRAfPPW8AeOmS4HAACnQBjpZGt41o0AAGAbhJEOojU8AAC2RRjpQmv4zAJawwMA0FWEkS60hmd0BACAriOMdAJTNQAA2A5hpBNoDQ8AgO0QRjqB1vAAANgOYaSTreHP7V/fGn7Nfrb4AgDQFYSRTpo8qGHdyF7WjQAA0BWEkU5qbH626dAxWsMDANAFhJFOojU8AAC2QRjpUmv4+qkaWsMDANB5hBEbTNWspd8IAACdRhjpgvMSwnRr+H20hgcAoNMII10Q7OtJa3gAALqIMNJFtIYHAKBrCCNdNLFh3ci6/bm0hgcAoBMII12UFBcsfp5ucqykUnZm0hoeAICOIozYoDX8+AS2+AIA0FmEERugNTwAAJ1HGLGBiQPqwwit4QEA6DjCiA30C/eTPsENreFTaQ0PAEBHEEZs1BqeqRoAADqHMGLj1vBrWMQKAECHEEZshNbwAAB0DmHEhq3hh9EaHgCADiOMdENreE7xBQCg/Qgj3bBuZC2t4QEAaDfCiA3RGh4AgI4jjNgQreEBAOg4woiNTWLdCAAAHUIY6aYwsungcVrDAwDQDoSRbmoNX1lTS2t4AADagTBiY7SGBwCgYwgj3WDiAFrDAwDQXoSRbjBhQJi4NLSGzyooN10OAAB2jTDSTa3hhze1hmd0BACAthBGurk1POfUAADQNsJIN6E1PAAA7UMY6Sa0hgcAoH0II93aGj5M32eqBgAAG4aR5ORkmTFjhkRHR+ueGsuXL2/z9R9++KFcdNFF0qtXLwkMDJTx48fLF198IVaaqmERKwAANgwjJSUlkpiYKEuWLGl3eFFh5NNPP5XNmzfLlClTdJjZsmWLODtawwMAcGbu0kHTpk3TV3stXry4xeNHHnlEVqxYIR999JEkJSWJFVrDH8kv063hp5wVYbokAADsTo+vGamtrZWioiIJDQ1t9TUVFRVSWFjY4nJEahqLU3wBALCzMPL3v/9diouL5aqrrmr1NYsWLZKgoKCmKzY2VhwV60YAALCjMPL222/LQw89JO+//75ERLQ+ZbFw4UIpKChoutLT08XRW8PvzaY1PAAARsPIu+++K3PnztVBZOrUqW2+1svLS++8aX45KlrDAwBgB2HknXfekTlz5ujb6dOni9VMGkBreAAAbBZG1HqPlJQUfSmpqan6flpaWtMUy6xZs1pMzajHjz/+uIwbN06ysrL0paZfrKJxEes6WsMDAND1MLJp0ya9JbdxW+6CBQv0/QceeEA/zszMbAomygsvvCDV1dUyb948iYqKarrmz58vVpEUF6Jbw+fRGh4AgK73Gbngggukrq71n+6XLl3a4vGqVavE6jzd61vDf7XrqJ6qGdonyHRJAADYDc6m6SFs8QUA4PQIIz1kYrPW8GWVNabLAQDAbhBGekj/htbwlTW1siE1z3Q5AADYDcKIgdbwbPEFAOAEwkgPYt0IAACnIoz0IFrDAwBwKsJIT7eGb9jWu3Y/UzUAACiEkR7GVA0AAC0RRnpY4yLWtftoDQ8AgEIY6WG0hgcAoCXCiIHW8Of2D9P32eILAABhxOxUzX7WjQAAQBgxYNKg+kWs36XSGh4AAMKIAbSGBwDgBMKIAbSGBwDgBMKI4VN81RZfAACsjDBiyISEcN0afk92kWQX0hoeAGBdhBFDQvxOtIZnqgYAYGWEEYNoDQ8AAGHEKFrDAwBAGDHeGt63oTX8rixawwMArIkwYrg1/HhawwMALI4wYtiJfiOsGwEAWBNhxDBawwMArI4wYget4aODvHVr+I0Hj5kuBwCAHkcYsYvW8A1bfPcyVQMAsB7CiB2YNIhzagAA1kUYsQO0hgcAWBlhxA7QGh4AYGWEEbs7xZd1IwAAayGM2InGRaxr99MaHgBgLYQROzGyoTV8bjGt4QEA1kIYsRO0hgcAWBVhxI7QGh4AYEWEETsysWHdyHcHaQ0PALAOwogdSejV0Bq+mtbwAADrIIzYEVrDAwCsiDBiZ2gNDwCwGsKIHbeGP0preACABRBG7LA1/DBawwMALIQwYofY4gsAsBLCiB2iNTwAwEoII3aI1vAAACshjNhpa/hzG1rDr2XdCADAyRFG7H7dCGEEAODcCCN2vm5EdWKlNTwAwJkRRuwUreEBAFZBGLFTtIYHAFgFYcSOTWxYN6K2+AIA4KwII3ZswoD61vC7s2gNDwBwXoQROxZKa3gAgAUQRuwcreEBAM6OMOIwreHzaA0PAHBKhBGHaQ1fodeOAADgbAgjDtQanqkaAIAzIow4AFrDAwCcGWHEwVrDl1fRGh4A4FwIIw7SGj6qsTV8Kq3hAQDOhTDiMK3h2eILAHBOhBEH0XRODetGAABOhjDiIGgNDwBwVoQRB2wNz8F5AABnQhhxIBMHsMUXAOB8CCMOum6E1vAAAGdBGHEgI/sG0xoeAOB0OhxGkpOTZcaMGRIdHa23nC5fvrzN12dmZsp1110ngwYNEldXV7nzzju7Uq+lebm70RoeAOB0OhxGSkpKJDExUZYsWdKu11dUVEivXr3k/vvv1+9D1zT2G2ERKwDAWbh39A3Tpk3TV3vFx8fLU089pe+/8sorHf3t0EoY2ZBa3xre28PNdEkAADjfmhE1mlJYWNjiQr2EXv60hgcAOBW7DCOLFi2SoKCgpis2NtZ0SXaD1vAAAGdjl2Fk4cKFUlBQ0HSlp6ebLsmu0BoeAGDpNSM9wcvLS19oX2v4iEBv0yUBAOBcIyM4c2v4odG0hgcAWDSMFBcXS0pKir6U1NRUfT8tLa1pimXWrFkt3tP4evXenJwcfX/nzp22+jNY0ol1I4QRAIDFpmk2bdokU6ZMaXq8YMECfTt79mxZunSpbnLWGEwaJSUlNd3fvHmzvP3229K3b185ePBg16q3+LqRZ1f9qMNIXV2dXtgKAIAlwsgFF1ygP/xaowLJydp6PWzTGn5IVKDpkgAA6BTWjDhwa/hx/UL1fbb4AgAcGWHEgbHFFwDgDAgjDmzyoJat4QEAcESEEQdGa3gAgDMgjDhJa3j6jQAAHBVhxMFNbFg3kryXRawAAMdEGHFwE09qDQ8AgKMhjDhRa/j/7TlquhwAADqMMOIEpg6J1Ld//niXbDtcYLocAAA6hDDiBG49v79ugFZcUS2zX90o+48Wmy4JAIB2I4w4AW8PN3lp9mgZHhMkx0oq5YaXN8jh46WmywIAoF0II04iwNtDls4ZKwMj/CWzoFyuf2mD5BRVmC4LAIAzIow42WLWN24eJzEhPnIwr1SPkBSUVpkuCwCANhFGnEzvIG958+Zx0ivAS2/3nbN0o5RWVpsuCwCAVhFGnFB8uJ+8cfNYCfLxkO/T8uXWNzZLRTVn1wAA7BNhxEkN7h0or84ZI76ebvpU3zvfTZHqmlrTZQEAcArCiBMbGRciL9wwWjzdXOWz7Vnyh2XbpK6uznRZAAC0QBhxchMHhss/rk0SVxeR9zcdloc/2UUgAQDYFcKIBVw6tLc89otEff/ltany9Nf7TZcEAEATwohF/GJUjDxw+dn6/hNf7pWl61JNlwQAgEYYsZCbJvaTO6cO1Pcf/GinfLD5sOmSAAAgjFjN/AsHypwJ8fr+3R/8IP/dkWW6JACAxRFGLMbFxUX+OP1sPW1TU1snt729Rb7Zn2u6LACAhRFGLMjV1UX++vNhcuk5vaWyplbmvr5JtqQdN10WAMCiCCMW5e7mKk9dO0ImDgiX0soaufHV72RPVpHpsgAAFkQYsTAvdzf55w2jJCkuWArKqvTBeml5pabLAgBYDGHE4vy83GXpjWNlcO8AOVpUIb96+VvJLiw3XRYAwEIII5AgXw95/eax0jfMV9KPlcn1L22Q4yWVpssCAFgEYQRaRIC3vHnzOIkM9JJ9R4vlxlc3SnFFtemyAAAWQBhBk9hQXx1IQnw9ZOvhArnltU1SXlVjuiwAgJMjjKCFgZEB8tpNY8Xfy13WH8jTfUiqampNlwUAcGKEEZxieEywvDhrtHi6u8pXu7Ll7n//ILW1nPQLAOgehBGc1viEMHnuVyPF3dVFlm05Ig99tEPq6ggkAADbI4ygVRcOiZTHr0oUFxeR19Yf0qf9AgBga4QRtOmKEX3k/10xVN9/+uv98tKaA6ZLAgA4GcIIzuiGc/vK7y85S99/+JNd8t53aaZLAgA4EcII2uW3FyTIrZP76/sLP9wmn27LNF0SAMBJEEbQLi4uLnLvtMFy7dhYURtr5r+7RVbvzTFdFgDACRBG0KFA8vDMYTJ9eJRU1dTJrW9skk0Hj5kuCwDg4Agj6BA3Vxd58qoRcsFZvaS8qlbmLP1OdmQUmC4LAODACCPoMNUM7blfjZIx8SFSVF4ts1/ZKAdyik2XBQBwUIQRdIqPp5u8fOMYOSc6UHKLK+WGlzdKRn6Z6bIAAA6IMIJOC/T20OfY9A/3kyP5ZXL9yxskt7jCdFkAAAdDGEGXhPt7yRtzx0l0kLccyCnRUzaF5VWmywIAOBDCCLqsT7CPvDl3nIT5ecqOjEKZu3STlFXWmC4LAOAgCCOwif69/OX1m8dKgLe7bDx4TP7vrc1SWV1ruiwAgAMgjMBmzokOkldvHCPeHq6yak+OLHg/RWpUhzQAANpAGIFNjY4PleevHyUebi7y8Q+Zcv/ybVJXRyABALSOMAKbu+CsCFl8dZK4uoi8szFd/vr5btMlAQDsGGEE3UK1jH/kZ8P0/X+uPiDPrtpvuiQAgJ0ijKDbXDM2Tv5w2WB9/7HP98gb3x4yXRIAwA4RRtCtfj05QW6bMkDff2DFdlmRcsR0SQAAO0MYQbf73cWDZNb4vqLWsf7u/a2ycle26ZIAAHaEMIJu5+LiIg/OOEdmjoiW6to6+e1b38u3B/JMlwUAsBOEEfQIV1cX+dsvE2XqkAipqK6Vua9tkh8O55suCwBgBwgj6DEebq7yzHUj5dz+oVJcUa3PsdmXXWS6LACAYYQR9ChvDzd5afYYGR4TJMdLq+SGlzdK+rFS02UBAAwijKDH+Xu5y9I5Y2VghL9kFZbL9S9vkKNF5abLAgAYQhiBEaF+nvLGzeMkJsRHDuWVyqyXN0pBaZXpsgAABhBGYEzvIG95a+446RXgJbuziuTGpRulpKLadFkAgB5GGIFRfcP85I2bx0qQj4dsScuX37y5WSqqa0yXBQDoQYQRGDe4d6AsnTNGfD3dZM2+XPnFc+tlR0aB6bIAAD2EMAK7kBQXIi/NGi0B3u6y7UiB/PSZdbLos11SVskoCQA4O8II7MZ5A8Jl5YLz9Ym/NbV1+rTfixevluS9OaZLAwB0I8II7EpEoLcsuW6kHiWJCvKW9GNlMuuVjXLXeymSV1xhujwAgD2EkeTkZJkxY4ZER0frM0eWL19+xvesWrVKRo4cKV5eXjJgwABZunRpZ+uFRUw9O1K+XHC+3HhevLi4iCzbckSmPrFaPth8WOrUiXsAAOuGkZKSEklMTJQlS5a06/Wpqakyffp0mTJliqSkpMidd94pc+fOlS+++KIz9cJizdEe/Ok5suy3E2Rw7wDdsfV3/9qqm6QdzC0xXR4AwEZc6rrwY6YaGVm2bJnMnDmz1dfcc8898sknn8j27dubnrvmmmskPz9fPv/883b9PoWFhRIUFCQFBQUSGBjY2XLhwKpqauWlNamy+Ku9+qA9L3dXmT91oNwyqb8+8wYAYH/a+/nd7d/F169fL1OnTm3x3CWXXKKfb01FRYX+AzS/YG0qcPzfBQnyxZ2TZcKAMB1IHvt8j8x4eq2kpHP6LwA4sm4PI1lZWRIZGdniOfVYBYyysrLTvmfRokU6STVesbGx3V0mHER8uJ+8efM4efyXiRLi66E7t/7s2XXy0Ec79EnAAADHY5fj2wsXLtRDOo1Xenq66ZJgR9T04JWjYuSrBefLz5L6iJpofHXdQbn4idWycle26fIAAPYWRnr37i3Z2S0/INRjNXfk4+Nz2veoXTfq15tfwMnC/L3kyatHyOs3jZXYUB/JKCiXm1/bJPPe+l6OFnIKMAA4im4PI+PHj5eVK1e2eO7LL7/UzwO2MHlQL72W5NbJ/cXN1UU+2ZYpFz6xWt7ZmCa1tWwDBgCnCyPFxcV6i666GrfuqvtpaWlNUyyzZs1qev1vfvMbOXDggNx9992ye/duefbZZ+X999+Xu+66y5Z/Dlicr6e7LLxsiKyYN0GG9QmSovJqWfjhNrnmhW9l/9Fi0+UBAGy5tVc1MFM9Q042e/Zs3czsxhtvlIMHD+rXNX+PCh87d+6UmJgY+eMf/6hf115s7UVHVNfUymvrD8nj/90jpZU14unmKr+dkqB343i5u5kuDwAso7Cdn99d6jPSUwgj6IzDx0vl/uXbZdWe+rNtBkT4y6KfD5Mx8aGmSwMASyi0lz4jgCkxIb7y6o1j5B/XJkm4v6eervnl8+vlvmXbpKCsynR5AIAGhBE4/TbgnyZG623AV4+u71fz1oY0ueiJ1fLZtkzOuQEAO0AYgSUE+3rKo78YLu/ccq70D/eTo0UV8n9vfS+3vL5ZMvJP33wPANAzCCOwlPEJYfLp/Ely+08GiLuri3y1K1uPkrz2zUGpYRswABhBGIHleHu4ye8uPks+uWOSjIwLlpLKGvnTf3bIlc99I7uzOAcJAHoaYQSWdVbvAPn3b86TP19xjvh7uesD9y7/x1p57PPdUl5VY7o8ALAMwggszdXVRW4YH68XuF58dqRU19bJs6t+lEsXJ8s3+3NNlwcAlkAYAdQZSkHe8sKs0fL89aMkMtBLDuaVynUvbZDf/2urHC+pNF0eADg1wgjQzKVDe8uXC86XG87tKy4uIv/afFimPrFaVqQcYRswAHQTwghwkkBvD/nzzKHy79+Ml0GR/pJXUinz302R2a9+J+nHSk2XBwBOhzACtGJU31D5+PZJ8ruLBunzbZL35sjFTybLi8kH9Pk3AADbIIwAbfB0d5XbLxwon905Scb2C5Wyqhr5y6e75Iol62Tb4QLT5QGAUyCMAO2Q0Mtf3r3lXHn0ymES6O0uOzIK5Yola+Xhj3dKaWW16fIAwKERRoAObAO+ekycfPW78+Xy4VGiGra+tDZVLnoiWVbtOWq6PABwWIQRoIMiArzlmetGyis3jpY+wT5yJL9Mbnz1O5n/7hbJLa4wXR4AOBzCCNBJPxkcKf+9a7LcPLGfuLqIrEjJkAsfXy3vb0pnGzAAdIBLnQN81ywsLJSgoCApKCiQwMBA0+UAp9iani/3frhNdmXWn20zvn+Y3DNtsCTGBImLalgCABZU2M7Pb8IIYCNVNbXyytpUefKrvVJeVb/1t3+4n8xM6iMzR/SRuDBf0yUCQI8ijACGpOWVyuNf7pEvdmQ1hRJlVN8QHUwuHxYlIX6eRmsEgJ5AGAEMK66oli+2Z8nylCOybn+u3n2juLu6yAVnRcjPkvrIhUMixNvDzXSpANAtCCOAHckuLJePtmbIsi1HdI+SRgFe7jJtWG89YnJuvzC9fRgAnAVhBLBTe7OLZPmWI3r3jdoW3CgqyFt+OiJaj5gM7s3fcwCOjzAC2Lna2jr57uAxPY3zyQ+ZUlh+opPr4N4BOpSocBIV5GO0TgDoLMII4EAqqmvkf7tz9IjJ17uPSmXDQXxqV7DaJqymcS4d2lufKAwAjoIwAjiogtIq+XR7pl5fsjH1WItD+y4aEqmDyfmDeunHAGDPCCOAEzh8vFSvLVHBZP/R4qbng3099Pk4aipnZFwIjdUA2CXCCOBE1D9TtQtHL3zdmiE5RSfOwIkL9ZWZI6LliqQ++nRhALAXhBHASdXU1sk3P+bq0ZLPt2dJaWVN06+p9vNqGmdGYrSE+3sZrRMACgkjgPMrrayWL3dm6xGT5H25Oqgobq4uMmlguJ7GuejsSPH1dDddKgALKiSMANaSW1whH6vGaikZ+uC+Rr6ebnLpOfWN1c5LCBN3Nxa+AugZhBHAwlJzS/RoiephciivtOn5XgFe8tPE+sZq50QHsvAVQLcijADQC1+3pOfrYKLa0R8vrWr6tQER/vWN1RKjJTaUE4UB2B5hBEALldW1smZfjl74qtaZVFSfOFF4bHyoXJEULdOHRUmwLycKA7ANwgiAVhWVV+mdOGoa55sf86Txu4CHm4tMOStCfj6yjz5ZmBOFAXQFYQRAu2QVlMt/th6RZVsyZFdmsxOFvd31SMl5A8IlPsxX+ob6SZAv7egBtB9hBECH7c5SjdUyZEXKEcksKD/l11Xn175hfvXhpOm2/n6YnycLYgG0QBgB0KUThTekHpNPtmXI3qxiOZhXIkebdX09HX8vdx1M4sP8mm7jGm4jArzE1ZWgAlhNIWEEgK0brKltwvVXiRxsuFWPMwrKmtadnI63h6ue5tEhJdxPt7BvDC3RwT66SRsA59Pez2/aMgJoF9XFdUhUoL5OVl5VI4ePl7UIKY236vnyqlrZk12kr5OpRbNqa7EeSdG3vtI3XE0B+UlMiI940KQNcHqEEQBdpnbdqL4l6jpZVU2tZOSXnQgpuQ0jKsdKJS2vVCprauVATom+TqZGTPoE+zSsS2kcTalfq6ICDLt9AOfANA0AY9RZOlmF5XIot+W0z8GG27KqE4cAnkytlY0K9G5al9J8Ya0KLn5e/KwFmMaaEQAOTX1ryimq0CFFhZO0ZiHlYG6JFFVUt/l+dWpx810/ibHBMj4hjGkfoAcRRgA4LfVtS7W2rw8n9QGl+YjKsZLK074vxNdDLh3aW6YPi5Zz+4dyaCDQzQgjACyroKxKj6QcOlYfTn48Wiyr9+ZIXrOQEurnqYPJ5cOjZFy/MHb0AN2AMAIAzVTX1OreKR//kCmfb89scWhguL+nTBsaJdOHR8mY+FCCCWAjhBEAaCOYrD+QJ5+oYLIjS/KbBZNeAV5ymZrKGR4to/uG0KwN6ALCCAC0g9p6rA4L/OSHDH14YGH5iYWxkYFeesRkRmKUJMUSTICOIowAQAdVVtfKuv25eirnvzuzpKhZMIkK8pbLhtVP5STFBnMOD9AOhBEA6IKK6hpZuy9XT+X8d2e2FDfbSqwasV02rH4qJzEmiGACtIIwAgA2otrdr9HBJEO+3JktJZUnmrGplvVqtOTyYdEytE8gwQRohjACAN0UTFbtyZFPtmXKyl3ZUtosmKizdVQwmT4sSs6JJpgAhYQRAOheZZUqmByVj7dlyte7jrZoX6+6vtYHk2gZEhVAMIElFRJGAKDnlFZWy/92qxGTDPl691F9UnGj/uF+9cFkeJScFUkwgXUUEkYAwIySimpZufuoXmPyvz05epdOI3WysZrGUZ1fB0YGGK0T6G6EEQCwA2oXjlpborYLr1bBpOZEMBkUqYJJtB4xUSEFcDaEEQCwM4XlVfXBZGumJO/LkaqaE99+B/cO0CMmKpj070UwgXMgjACAnR/mp7YJq6kctW24uvbEt+KzowKbduXEh/sZrRPoCsIIADiI/NJK3VhNNVhTHWCbBxPVu0RP5QyLkrgwX6N1Ah1FGAEAB3S8RAWTLL3GRJ2ZU9MsmAyPCZKLhkTKqL4hkhgbLH5e7kZrBc6EMAIADi6vuEK+2JGttwuv/zFPmuUSUWf2De4dqIPJyL7BMiouVGJDfdg2DLtCGAEAJ5JbXKFPFV5/IE+2HDouGQXlp7wm3N9TRsapcBKiQ8qwPkHi7eFmpF5AIYwAgBPLLCiT7w/ly+ZDx+X7tOOyI6Ogxe4cxd3VRbelV+FEhRQVUKKDfYzVDOspJIwAgLXOzNl+pEAHExVQNh/K16MpJ+sd6K1DSVJcsL49JzpIPN1djdQM51dIGAEA61Lf2g8fL2sKJ+p2V2ZRiwWxigoiw/sENYyeBOsRlIhAb2N1w7kQRgAAp5yfszW9fvTk+4aAcry06pTXxYT41C+MbZjaUQ3Z3N0YPYGdhZElS5bI3/72N8nKypLExER5+umnZezYsad9bVVVlSxatEhee+01OXLkiJx11lny6KOPyqWXXmrzPwwAoP3Ut//U3BL5Pq1+7cmWtOOyJ7tITv5U8PFwk8TYoKZwkhQXIqF+nqbKhgPptjDy3nvvyaxZs+T555+XcePGyeLFi+Vf//qX7NmzRyIiIk55/T333CNvvvmmvPjiizJ48GD54osvZMGCBfLNN99IUlKSTf8wAICut6zfmp5fvzg2rT6gFJVXn/I6dRKxCiWNW4sHRgSIm9pvDPREGFEBZMyYMfLMM8/ox7W1tRIbGyu333673Hvvvae8Pjo6Wu677z6ZN29e03NXXnml+Pj46JDSHoQRADCjtrZO9ucU62mdxrUnP+aUnPI6fy93vSi2MaCMiA2WIB8PIzXDfrT387tD7fsqKytl8+bNsnDhwqbnXF1dZerUqbJ+/frTvqeiokK8vVsuhlJBZO3atR35rQEABri6usigyAB9XTM2rql9/Za0/KbFsSnp+fp0YnXGjroU1XttYIR/U98TdZvQy4+mbOh6GMnNzZWamhqJjIxs8bx6vHv37tO+55JLLpEnnnhCJk+eLAkJCbJy5Ur58MMP9X+nNSrAqKt5sgIA2IdgX0+ZMjhCX0p1Ta1ea6LWnjQujD2UVyp7s4v19e536Q3v85Ck2PodO0NjgiQu1Ff6BPvQmA0dCyOd8dRTT8ktt9yi14uoRKwCyZw5c+SVV15p9T1qwetDDz3U3aUBAGxA7bRR/UrUdcO5ffVzqsdJfTCpDyhbD+dLfmmV/G9Pjr6aiwz0ktgQX72LJzbUt/5+qI++jQryZiePBXRozYiapvH19ZV///vfMnPmzKbnZ8+eLfn5+bJixYpW31teXi55eXl6DYlaW/Lxxx/Ljh072j0yotalsGYEABxTZXWt7MosbJra2X+0WNKPlUpJZeuj5IpaFKsCiQom6uyd+tsTwaWXv5eeSoKF1ox4enrKqFGj9FRLYxhRC1jV49tuu63N96p1I3369NFbfT/44AO56qqrWn2tl5eXvgAAzkE1V1MnDatrzoR++jn1s7Dqc6JCSfrxUt2krf5+mRw+Vv+4sqZW36pr/YHT/3d1MDlpZKUxuKipIdapOOE0jdqWq0ZCRo8erXuLqK29JSUleupFUdt+VehQUy3Khg0bdH+RESNG6NsHH3xQB5i7777b9n8aAIDDUCFB9StRlwopp9vJc7SoQg4frw8r6ccaw0r9fXU+jxpxOZBToq/TUbt8VEiJaWVkRf06zOvwV+Hqq6+WnJwceeCBB3TTMxUyPv/886ZFrWlpaXqHTfPpmfvvv18OHDgg/v7+ctlll8kbb7whwcGn/sUDAKCRmn7pHeStr9Hxoaf8elVNrWQVlLcIKM1HWFSQUbt8dmcV6et0Qnw9WqxT0aGlIaiwuLbn0A4eAOC0hwfqYNIQUA6fFFrUgtozYXGtHa4ZAQDAUahRjQER/vo6naLyqhbrVNStmhJqfE4trs0urNDXpkPHT7u4NjrYW/qF+0u/MF+JD/eTfg2XGlUhqLQfYQQAYEkB3h4yJEpdp/7E3t7FtfXrWMok+aT3e7i56JEU1TY/PsxP+vXyk34Nt5EB3uwAOglhBACATi6uPZRXIgfzSiQ1t1RSc4vloLrNK2lzYa23h2t9QGkYRYkP96sPLeF+EubnacndP6wZAQDAhlRQySwsl9ScEh1M1G19YCnRoyvVta1/7AZ4uevRk+ZhpTGwOOJZP912UJ4JhBEAgDNQO4DUFM/B3BI5kFuib1MbroyCMmnrE1mNmjRfl6JDSpgKKr7i62mfEx2EEQAAHGz3T9qx0qZw0jywqCmhtvQO9NahRC+mbXar1q14uZvbnkwYAQDASRRXVDeNojSNpjRM/bS1RVmtk+0T4tO04+fEGhV//bzaEdSdCCMAAFjA8ZJKHUyaT/k0hpa2zv45ecfPT0dEy/AY2zYkpc8IAAAWEOLnqa+RcSEtnldjDTlFFfXBJK/lGpWDeaWn7PgZFhNk8zDSXoQRAACckIuLi0QEeutrXP+wU3b8qAWzeitybrHemjysT5CxWgkjAABYjKuriz6HR10TB4abLkfoVQsAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIxyiFN76+rq9G1hYaHpUgAAQDs1fm43fo47dBgpKirSt7GxsaZLAQAAnfgcDwoKavXXXerOFFfsQG1trWRkZEhAQIC4uLjYNLGpgJOeni6BgYE2+++i8/ia2Be+HvaFr4d94etxZipiqCASHR0trq6ujj0yov4AMTEx3fbfV3+J+ItkX/ia2Be+HvaFr4d94evRtrZGRBqxgBUAABhFGAEAAEZZOox4eXnJn/70J30L+8DXxL7w9bAvfD3sC18P23GIBawAAMB5WXpkBAAAmEcYAQAARhFGAACAUYQRAABglKXDyJIlSyQ+Pl68vb1l3LhxsnHjRtMlWdKiRYtkzJgxusNuRESEzJw5U/bs2WO6LDT461//qjsf33nnnaZLsbQjR47I9ddfL2FhYeLj4yPDhg2TTZs2mS7LkmpqauSPf/yj9OvXT38tEhIS5M9//vMZz19B6ywbRt577z1ZsGCB3pb1/fffS2JiolxyySVy9OhR06VZzurVq2XevHny7bffypdffilVVVVy8cUXS0lJienSLO+7776Tf/7znzJ8+HDTpVja8ePHZcKECeLh4SGfffaZ7Ny5Ux5//HEJCQkxXZolPfroo/Lcc8/JM888I7t27dKPH3vsMXn66adNl+awLLu1V42EqJ/G1V+mxvNv1BkDt99+u9x7772my7O0nJwcPUKiQsrkyZNNl2NZxcXFMnLkSHn22Wfl4YcflhEjRsjixYtNl2VJ6nvSunXrZM2aNaZLgYhcfvnlEhkZKS+//HLTc1deeaUeJXnzzTeN1uaoLDkyUllZKZs3b5apU6e2OP9GPV6/fr3R2iBSUFCgb0NDQ02XYmlqtGr69Okt/p3AjP/85z8yevRo+eUvf6mDelJSkrz44oumy7Ks8847T1auXCl79+7Vj7du3Spr166VadOmmS7NYTnEQXm2lpubq+f8VLJtTj3evXu3sbpQP0Kl1iaoIemhQ4eaLsey3n33XT19qaZpYN6BAwf0tICaWv7DH/6gvy533HGHeHp6yuzZs02XZ8mRKnVi7+DBg8XNzU1/nvzlL3+RX/3qV6ZLc1iWDCOw75/Gt2/frn/KgBnqOPT58+fr9TtqcTfsI6SrkZFHHnlEP1YjI+rfyfPPP08YMeD999+Xt956S95++20555xzJCUlRf8QFR0dzdejkywZRsLDw3Wazc7ObvG8ety7d29jdVndbbfdJh9//LEkJydLTEyM6XIsS01hqoXcar1II/WTn/q6qDVWFRUV+t8Pek5UVJScffbZLZ4bMmSIfPDBB8ZqsrLf//73enTkmmuu0Y/VzqZDhw7pnYGEkc6x5JoRNbQ5atQoPefX/CcP9Xj8+PFGa7MitYZaBZFly5bJ119/rbfLwZwLL7xQtm3bpn/aa7zUT+VqCFrdJ4j0PDVtefJ2d7VeoW/fvsZqsrLS0lK9zrA59e9CfY6gcyw5MqKouVeVYNU32bFjx+pdAmor6Zw5c0yXZsmpGTXcuWLFCt1rJCsrSz8fFBSkV6ejZ6mvwcnrdfz8/HR/C9bxmHHXXXfpRZNqmuaqq67SPZFeeOEFfaHnzZgxQ68RiYuL09M0W7ZskSeeeEJuuukm06U5rjoLe/rpp+vi4uLqPD0968aOHVv37bffmi7JktRfw9Ndr776qunS0OD888+vmz9/vukyLO2jjz6qGzp0aJ2Xl1fd4MGD61544QXTJVlWYWGh/vegPj+8vb3r+vfvX3fffffVVVRUmC7NYVm2zwgAALAPllwzAgAA7AdhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgJj0/wFBNrbyzEL9VQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2863f",
   "metadata": {},
   "source": [
    "# Summary & Key Takeaways\n",
    "\n",
    "## Method Comparison\n",
    "\n",
    "| Aspect | Method 1 (Manual) | Method 2 (Built-in) |\n",
    "|--------|-------------------|---------------------|\n",
    "| **Code Complexity** | More complex (manual loops) | Simpler (automatic) |\n",
    "| **Speed** | Slower | Faster (optimized) |\n",
    "| **Control** | Full control over each step | Less control |\n",
    "| **Input Shape** | `(batch, features)` per step | `(batch, seq_len, features)` |\n",
    "| **Best For** | Learning, custom RNN cells | Production, standard RNNs |\n",
    "\n",
    "---\n",
    "\n",
    "## Important Concepts Covered\n",
    "\n",
    "### 1. **Sequential Processing**\n",
    "RNNs process data one step at a time, maintaining a hidden state that acts as \"memory\"\n",
    "\n",
    "### 2. **Hidden State**\n",
    "- Carries information from previous time steps\n",
    "- Gets updated at each step\n",
    "- Final hidden state contains information about the entire sequence\n",
    "\n",
    "### 3. **One-Hot Encoding**\n",
    "- Converts categorical data (characters) into numerical vectors\n",
    "- Each character gets a unique position in a vector\n",
    "- Essential for neural network input\n",
    "\n",
    "### 4. **Tensor Shapes Matter!**\n",
    "- **Method 1**: `torch.zeros(seq_len, 1, features)` - process one at a time\n",
    "- **Method 2**: `(batch, seq_len, features)` - process entire batch\n",
    "- Always verify tensor shapes match model expectations\n",
    "\n",
    "### 5. **Image as Sequence**\n",
    "- Images can be treated as sequences (rows as time steps)\n",
    "- CIFAR-10: `(3, 32, 32)` → `(32, 96)` where 32 = rows, 96 = pixels×channels\n",
    "- Reshape: `permute(0,2,3,1)` then `reshape(-1, seq_len, features)`\n",
    "\n",
    "---\n",
    "\n",
    "## Common Pitfalls & Solutions\n",
    "\n",
    "### ❌ **Problem**: Shape mismatch errors\n",
    "**Solution**: Always check tensor shapes with `.shape` before passing to model\n",
    "\n",
    "### ❌ **Problem**: Forgetting to loop in Method 1\n",
    "**Solution**: Must manually loop through sequence for manual RNN\n",
    "\n",
    "### ❌ **Problem**: Using all outputs instead of last one (Method 2)\n",
    "**Solution**: Use `out[:, -1, :]` to get last time step\n",
    "\n",
    "### ❌ **Problem**: Forgetting `.to(device)` when using GPU\n",
    "**Solution**: Always move model and data to same device\n",
    "\n",
    "### ❌ **Problem**: Not reshaping test data the same as training data\n",
    "**Solution**: Apply identical preprocessing to both train and test data\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use RNNs?\n",
    "\n",
    "✅ **Good for:**\n",
    "- Text classification\n",
    "- Sequence prediction\n",
    "- Time series analysis\n",
    "- Variable-length inputs\n",
    "\n",
    "❌ **Consider alternatives for:**\n",
    "- Very long sequences (use LSTM/GRU instead)\n",
    "- Parallel processing (use Transformers)\n",
    "- Image classification (CNNs are better)\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Try LSTM/GRU**: Better at capturing long-term dependencies\n",
    "2. **Experiment with bidirectional RNNs**: Process sequences forward and backward\n",
    "3. **Stack multiple RNN layers**: Increase model capacity\n",
    "4. **Try different optimizers**: Adam vs SGD vs RMSprop\n",
    "5. **Tune hyperparameters**: hidden_size, learning_rate, num_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53a3768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
