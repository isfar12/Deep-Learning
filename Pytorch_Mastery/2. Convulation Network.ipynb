{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ce7d6a",
   "metadata": {},
   "source": [
    "# PyTorch Mastery — 2. Convolutional Neural Network (CNN)\n",
    "\n",
    "Welcome! In this tutorial, we build a Convolutional Neural Network (CNN) for image classification on the CIFAR-10 dataset. CNNs excel at computer vision tasks by learning spatial hierarchies through convolution and pooling layers.\n",
    "\n",
    "## Tutorial flow\n",
    "\n",
    "1. Import libraries (PyTorch, torchvision)\n",
    "2. Load and transform CIFAR-10 dataset\n",
    "3. Create DataLoaders for batching\n",
    "4. Visualize sample images\n",
    "5. Understand Conv2D, pooling, and layer composition (with diagrams)\n",
    "6. Define a CNN architecture\n",
    "7. Set up loss and optimizer\n",
    "8. Train the model\n",
    "9. Evaluate test accuracy\n",
    "\n",
    "Skim the code first, then read the explanatory markdown cells that follow each block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874683e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9932ddd4",
   "metadata": {},
   "source": [
    "## Imports explained\n",
    "\n",
    "- `torch`, `torch.nn`, `torch.optim`, `torch.nn.functional`: Core PyTorch for tensors, layers, optimizers, and activation functions.\n",
    "- `torch.utils.data.TensorDataset, DataLoader, Dataset`: Tools for wrapping data and iterating in batches.\n",
    "- `torchvision.datasets`: Pre-packaged datasets (MNIST, CIFAR-10, ImageNet, etc.).\n",
    "- `torchvision.transforms`: Image preprocessing (resize, normalize, augment, convert to tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "407185c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform: Convert image to tensor and normalize (0–1 range)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))   # normalize to mean=0.5, std=0.5\n",
    "])\n",
    "\n",
    "# Download MNIST training & test datasets\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root='./data', train=False, transform=transform, download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7977b",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 dataset with transforms\n",
    "\n",
    "**CIFAR-10**: 60,000 32×32 color images in 10 classes (airplane, car, bird, cat, deer, dog, frog, horse, ship, truck); 50k train, 10k test.\n",
    "\n",
    "**Transforms**:\n",
    "- `transforms.ToTensor()`: Converts PIL image or NumPy array (H×W×C, [0, 255]) to PyTorch tensor (C×H×W, [0.0, 1.0]).\n",
    "- `transforms.Normalize((0.5,), (0.5,))`: Applies `(pixel - mean) / std` per channel. Here mean=0.5, std=0.5 → pixel range becomes [-1, 1].\n",
    "  - Note: CIFAR-10 is RGB (3 channels), so ideally use `(0.5, 0.5, 0.5)` for mean and std. Using `(0.5,)` works but only normalizes the first channel properly; update to 3-tuple for consistency.\n",
    "\n",
    "**datasets.CIFAR10**:\n",
    "- `root='./data'`: directory to store downloaded files.\n",
    "- `train=True/False`: load train or test split.\n",
    "- `transform=transform`: apply transforms on-the-fly when loading each image.\n",
    "- `download=True`: auto-download if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcaeed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d95515",
   "metadata": {},
   "source": [
    "## Create DataLoaders\n",
    "\n",
    "- `DataLoader(dataset, batch_size=64, shuffle=True)`: wraps a dataset and yields batches.\n",
    "  - `batch_size=64`: each iteration returns 64 images and 64 labels.\n",
    "  - `shuffle=True` (train): randomizes order each epoch → better generalization.\n",
    "  - `shuffle=False` (test): consistent evaluation order.\n",
    "- DataLoader automatically handles multi-processing (set `num_workers` for speed on multi-core systems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f48fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# CIFAR10 class names\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Get a sample from the dataset\n",
    "image, label = train_dataset[1]\n",
    "\n",
    "# Denormalize the image (reverse the normalization)\n",
    "image = image / 2 + 0.5\n",
    "\n",
    "# Convert tensor to numpy and transpose from (C, H, W) to (H, W, C)\n",
    "image = image.numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(image)\n",
    "plt.title(f'Label: {classes[label]}')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ed2e6",
   "metadata": {},
   "source": [
    "## Visualize a single sample image\n",
    "\n",
    "- `train_dataset[1]`: returns `(image_tensor, label)`.\n",
    "- `image / 2 + 0.5`: reverses the normalization `(x - 0.5) / 0.5` → brings pixel values back to [0, 1].\n",
    "- `.numpy().transpose(1, 2, 0)`: converts from PyTorch's (C, H, W) to Matplotlib's (H, W, C).\n",
    "- `plt.imshow()`: displays the RGB image.\n",
    "- `classes[label]`: maps integer label to human-readable class name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a409ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the height for the images\n",
    "height = 2  # Adjust the height as needed\n",
    "\n",
    "# Create a figure with 1 row and 2 columns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(4, height))\n",
    "\n",
    "# Display the first image\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title(f'Label: {classes[label]}')\n",
    "axes[0].axis('off')  # Hide axes\n",
    "\n",
    "# Display another sample image from the dataset (for example, the first image)\n",
    "image2, label2 = train_dataset[0]\n",
    "image2 = image2 / 2 + 0.5  # Denormalize\n",
    "image2 = image2.numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Display the second image\n",
    "axes[1].imshow(image2)\n",
    "axes[1].set_title(f'Label: {classes[label2]}')\n",
    "axes[1].axis('off')  # Hide axes\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4ca9b8",
   "metadata": {},
   "source": [
    "## Understanding Convolutional Layers: Visual Guide\n",
    "\n",
    "Before diving into the code, let's understand the key concepts of convolutional layers through visual diagrams. These illustrations show how Conv2D operations work with different parameters:\n",
    "\n",
    "### What you'll learn from these diagrams:\n",
    "\n",
    "1. **Basic 2D Convolution**: How a small kernel (filter) slides across an image to detect features\n",
    "2. **Multi-channel Convolution**: How CNNs process RGB images (or feature maps with multiple channels)\n",
    "3. **Padding**: Why and how we add borders of zeros to control output dimensions\n",
    "4. **Grouped Convolutions**: An efficient variant that splits channels into independent groups\n",
    "5. **Pooling Operations**: Different pooling strategies (max, average, etc.) for downsampling\n",
    "\n",
    "These visual concepts are critical for understanding the `ConvNet` class we'll build next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5888f",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"asset/conv2d.png\" alt=\"2D convolution\" width=\"520\">\n",
    "  <figcaption>2D conv: a 3×3 kernel slides over H×W; stride/padding control output size.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/conv2d_multi_channel.png\" alt=\"Multi-channel conv\" width=\"520\">\n",
    "  <figcaption>Multi‑channel: each filter spans all input channels; outputs sum across channels.</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/padding.png\" alt=\"Padding\" width=\"520\">\n",
    "  <figcaption>Padding: add zeros around borders; e.g., pad=1 with 3×3 keeps spatial size (“same”).</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/group.png\" alt=\"Grouped conv\" width=\"520\">\n",
    "  <figcaption>Groups: split channels into groups; depthwise conv uses groups=in_channels.</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "  <img src=\"asset/image.png\" alt=\"Grouped conv\" width=\"520\">\n",
    "  <figcaption>Different Pooling Layers Applied</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b988aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels= 3,out_channels=16, kernel_size=3, padding=1)  # Input channels=3 for RGB\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 2x2 max pooling\n",
    "        self.conv2=nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128) # calculate flattened size by printing shape after conv and pool layers\n",
    "        self.fc2 = nn.Linear(128, 10)  # 10 output classes for CIFAR-10\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.pool(self.relu(self.conv1(x)))\n",
    "        x=self.pool(self.relu(self.conv2(x)))\n",
    "        x=x.view(-1, 32*8*8)\n",
    "        x=self.relu(self.fc1(x))\n",
    "        x=self.relu(self.fc2(x))\n",
    "        \n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0340841",
   "metadata": {},
   "source": [
    "## Define the CNN architecture (`ConvNet`)\n",
    "\n",
    "### Layers breakdown\n",
    "\n",
    "**Convolutional layers**:\n",
    "- `nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)`:\n",
    "  - `in_channels=3`: RGB input (3 color channels).\n",
    "  - `out_channels=16`: produces 16 feature maps (each filter learns different patterns like edges, textures).\n",
    "  - `kernel_size=3`: 3×3 filter slides over the image.\n",
    "  - `padding=1`: adds 1 pixel of zeros around borders → keeps spatial size same when stride=1.\n",
    "  - Output size formula: `(H + 2×padding - kernel_size) / stride + 1`.  \n",
    "    With padding=1, kernel=3, stride=1: (32 + 2 - 3)/1 + 1 = 32 → output is 32×32×16.\n",
    "\n",
    "- `nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)`: second conv increases depth to 32 feature maps.\n",
    "\n",
    "**Pooling layer**:\n",
    "- `nn.MaxPool2d(2, 2)`: 2×2 max pooling with stride=2.\n",
    "  - Takes max value in each 2×2 window → reduces spatial size by half.\n",
    "  - After first pool: 32×32 → 16×16; after second pool: 16×16 → 8×8.\n",
    "  - Pooling provides translation invariance and reduces parameters.\n",
    "\n",
    "**Fully connected (linear) layers**:\n",
    "- `nn.Linear(32 * 8 * 8, 128)`: flattens 32×8×8 = 2048 features into a vector, then maps to 128 hidden units.\n",
    "- `nn.Linear(128, 10)`: maps 128 → 10 output logits (one per CIFAR-10 class).\n",
    "\n",
    "**Activation**:\n",
    "- `nn.ReLU()`: applies `max(0, x)` for non-linearity; enables learning complex patterns.\n",
    "\n",
    "### Forward pass flow\n",
    "\n",
    "1. **Input**: (batch_size, 3, 32, 32) — RGB images\n",
    "2. **conv1 → ReLU → pool**: (N, 3, 32, 32) → (N, 16, 32, 32) → (N, 16, 16, 16)\n",
    "3. **conv2 → ReLU → pool**: (N, 16, 16, 16) → (N, 32, 16, 16) → (N, 32, 8, 8)\n",
    "4. **Flatten**: (N, 32, 8, 8) → (N, 2048) using `.view(-1, 32*8*8)`\n",
    "5. **fc1 → ReLU**: (N, 2048) → (N, 128)\n",
    "6. **fc2** (no activation): (N, 128) → (N, 10) logits for CrossEntropyLoss\n",
    "\n",
    "**Important note**: The last `ReLU` on fc2 in the code (`x=self.relu(self.fc2(x))`) is **incorrect**. Remove it—CrossEntropyLoss expects raw logits. The correct last line should be:\n",
    "```python\n",
    "x = self.fc2(x)  # no ReLU here\n",
    "return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a164394",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"asset/understand_conv2linear.png\" alt=\"Grouped conv\" width=\"520\">\n",
    "  <figcaption>This is a simple method. In the forward pass, only code till pool layer. Then send random model and see what is the final output. This output shape we will just copy paste into the ``fc layer `` so that the conversion occurs without error</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa5ff5",
   "metadata": {},
   "source": [
    "\n",
    "### 1. **Match Kernel, Stride, and Padding to Image Scale**\n",
    "\n",
    "* **Small images (e.g., 32×32 like CIFAR-10):**\n",
    "\n",
    "  * Use **smaller kernels (3×3)** and **padding=1** to preserve spatial dimensions longer.\n",
    "  * Use **stride=1** in early layers to capture fine details.\n",
    "  * Too many pooling/stride layers will shrink your feature maps too fast.\n",
    "* **Large images (e.g., 224×224 like ImageNet):**\n",
    "\n",
    "  * You can start with **larger kernels (7×7 or 5×5)** and a stride of 2 to downsample faster.\n",
    "\n",
    "**Rule of thumb:**\n",
    "Preserve more spatial resolution early → go deeper → downsample later.\n",
    "\n",
    "\n",
    "### 2. **Gradually Increase Filters**\n",
    "\n",
    "* Early layers detect edges and textures → fewer filters (e.g., 16 or 32).\n",
    "* Deeper layers detect complex shapes → more filters (e.g., 64, 128, 256…).\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Conv2d(3, 32, 3) → Conv2d(32, 64, 3) → Conv2d(64, 128, 3)\n",
    "```\n",
    "\n",
    "This gives the model hierarchical feature richness.\n",
    "\n",
    "\n",
    "### 3. **Use Pooling Sparingly**\n",
    "\n",
    "* Pooling (e.g., MaxPool2d(2,2)) reduces size but also loses information.\n",
    "* Too much pooling on small images will destroy detail.\n",
    "* Instead of multiple pooling layers, sometimes you can use **stride=2** in later convs to learn downsampling.\n",
    "\n",
    "Example:\n",
    "For CIFAR-10 (32×32):\n",
    "\n",
    "* 2 pool layers → output ≈ 8×8 → perfect for flattening.\n",
    "\n",
    "\n",
    "### 4. **Compute Flattened Size Properly**\n",
    "\n",
    "* Always check output shape after convolutions and pooling before `fc1`.\n",
    "\n",
    "```python\n",
    "print(x.shape)\n",
    "```\n",
    "\n",
    "If you miscalculate, you’ll get shape errors like “mat1 and mat2 shapes cannot be multiplied.”\n",
    "\n",
    "For CIFAR-10:\n",
    "After two (2×2) poolings on 32×32 → (32 / 4) = 8 → `32 * 8 * 8`.\n",
    "\n",
    "\n",
    "\n",
    "### 5. **Add Normalization and Dropout**\n",
    "\n",
    "* Add **BatchNorm2d** after each convolution → stabilizes training.\n",
    "* Add **Dropout** after ReLU or between dense layers → prevents overfitting.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "self.bn1 = nn.BatchNorm2d(16)\n",
    "self.dropout = nn.Dropout(0.5)\n",
    "```\n",
    "\n",
    "\n",
    "### 6. **Activation Choices**\n",
    "\n",
    "* **ReLU** is standard for speed and simplicity.\n",
    "* Try **LeakyReLU** or **GELU** for deeper models or small datasets to avoid dead neurons.\n",
    "\n",
    "\n",
    "### 7. **Keep Output Layer Logical**\n",
    "\n",
    "* Number of neurons in final `fc2` = number of classes (e.g., 10 for CIFAR-10).\n",
    "* Don’t apply ReLU after final output if you’re using `CrossEntropyLoss` (it already includes `Softmax` internally).\n",
    "\n",
    "### 8. **Monitor Overfitting**\n",
    "\n",
    "* If validation accuracy stagnates while training accuracy increases → too many parameters.\n",
    "* Reduce `fc` layer size, add dropout, or use data augmentation.\n",
    "\n",
    "\n",
    "### Summary Rule Set\n",
    "\n",
    "| Image Size         | Kernel        | Filters Progression | Pooling             | Tips                       |\n",
    "| ------------------ | ------------- | ------------------- | ------------------- | -------------------------- |\n",
    "| 32×32 (CIFAR-10)   | 3×3           | 16→32→64            | MaxPool(2,2) twice  | Avoid too deep nets        |\n",
    "| 64×64              | 3×3 or 5×5    | 32→64→128           | MaxPool(2,2) thrice | Use dropout                |\n",
    "| 224×224 (ImageNet) | 7×7, then 3×3 | 64→128→256→512      | Stride & Pool       | Use BatchNorm, deeper nets |\n",
    "\n",
    "\n",
    "\n",
    "###  **How filters make an impact**\n",
    "\n",
    "Each **filter** (or kernel) in a convolutional layer looks for a specific **pattern** in the image.\n",
    "\n",
    "* The first filters usually detect **basic shapes** — like edges or corners.\n",
    "* As you go deeper, filters combine those patterns into **bigger features** — like eyes, wheels, or textures.\n",
    "* The more filters you have, the **more types of features** the model can learn.\n",
    "\n",
    "But there’s a tradeoff:\n",
    "\n",
    "* More filters → more patterns learned, but also more **computation and risk of overfitting** (especially with small datasets).\n",
    "  So you increase filters gradually across layers, not all at once.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Conv1: 16 filters  → learns simple edges  \n",
    "Conv2: 32 filters → learns small shapes  \n",
    "Conv3: 64 filters → learns complex structures\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Using pooling layers carefully**\n",
    "\n",
    "Pooling layers (like `MaxPool2d`) reduce image size. This helps the model learn **faster** and use **less memory**, but it also removes some details.\n",
    "\n",
    "If you use **too many pooling layers** (or use them too early), the image becomes too small too quickly, and you lose useful information — for example, small features that matter for classification.\n",
    "\n",
    "So “using pooling carefully” means:\n",
    "\n",
    "* Don’t add a pooling layer after every convolution unless needed.\n",
    "* On **small images (like 32×32)**, 2 pooling layers are usually enough.\n",
    "* On **large images**, you can use more pooling or stride to reduce size faster.\n",
    "\n",
    "Example:\n",
    "CIFAR-10 (32×32)\n",
    "\n",
    "```\n",
    "Conv → Pool → Conv → Pool → Flatten → FC\n",
    "```\n",
    "\n",
    "That gives you 8×8 feature maps — enough detail left for the dense layers.\n",
    "\n",
    "\n",
    "In short:\n",
    "\n",
    "* Filters decide **what patterns** the network learns.\n",
    "* Pooling decides **how much detail** the network keeps.\n",
    "  You want enough filters to learn variety, and enough resolution (not over-pooled) to keep useful information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4630a664",
   "metadata": {},
   "source": [
    "\n",
    "### **Every 2–3 convolution layers → one pooling layer**\n",
    "\n",
    "That’s the most common spacing.\n",
    "It lets the network extract richer features before reducing size.\n",
    "\n",
    "Example pattern:\n",
    "\n",
    "```\n",
    "Conv → ReLU  \n",
    "Conv → ReLU  \n",
    "Pool  \n",
    "Conv → ReLU  \n",
    "Conv → ReLU  \n",
    "Pool\n",
    "```\n",
    "\n",
    "Here, two conv layers learn details at the same resolution before pooling shrinks the image.\n",
    "\n",
    "\n",
    "\n",
    "### **Why not after every conv?**\n",
    "\n",
    "If you pool too soon, you’ll lose important details before the model learns to recognize them.\n",
    "If your image is small (like 32×32), just two poolings can already reduce it to 8×8 — that’s enough.\n",
    "\n",
    "\n",
    "\n",
    "### **Rule of thumb by image size:**\n",
    "\n",
    "| Image size         | Good pooling gap      | Example                           |\n",
    "| ------------------ | --------------------- | --------------------------------- |\n",
    "| 32×32 (CIFAR-10)   | Every 2 conv layers   | 2 pooling layers total            |\n",
    "| 64×64              | Every 2 conv layers   | 2–3 pooling layers                |\n",
    "| 224×224 (ImageNet) | Every 2–3 conv layers | 4–5 pooling or stride downsamples |\n",
    "\n",
    "\n",
    "\n",
    "### **Alternative to frequent pooling**\n",
    "\n",
    "Instead of pooling too often, you can:\n",
    "\n",
    "* Use **stride=2** in some conv layers to downsample gradually.\n",
    "* Use **global average pooling** before the final dense layer instead of flattening a large tensor.\n",
    "\n",
    "\n",
    "\n",
    "**In short:**\n",
    "Add a pooling layer **after every 2–3 conv layers**, depending on image size.\n",
    "You want to **extract features first**, then **reduce size** — not the other way around.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a1ff3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=2048, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=ConvNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02211d86",
   "metadata": {},
   "source": [
    "## Instantiate the model\n",
    "\n",
    "- `model = ConvNet()`: creates an instance of the CNN.\n",
    "- `print(model)`: displays the layer hierarchy and parameter counts (useful for debugging architecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ecfc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682d3e7",
   "metadata": {},
   "source": [
    "## Loss function and optimizer\n",
    "\n",
    "- `nn.CrossEntropyLoss()`:\n",
    "  - Combines `LogSoftmax` + `NLLLoss` for multi-class classification.\n",
    "  - Expects:\n",
    "    - **Input (logits)**: shape (N, num_classes), dtype float32.\n",
    "    - **Target**: shape (N,), dtype int64, values in [0, num_classes-1].\n",
    "  - Automatically applies softmax internally; don't apply it in your model's forward.\n",
    "  \n",
    "- `optim.Adam(model.parameters(), lr=0.001)`:\n",
    "  - Adam optimizer: adaptive learning rates per parameter (momentum + RMSProp).\n",
    "  - `lr=0.001`: learning rate; controls step size in weight updates.\n",
    "  \n",
    "- `epochs = 10`: number of full passes through the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b962f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.9503\n",
      "Epoch [2/10], Loss: 1.7221\n",
      "Epoch [3/10], Loss: 1.6037\n",
      "Epoch [4/10], Loss: 1.9055\n",
      "Epoch [5/10], Loss: 1.6187\n",
      "Epoch [6/10], Loss: 0.6906\n",
      "Epoch [7/10], Loss: 0.9529\n",
      "Epoch [8/10], Loss: 0.9014\n",
      "Epoch [9/10], Loss: 0.8625\n",
      "Epoch [10/10], Loss: 0.4569\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b6878b",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "**Epoch loop**: runs 10 times over the entire training dataset.\n",
    "\n",
    "**Batch loop**: iterates through mini-batches of 64 images.\n",
    "\n",
    "**Training steps per batch**:\n",
    "1. `optimizer.zero_grad()`: clears old gradients from previous iteration (PyTorch accumulates gradients by default).\n",
    "2. `outputs = model(inputs)`: forward pass → computes logits (N, 10).\n",
    "3. `loss = criterion(outputs, labels)`: computes cross-entropy loss.\n",
    "4. `loss.backward()`: backpropagation → computes gradients of loss w.r.t. all parameters.\n",
    "5. `optimizer.step()`: updates weights using gradients: `weight = weight - lr * gradient`.\n",
    "\n",
    "**Print**: shows loss for the last batch of each epoch (better: track average loss per epoch by summing and dividing by `len(train_loader)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636fd176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 63.65%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct=0\n",
    "total=0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff9fc9",
   "metadata": {},
   "source": [
    "## Evaluate on the test set\n",
    "\n",
    "**Steps**:\n",
    "1. `model.eval()`: sets model to evaluation mode (disables dropout, fixes batch norm stats if present).\n",
    "2. `torch.no_grad()`: disables gradient computation → saves memory and speeds up inference.\n",
    "3. For each batch:\n",
    "   - `outputs = model(X_batch)`: forward pass → (N, 10) logits.\n",
    "   - `torch.max(outputs, 1)`: returns (max_values, indices) along dim=1 → `predicted` holds class indices with highest logit.\n",
    "   - `total += y_batch.size(0)`: count samples.\n",
    "   - `correct += (predicted == y_batch).sum().item()`: count correct predictions.\n",
    "4. **Accuracy** = (correct / total) × 100.\n",
    "\n",
    "**Note**: This is top-1 accuracy. For deeper analysis, compute confusion matrix, per-class precision/recall, or top-5 accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a581d5",
   "metadata": {},
   "source": [
    "## Summary and next steps\n",
    "\n",
    "You've built a simple CNN for CIFAR-10 image classification! Key concepts covered:\n",
    "\n",
    "- **Conv2d**: learns spatial features via small kernels sliding over images.\n",
    "- **MaxPool2d**: downsamples spatial dimensions, reduces parameters, adds translation invariance.\n",
    "- **Flatten**: converts 4D (N, C, H, W) feature maps to 2D (N, features) for fully connected layers.\n",
    "- **CrossEntropyLoss**: combines softmax + negative log-likelihood for multi-class problems.\n",
    "- **Training loop**: zero_grad → forward → loss → backward → step.\n",
    "- **Evaluation**: model.eval() + no_grad() for inference.\n",
    "\n",
    "### Improvements to try\n",
    "\n",
    "1. **Fix the last ReLU**: Remove `self.relu(self.fc2(x))` → use `self.fc2(x)` directly. CrossEntropyLoss expects logits, not activated outputs.\n",
    "\n",
    "2. **Better normalization**: Update transform to 3-channel mean/std:\n",
    "   ```python\n",
    "   transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "   # CIFAR-10 dataset-specific statistics\n",
    "   ```\n",
    "\n",
    "3. **Track average loss per epoch**: Instead of printing the last batch loss, sum all losses and divide by `len(train_loader)`.\n",
    "\n",
    "4. **Add more conv layers**: Stack 3–4 conv blocks with increasing channels (16→32→64→128) for deeper feature extraction.\n",
    "\n",
    "5. **Data augmentation**: Add `transforms.RandomHorizontalFlip()`, `transforms.RandomCrop(32, padding=4)` to reduce overfitting.\n",
    "\n",
    "6. **Batch normalization**: Insert `nn.BatchNorm2d(channels)` after each conv to stabilize training.\n",
    "\n",
    "7. **Dropout**: Add `nn.Dropout(0.5)` before fc layers to prevent overfitting.\n",
    "\n",
    "8. **Learning rate scheduler**: Use `torch.optim.lr_scheduler.StepLR` to decay learning rate over epochs.\n",
    "\n",
    "9. **Save/load model**: Use `torch.save(model.state_dict(), 'cifar_cnn.pth')` to save trained weights.\n",
    "\n",
    "10. **Visualize feature maps**: Extract intermediate conv outputs and plot them to see what patterns the network learned.\n",
    "\n",
    "### Architecture evolution\n",
    "\n",
    "- **LeNet** (1998): pioneered CNNs for digit recognition.\n",
    "- **AlexNet** (2012): deeper, ReLU, dropout → ImageNet breakthrough.\n",
    "- **VGG** (2014): uniform 3×3 convs, very deep (16–19 layers).\n",
    "- **ResNet** (2015): skip connections enable 100+ layers.\n",
    "- **EfficientNet** (2019): compound scaling (width × depth × resolution).\n",
    "\n",
    "Try implementing VGG-like blocks or ResNet skip connections next!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
