{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ce38ca",
   "metadata": {},
   "source": [
    "# Custom Dataset Building in PyTorch\n",
    "\n",
    "## What You'll Learn\n",
    "This notebook teaches you how to:\n",
    "1. **Build Custom Datasets** for images with labels (Cats & Dogs)\n",
    "2. **Handle Text Datasets** for image captioning (Flickr8k)\n",
    "3. **Create DataLoaders** for efficient batch processing\n",
    "4. **Build Vocabularies** for text processing\n",
    "\n",
    "---\n",
    "\n",
    "## Why Do We Need Custom Datasets?\n",
    "\n",
    "**The Restaurant Analogy:**\n",
    "- **PyTorch's Built-in Datasets** = Fast food menu (limited options: MNIST, CIFAR10)\n",
    "- **Custom Datasets** = Your own restaurant menu (any dish you want!)\n",
    "\n",
    "**Real Problem:**\n",
    "Most real-world projects need custom data (medical images, company photos, specific text), not just standard datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Two Examples Covered\n",
    "\n",
    "### Example 1: Cats & Dogs Classification\n",
    "**Goal:** Given an image ‚Üí predict \"cat\" or \"dog\"\n",
    "\n",
    "**Files needed:**\n",
    "- `cats_dogs/` folder with images (`cat_001.jpg`, `dog_001.jpg`, etc.)\n",
    "- `cats_dogs.csv` file with labels (filename, class)\n",
    "\n",
    "### Example 2: Image Captioning\n",
    "**Goal:** Given an image ‚Üí generate a text description\n",
    "\n",
    "**Files needed:**\n",
    "- `flickr8k_images/` folder with photos\n",
    "- `captions.txt` file with image-caption pairs\n",
    "\n",
    "Let's build both!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be21572",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Top panel ‚Äî dataset assets\n",
    "\n",
    "* A folder named **cats_dogs_resized** ‚Üí this is the directory that holds the prepared (resized) images.\n",
    "* A file named **cats_dogs** (Excel workbook) ‚Üí a spreadsheet used for labels/metadata (e.g., filename ‚Üí class). The size shows ~394 KB.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/cat_dog_directory.png\" alt=\"File Directory containing cat dog image and csv file with their identity\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "2. Middle panel ‚Äî the images themselves\n",
    "\n",
    "* Inside **cats_dogs_resized**, Windows Explorer displays a grid of **dog photos** (filenames like `dog_3286.jpg`, `dog_3297.jpg`, ‚Ä¶ `dog_3312.jpg`).\n",
    "* These are varied pictures (different breeds/poses/backgrounds) but all **uniformly resized**, suitable for a ML dataset.\n",
    "<figure>\n",
    "  <img src=\"asset/cat_dog_image_directory.png\" alt=\"inside of folder\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "3. Bottom panel ‚Äî the label spreadsheet\n",
    "\n",
    "* An Excel sheet with **filenames in Column A** (e.g., `dog_2511.jpg`, `dog_2512.jpg`, ‚Ä¶).\n",
    "* **Column B contains numeric class labels** (shown as `1` for these rows). Given the filenames are ‚Äúdog_‚Ä¶‚Äù, this implies a mapping such as **1 = dog** (and likely **0 = cat** elsewhere in the sheet).\n",
    "* Columns C/D are empty in the visible portion (reserved for other info if needed).\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/cat_dog_csv_structure.png\" alt=\"inside of the csv\" width=\"400\">\n",
    "</figure>\n",
    "\n",
    "In short: the image depicts a typical classification dataset setup‚Äî(1) a folder of resized images, (2) a preview of those images, and (3) a spreadsheet mapping each filename to a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0256282",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üê± Part 1: Cats & Dogs Classification Dataset\n",
    "\n",
    "## üìÇ Understanding the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f55f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2eb4e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Import Required Libraries\n",
    "\n",
    "```python\n",
    "import torch                        # Main PyTorch library\n",
    "from torch.utils.data import DataLoader, Dataset  # Data handling tools\n",
    "from skimage import io             # Image reading\n",
    "import pandas as pd                # CSV file handling\n",
    "import os                          # File path operations\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2b6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d6f12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Set Device (CPU or GPU)\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    "**Purpose:** Automatically use GPU if available (10-20x faster than CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f28bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatsandDogsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations=pd.read_csv(csv_file)\n",
    "        self.root_dir=root_dir\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path=os.path.join(self.root_dir, self.annotations.iloc[index,0]) #ith row, 0th column\n",
    "        image=io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index,1])) #ith row, 1st column\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22faef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Build the Custom Dataset Class\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Every PyTorch Dataset must implement:\n",
    "1. **`__init__`**: Setup (read CSV, store paths)\n",
    "2. **`__len__`**: Return total number of items\n",
    "3. **`__getitem__`**: Get one item (image + label) by index\n",
    "\n",
    "### How It Works\n",
    "\n",
    "```python\n",
    "class CatsandDogsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file: Path to labels CSV (filename, class)\n",
    "            root_dir: Folder containing images\n",
    "            transform: Image transformations\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)  # Load labels\n",
    "        self.root_dir = root_dir                  # Image folder path\n",
    "        self.transform = transform                # Transforms to apply\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of samples\"\"\"\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Get one sample (image, label)\"\"\"\n",
    "        # 1. Build image path\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        \n",
    "        # 2. Load image\n",
    "        image = io.imread(img_path)\n",
    "        \n",
    "        # 3. Get label\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "        \n",
    "        # 4. Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, y_label\n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "**CSV file (`cats_dogs.csv`):**\n",
    "```\n",
    "filename,class\n",
    "cat_001.jpg,0\n",
    "dog_001.jpg,1\n",
    "cat_002.jpg,0\n",
    "```\n",
    "\n",
    "**Usage:**\n",
    "```python\n",
    "dataset = CatsandDogsDataset('cats_dogs.csv', 'cats_dogs/')\n",
    "image, label = dataset[0]  # Get first item\n",
    "# image: numpy array or tensor\n",
    "# label: tensor(0) for cat, tensor(1) for dog\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset=CatsandDogsDataset(csv_file='cats_dogs.csv', root_dir='cats_dogs',transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08d196",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Apply Transforms\n",
    "\n",
    "### Why Transforms?\n",
    "\n",
    "1. **Standardize input size**: Neural networks need fixed dimensions\n",
    "2. **Normalize values**: Convert pixel values from [0, 255] ‚Üí [0.0, 1.0]\n",
    "3. **Data augmentation**: Random crops, flips increase training data variety\n",
    "\n",
    "### Transform Pipeline\n",
    "\n",
    "```python\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset = CatsandDogsDataset(\n",
    "    csv_file='cats_dogs.csv',\n",
    "    root_dir='cats_dogs',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToPILImage(),      # numpy ‚Üí PIL Image\n",
    "        transforms.Resize((128, 128)), # Resize to 128√ó128\n",
    "        transforms.ToTensor()          # PIL ‚Üí tensor, [0-255] ‚Üí [0.0-1.0]\n",
    "    ])\n",
    ")\n",
    "```\n",
    "\n",
    "### Transformation Flow\n",
    "\n",
    "**Input:** `numpy array (H, W, 3)` with values `[0-255]`\n",
    "\n",
    "‚Üì `ToPILImage()`\n",
    "\n",
    "**PIL Image** (required for Resize)\n",
    "\n",
    "‚Üì `Resize((128, 128))`\n",
    "\n",
    "**PIL Image (128, 128)**\n",
    "\n",
    "‚Üì `ToTensor()`\n",
    "\n",
    "**Output:** `torch.Tensor (3, 128, 128)` with values `[0.0, 1.0]`\n",
    "\n",
    "**Note:** `ToTensor()` automatically:\n",
    "- Changes shape from `(H, W, C)` ‚Üí `(C, H, W)`\n",
    "- Normalizes values: `pixel / 255`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(dataset, [800, 200])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d6150",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Split into Train/Test Sets\n",
    "\n",
    "### Purpose\n",
    "\n",
    "- **Training set**: Model learns from this (80%)\n",
    "- **Test set**: Evaluate performance on unseen data (20%)\n",
    "\n",
    "### Implementation\n",
    "\n",
    "```python\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [800, 200])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "### Why Batching?\n",
    "\n",
    "**Without batching (batch_size=1):**\n",
    "- Process 1 image ‚Üí update weights ‚Üí repeat 800 times\n",
    "- Very slow!\n",
    "\n",
    "**With batching (batch_size=32):**\n",
    "- Process 32 images ‚Üí update weights ‚Üí repeat 25 times\n",
    "- Much faster!\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **`batch_size=32`**: Number of samples per batch\n",
    "- **`shuffle=True`**: Randomize order each epoch (prevents memorization)\n",
    "\n",
    "---\n",
    "\n",
    "# Part 2: Image Captioning Dataset\n",
    "\n",
    "## What's Different?\n",
    "\n",
    "**Cats & Dogs:**\n",
    "- Input: Image ‚Üí Output: Single number (0 or 1)\n",
    "\n",
    "**Image Captioning:**\n",
    "- Input: Image ‚Üí Output: **Sentence** (\"A dog playing in the park\")\n",
    "\n",
    "**New Challenges:**\n",
    "1. Convert words to numbers (vocabulary)\n",
    "2. Handle variable-length captions\n",
    "3. Pad sequences to same length for batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6969e62",
   "metadata": {},
   "source": [
    "This image shows the structure of the Flickr8k Image Captioning dataset, where each photo (in /images) is linked to multiple natural-language descriptions (in /captions). The task is to build a model that can look at a new image and describe it in words.\n",
    "\n",
    "\n",
    "###  Folder structure\n",
    "<figure>\n",
    "  <img src=\"asset/image_caption_directory.png\" alt=\"File Directory containing  image and text file with their caption\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "* **`images`** ‚Üí A folder containing all the image files.\n",
    "* **`captions`** ‚Üí A text file (`.txt`) containing all the image-caption pairs.\n",
    "  This setup is part of the **Flickr8k dataset**, which is widely used for *image captioning* tasks in deep learning.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Folder content preview\n",
    "<figure>\n",
    "  <img src=\"asset/image_caption_image_directory.png\" alt=\"inside of folder\" width=\"800\">\n",
    "</figure>\n",
    "* The path is `Desktop > customdata > flickr8k > images`.\n",
    "* It shows thumbnails of several **JPEG images**, each named with an ID like `667626_18933d13e`, `10815824_2997e03d76`, etc.\n",
    "* The images show various human and animal activities (children playing, dogs, people near water, etc.).\n",
    "  These are the **input images** used for training and evaluation in the captioning model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Captions file opened in Notepad\n",
    "<figure>\n",
    "  <img src=\"asset/image_caption_caption_file.png\" alt=\"inside of the txt\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "* The text file has **two columns**:\n",
    "\n",
    "  * **Column 1:** Image filename (e.g., `1000268201_693b08cb0e.jpg`)\n",
    "  * **Column 2:** The **caption** (natural language description).\n",
    "* Example lines:\n",
    "\n",
    "  * `1000268201_693b08cb0e.jpg, A child in a pink dress is climbing up a set of stairs in an entry way.`\n",
    "  * `1000268201_693b08cb0e.jpg, A girl going into a wooden building.`\n",
    "  * `1000268201_693b08cb0e.jpg, A little girl climbing into a wooden playhouse.`\n",
    "* Note that the same image appears multiple times, each with a **different caption** ‚Üí this is a common feature in Flickr8k: *five human-written captions per image*.\n",
    "\n",
    "\n",
    "**In short:**\n",
    "This image shows the structure of the **Flickr8k Image Captioning dataset**, where each photo (in `/images`) is linked to multiple natural-language descriptions (in `/captions`). The task is to build a model that can look at a new image and describe it in words.\n",
    "\n",
    "\n",
    "| Component       | Description                               | Purpose                              |\n",
    "| --------------- | ----------------------------------------- | ------------------------------------ |\n",
    "| `images` folder | Set of photos (8,000 total)               | Visual input                         |\n",
    "| `captions.txt`  | Image filename + 5 human captions         | Ground-truth text labels             |\n",
    "| Combined usage  | Each image paired with multiple sentences | For training Image Captioning models |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b04176",
   "metadata": {},
   "source": [
    "```python\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [800, 200])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "### üìä What Happens Here\n",
    "\n",
    "**1. `random_split(dataset, [800, 200])`**\n",
    "- Randomly splits 1000 images into 800 and 200\n",
    "- Returns two subset objects\n",
    "\n",
    "**2. `DataLoader` for Training:**\n",
    "- `batch_size=32`: Delivers 32 images at a time (not one by one)\n",
    "- `shuffle=True`: Randomizes order each epoch (prevents memorization)\n",
    "\n",
    "**3. `DataLoader` for Testing:**\n",
    "- Same batch size\n",
    "- Shuffle to avoid bias\n",
    "\n",
    "### üéØ Why Batching?\n",
    "\n",
    "**Without batching (batch_size=1):**\n",
    "```python\n",
    "for image, label in train_loader:  # One image at a time\n",
    "    # image.shape = (1, 3, 128, 128)\n",
    "    # Train on 1 image ‚Üí update weights ‚Üí repeat 800 times\n",
    "    # Super slow! üê¢\n",
    "```\n",
    "\n",
    "**With batching (batch_size=32):**\n",
    "```python\n",
    "for images, labels in train_loader:  # 32 images at once\n",
    "    # images.shape = (32, 3, 128, 128)\n",
    "    # Train on 32 images ‚Üí update weights ‚Üí repeat 25 times\n",
    "    # Much faster! ‚ö°\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üì∑ Part 2: Image Captioning Dataset\n",
    "\n",
    "## ü§î What's Different Here?\n",
    "\n",
    "**Cats & Dogs:**\n",
    "- Input: Image ‚Üí Output: Single number (0 or 1)\n",
    "\n",
    "**Image Captioning:**\n",
    "- Input: Image ‚Üí Output: **Sentence** (\"A dog playing in the park\")\n",
    "\n",
    "**New Challenge:** How do we handle text?\n",
    "- Need to convert words to numbers (vocabulary)\n",
    "- Need to handle variable-length captions\n",
    "- Need to pad sequences to same length\n",
    "\n",
    "Let's build it step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6969e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} #iots: index to string\n",
    "        self.stoi = {v: k for k, v in self.itos.items()} #stoi: string to index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    #        \"I love dogs\" -> ['i', 'love', 'dogs']\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] +=1\n",
    "                \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx +=1\n",
    "                    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e17dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81169e12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 6: Build a Vocabulary Class\n",
    "\n",
    "### Why We Need This\n",
    "\n",
    "Neural networks can't process text directly. They need numbers.\n",
    "\n",
    "**Problem:** Convert \"A dog running\" ‚Üí `[4, 5, 6]`\n",
    "**Solution:** Build a vocabulary that maps each word to a unique number.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "**Special Tokens:**\n",
    "- `<PAD>` = 0: Padding for variable-length sequences\n",
    "- `<SOS>` = 1: Start of sequence\n",
    "- `<EOS>` = 2: End of sequence\n",
    "- `<UNK>` = 3: Unknown words (not in vocabulary)\n",
    "\n",
    "**Frequency Threshold:**\n",
    "- Only include words appearing ‚â• `freq_threshold` times\n",
    "- Rare words ‚Üí `<UNK>` (prevents vocabulary explosion)\n",
    "\n",
    "### Example\n",
    "\n",
    "**Input sentences (freq_threshold=3):**\n",
    "```python\n",
    "sentences = [\n",
    "    \"A dog playing in park\",\n",
    "    \"A cat sleeping on sofa\",\n",
    "    \"A dog running in park\",\n",
    "    \"A bird flying in sky\",\n",
    "    \"A dog jumping in park\"\n",
    "]\n",
    "```\n",
    "\n",
    "**Word frequencies:**\n",
    "```python\n",
    "{'a': 5, 'dog': 3, 'in': 4, 'park': 3, 'playing': 1, 'cat': 1, ...}\n",
    "```\n",
    "\n",
    "**Vocabulary (only words with freq ‚â• 3):**\n",
    "```python\n",
    "vocab.stoi = {\n",
    "    '<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3,\n",
    "    'a': 4, 'in': 5, 'dog': 6, 'park': 7\n",
    "}\n",
    "```\n",
    "\n",
    "**Numericalize:**\n",
    "```python\n",
    "vocab.numericalize(\"A dog playing in park\")\n",
    "# Output: [4, 6, 3, 5, 7]\n",
    "#         'a' 'dog' '<UNK>' 'in' 'park'\n",
    "# Note: 'playing' ‚Üí '<UNK>' (frequency too low)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfbf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickerDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir=root_dir\n",
    "        self.df=pd.read_csv(captions_file)\n",
    "        self.transform=transform\n",
    "        \n",
    "        self.imgs=self.df['image']\n",
    "        self.captions=self.df['caption']\n",
    "        \n",
    "        self.vocab=Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption=self.captions[index]\n",
    "        img_id=self.imgs[index]\n",
    "        img_path=os.path.join(self.root_dir, img_id)\n",
    "        image=Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image=self.transform(image)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return image, torch.tensor(numericalized_caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8005c96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Import Additional Libraries\n",
    "\n",
    "```python\n",
    "import spacy                        # Text tokenization\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence  # Padding sequences\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image              # Image handling\n",
    "```\n",
    "\n",
    "**New additions:**\n",
    "- **`spacy`**: Natural language processing (tokenization)\n",
    "- **`pad_sequence`**: Makes all captions same length for batching\n",
    "- **`PIL`**: Better image handling than skimage\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Build Flickr Dataset Class\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "**Files needed:**\n",
    "- `flickr8k_images/`: Folder with 8,000 images\n",
    "- `captions.txt`: CSV with columns `[image, caption]`\n",
    "\n",
    "**Each image has 5 different captions** (written by different people).\n",
    "\n",
    "### What `__getitem__` Returns\n",
    "\n",
    "```python\n",
    "image, caption = dataset[0]\n",
    "\n",
    "# image: Transformed tensor (3, 224, 224)\n",
    "# caption: Numericalized caption with special tokens\n",
    "#          [<SOS>, word1, word2, ..., <EOS>]\n",
    "#          e.g., [1, 4, 45, 6, 89, 2]\n",
    "```\n",
    "\n",
    "### Example Flow\n",
    "\n",
    "**CSV file:**\n",
    "```\n",
    "image,caption\n",
    "dog_001.jpg,A brown dog running in the park\n",
    "cat_002.jpg,A white cat sleeping on a sofa\n",
    "```\n",
    "\n",
    "**Getting item 0:**\n",
    "1. `caption = \"A brown dog running in the park\"`\n",
    "2. `img_path = \"flickr8k_images/dog_001.jpg\"`\n",
    "3. Load image ‚Üí apply transforms ‚Üí tensor `(3, 224, 224)`\n",
    "4. Numericalize caption:\n",
    "   ```python\n",
    "   [1] + [4, 45, 6, 89, 5, 12, 7] + [2]\n",
    "   # <SOS> + words + <EOS>\n",
    "   ```\n",
    "5. Return `(image_tensor, caption_tensor)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02788bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx=pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        images = [item[0].unsqueeze(0) for item in batch]\n",
    "        images = torch.cat(images, dim=0)\n",
    "        captions = [item[1] for item in batch]\n",
    "        captions = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        return images, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2cf748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    root_folder,\n",
    "    annotation_file,\n",
    "    transform,\n",
    "    batch_size=32,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    dataset = FlickerDataset(\n",
    "        root_dir=root_folder,\n",
    "        captions_file=annotation_file,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fdffb9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: Custom Collate Function (Padding)\n",
    "\n",
    "### Why Do We Need This?\n",
    "\n",
    "Captions have different lengths:\n",
    "- Caption 1: `[<SOS>, \"a\", \"dog\", <EOS>]` ‚Üí 4 words\n",
    "- Caption 2: `[<SOS>, \"a\", \"cat\", \"on\", \"sofa\", <EOS>]` ‚Üí 6 words\n",
    "\n",
    "**Problem:** PyTorch requires all tensors in a batch to have the same shape.\n",
    "\n",
    "**Solution:** Pad shorter captions with `<PAD>` token to match the longest caption.\n",
    "\n",
    "### How Padding Works\n",
    "\n",
    "**Batch of 3 samples:**\n",
    "```python\n",
    "batch = [\n",
    "    (image1, tensor([1, 4, 6, 2])),          # Length 4\n",
    "    (image2, tensor([1, 4, 5, 7, 8, 2])),    # Length 6 (longest)\n",
    "    (image3, tensor([1, 4, 6, 5, 7, 2]))     # Length 6\n",
    "]\n",
    "```\n",
    "\n",
    "**After `pad_sequence`:**\n",
    "```python\n",
    "captions = tensor([\n",
    "    [1, 1, 1],      # <SOS> for all\n",
    "    [4, 4, 4],      # 'a' for all\n",
    "    [6, 5, 6],      # 'dog', 'cat', 'dog'\n",
    "    [2, 7, 5],      # <EOS>, 'on', 'in'\n",
    "    [0, 8, 7],      # <PAD>, 'sofa', 'park'\n",
    "    [0, 2, 2]       # <PAD>, <EOS>, <EOS>\n",
    "])\n",
    "# Shape: (max_length, batch_size) = (6, 3)\n",
    "```\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The `MyCollate` class:\n",
    "1. Takes a batch of `(image, caption)` pairs\n",
    "2. Stacks images into a single tensor\n",
    "3. Pads captions to the same length\n",
    "4. Returns both as tensors\n",
    "\n",
    "**Note:** `batch_first=False` because RNN/LSTM expects `(sequence_length, batch_size, embedding_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59717788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "dataloader=get_loader(\n",
    "    root_folder=\"flickr8k_images\",\n",
    "    annotation_file=\"captions.txt\",\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897099d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 10: Create DataLoader with Custom Collate\n",
    "\n",
    "### What `get_loader` Does\n",
    "\n",
    "This function combines everything we've built:\n",
    "1. Creates a `FlickerDataset` instance\n",
    "2. Gets the `<PAD>` token index from vocabulary\n",
    "3. Creates a `DataLoader` with custom padding function\n",
    "\n",
    "### Parameters Explained\n",
    "\n",
    "```python\n",
    "def get_loader(\n",
    "    root_folder,        # Path to images folder\n",
    "    annotation_file,    # Path to captions CSV\n",
    "    transform,          # Image transformations\n",
    "    batch_size=32,      # Number of samples per batch\n",
    "    num_workers=2,      # CPU threads for parallel loading\n",
    "    shuffle=True,       # Randomize order each epoch\n",
    "    pin_memory=True,    # Faster CPU ‚Üí GPU transfer\n",
    "):\n",
    "```\n",
    "\n",
    "**Key Parameters:**\n",
    "- **`batch_size=32`**: Load 32 image-caption pairs at once (faster than one-by-one)\n",
    "- **`num_workers=2`**: Use 2 CPU threads to load data in parallel while GPU trains\n",
    "- **`shuffle=True`**: Randomize order to prevent model from memorizing sequence\n",
    "- **`pin_memory=True`**: Allocate data in pinned memory (only useful with GPU)\n",
    "- **`collate_fn=MyCollate(pad_idx)`**: Use our custom padding function for variable-length captions\n",
    "\n",
    "### What It Returns\n",
    "\n",
    "The function returns both:\n",
    "1. **`loader`**: DataLoader for batching\n",
    "2. **`dataset`**: Dataset instance (useful for accessing vocabulary)\n",
    "\n",
    "```python\n",
    "loader, dataset = get_loader(...)\n",
    "vocab_size = len(dataset.vocab)  # Can access vocab from dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea2ffdd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 11: Using the DataLoader\n",
    "\n",
    "### Basic Usage\n",
    "\n",
    "```python\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataloader = get_loader(\n",
    "    root_folder=\"flickr8k_images\",\n",
    "    annotation_file=\"captions.txt\",\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    batch_size=32,\n",
    ")\n",
    "```\n",
    "\n",
    "### Looping Through Data\n",
    "\n",
    "```python\n",
    "for images, captions in dataloader:\n",
    "    # images.shape = (32, 3, 224, 224)\n",
    "    #   32 images, 3 channels (RGB), 224√ó224 pixels\n",
    "    \n",
    "    # captions.shape = (max_length, 32)\n",
    "    #   max_length = longest caption in batch\n",
    "    #   32 captions (one per image)\n",
    "    \n",
    "    # Feed to your model here\n",
    "    outputs = model(images, captions)\n",
    "```\n",
    "\n",
    "### Note About Running\n",
    "\n",
    "If you don't have the Flickr8k dataset downloaded, running this cell will show:\n",
    "```\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'flickr8k_images'\n",
    "```\n",
    "\n",
    "**This is expected!** The code structure is correct. To run it, you need the actual dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "**Part 1: Cats & Dogs Classification**\n",
    "1. Custom `CatsandDogsDataset` class\n",
    "2. Image loading and transforms\n",
    "3. Train/test split\n",
    "4. DataLoader for batch processing\n",
    "\n",
    "**Part 2: Image Captioning**\n",
    "1. `Vocabulary` class for text processing\n",
    "2. `FlickerDataset` with special tokens\n",
    "3. Custom collate function for padding\n",
    "4. Complete DataLoader pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Custom Dataset Requirements\n",
    "Every PyTorch Dataset needs:\n",
    "```python\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):           # Setup\n",
    "    def __len__(self):            # Return total count\n",
    "    def __getitem__(self, idx):   # Return one item\n",
    "```\n",
    "\n",
    "### Text Processing Pipeline\n",
    "1. **Tokenization**: Split text into words\n",
    "2. **Vocabulary building**: Map words to numbers\n",
    "3. **Special tokens**: `<SOS>`, `<EOS>`, `<PAD>`, `<UNK>`\n",
    "4. **Numericalization**: Convert text to numbers\n",
    "\n",
    "### Handling Variable-Length Sequences\n",
    "- Use `pad_sequence` to make all sequences same length\n",
    "- Custom collate function applies padding automatically\n",
    "- Essential for batch processing in RNNs/LSTMs\n",
    "\n",
    "### DataLoader Benefits\n",
    "- **Batching**: Process multiple samples at once\n",
    "- **Shuffling**: Randomize order for better training\n",
    "- **Parallel loading**: Load data while GPU trains\n",
    "- **Memory management**: Efficient data transfer\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "Now you can:\n",
    "1. Build CNN classifiers for custom image datasets\n",
    "2. Create image captioning models (CNN + RNN)\n",
    "3. Process text data for NLP tasks\n",
    "4. Handle any custom dataset structure\n",
    "\n",
    "**You're ready to train models on your own data!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
