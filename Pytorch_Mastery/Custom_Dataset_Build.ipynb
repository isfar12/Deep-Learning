{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ce38ca",
   "metadata": {},
   "source": [
    "# üìö Custom Dataset Building in PyTorch\n",
    "\n",
    "## üéØ What You'll Learn\n",
    "This notebook teaches you how to:\n",
    "1. **Build Custom Datasets** for images with labels (Cats & Dogs)\n",
    "2. **Handle Text Datasets** for image captioning (Flickr8k)\n",
    "3. **Create DataLoaders** for efficient batch processing\n",
    "4. **Build Vocabularies** for text processing\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î Why Do We Need Custom Datasets?\n",
    "\n",
    "**The Restaurant Analogy:**\n",
    "- **PyTorch's Built-in Datasets** = Fast food menu (limited options: MNIST, CIFAR10)\n",
    "- **Custom Datasets** = Your own restaurant menu (any dish you want!)\n",
    "\n",
    "**Real Problem:**\n",
    "Most real-world projects need custom data (medical images, company photos, specific text), not just standard datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Two Examples Covered\n",
    "\n",
    "### Example 1: Cats & Dogs Classification üê±üê∂\n",
    "**Goal:** Given an image ‚Üí predict \"cat\" or \"dog\"\n",
    "\n",
    "**Files needed:**\n",
    "- `cats_dogs/` folder with images (`cat_001.jpg`, `dog_001.jpg`, etc.)\n",
    "- `cats_dogs.csv` file with labels (filename, class)\n",
    "\n",
    "### Example 2: Image Captioning üì∑üí¨\n",
    "**Goal:** Given an image ‚Üí generate a text description\n",
    "\n",
    "**Files needed:**\n",
    "- `flickr8k_images/` folder with photos\n",
    "- `captions.txt` file with image-caption pairs\n",
    "\n",
    "Let's build both!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be21572",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Top panel ‚Äî dataset assets\n",
    "\n",
    "* A folder named **cats_dogs_resized** ‚Üí this is the directory that holds the prepared (resized) images.\n",
    "* A file named **cats_dogs** (Excel workbook) ‚Üí a spreadsheet used for labels/metadata (e.g., filename ‚Üí class). The size shows ~394 KB.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/cat_dog_directory.png\" alt=\"File Directory containing cat dog image and csv file with their identity\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "2. Middle panel ‚Äî the images themselves\n",
    "\n",
    "* Inside **cats_dogs_resized**, Windows Explorer displays a grid of **dog photos** (filenames like `dog_3286.jpg`, `dog_3297.jpg`, ‚Ä¶ `dog_3312.jpg`).\n",
    "* These are varied pictures (different breeds/poses/backgrounds) but all **uniformly resized**, suitable for a ML dataset.\n",
    "<figure>\n",
    "  <img src=\"asset/cat_dog_image_directory.png\" alt=\"inside of folder\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "3. Bottom panel ‚Äî the label spreadsheet\n",
    "\n",
    "* An Excel sheet with **filenames in Column A** (e.g., `dog_2511.jpg`, `dog_2512.jpg`, ‚Ä¶).\n",
    "* **Column B contains numeric class labels** (shown as `1` for these rows). Given the filenames are ‚Äúdog_‚Ä¶‚Äù, this implies a mapping such as **1 = dog** (and likely **0 = cat** elsewhere in the sheet).\n",
    "* Columns C/D are empty in the visible portion (reserved for other info if needed).\n",
    "\n",
    "<figure>\n",
    "  <img src=\"asset/cat_dog_csv_structure.png\" alt=\"inside of the csv\" width=\"400\">\n",
    "</figure>\n",
    "\n",
    "In short: the image depicts a typical classification dataset setup‚Äî(1) a folder of resized images, (2) a preview of those images, and (3) a spreadsheet mapping each filename to a class label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0256282",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üê± Part 1: Cats & Dogs Classification Dataset\n",
    "\n",
    "## üìÇ Understanding the Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f55f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from skimage import io\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2eb4e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Step 1: Import Required Libraries\n",
    "\n",
    "### üçï Simple Analogy: Kitchen Tools\n",
    "Before cooking, you gather your tools:\n",
    "- **Knife** = `torch` (main PyTorch library)\n",
    "- **Cutting board** = `Dataset` (structure for organizing data)\n",
    "- **Serving tray** = `DataLoader` (delivers batches of data)\n",
    "- **Recipe book** = `pandas` (reads CSV files)\n",
    "- **Camera** = `io` (loads images from disk)\n",
    "\n",
    "### üîß What Each Library Does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2b6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d6f12",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch                        # Main PyTorch library (tensors, neural networks)\n",
    "from torch.utils.data import DataLoader, Dataset  # Tools for data handling\n",
    "from skimage import io             # For reading images\n",
    "import pandas as pd                # For reading CSV files\n",
    "import os                          # For file path operations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üñ•Ô∏è Step 2: Set Device (CPU or GPU)\n",
    "\n",
    "### üöó Simple Analogy: Choosing Your Vehicle\n",
    "- **GPU (cuda)** = Race car üèéÔ∏è (fast, for heavy workloads)\n",
    "- **CPU** = Regular car üöó (slower, but always available)\n",
    "\n",
    "### üîß Technical Explanation\n",
    "This line checks if you have a GPU available. If yes, use it; otherwise, use CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f28bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatsandDogsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations=pd.read_csv(csv_file)\n",
    "        self.root_dir=root_dir\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path=os.path.join(self.root_dir, self.annotations.iloc[index,0]) #ith row, 0th column\n",
    "        image=io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index,1])) #ith row, 1st column\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, y_label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22faef",
   "metadata": {},
   "source": [
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    "**What happens:**\n",
    "- If GPU exists ‚Üí `device = \"cuda\"` ‚ö°\n",
    "- If no GPU ‚Üí `device = \"cpu\"` üê¢\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Step 3: Build the Custom Dataset Class\n",
    "\n",
    "### üçï Simple Analogy: Pizza Delivery Service\n",
    "\n",
    "Imagine you run a pizza delivery service:\n",
    "1. **Menu (CSV file)**: List of orders ‚Üí `[\"Margherita.jpg\", \"Pepperoni\"]`\n",
    "2. **Kitchen (root_dir)**: Where pizzas are stored ‚Üí `pizzas/` folder\n",
    "3. **Transform**: Heat up the pizza before delivery (resize, normalize images)\n",
    "\n",
    "When customer orders pizza #5:\n",
    "- Look up order #5 in menu ‚Üí get filename and type\n",
    "- Go to kitchen, grab that pizza\n",
    "- Heat it up (transform)\n",
    "- Deliver (return image + label)\n",
    "\n",
    "### üîß Technical Explanation: The Dataset Class\n",
    "\n",
    "A **Dataset** in PyTorch must have 3 methods:\n",
    "1. **`__init__`**: Setup (read CSV, store paths)\n",
    "2. **`__len__`**: Return total number of items\n",
    "3. **`__getitem__`**: Get one item (image + label) by index\n",
    "\n",
    "Let's break down each part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23aebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset=CatsandDogsDataset(csv_file='cats_dogs.csv', root_dir='cats_dogs',transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08d196",
   "metadata": {},
   "source": [
    "```python\n",
    "class CatsandDogsDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "        \n",
    "        Args:\n",
    "            csv_file: Path to CSV with [filename, label] columns\n",
    "            root_dir: Directory with all the images\n",
    "            transform: Optional transforms to apply to images\n",
    "        \"\"\"\n",
    "        self.annotations = pd.read_csv(csv_file)  # Read CSV into DataFrame\n",
    "        self.root_dir = root_dir                   # Store image folder path\n",
    "        self.transform = transform                 # Store transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples\"\"\"\n",
    "        return len(self.annotations)  # Number of rows in CSV\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get one sample (image + label) by index\n",
    "        \n",
    "        Args:\n",
    "            index: Integer index (0 to len-1)\n",
    "            \n",
    "        Returns:\n",
    "            image: Transformed image tensor\n",
    "            y_label: Class label (0 or 1)\n",
    "        \"\"\"\n",
    "        # 1. Get image path from CSV (row=index, column=0)\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        \n",
    "        # 2. Load image from disk\n",
    "        image = io.imread(img_path)  # Returns numpy array (H, W, C)\n",
    "        \n",
    "        # 3. Get label from CSV (row=index, column=1)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n",
    "        \n",
    "        # 4. Apply transforms if provided\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, y_label\n",
    "```\n",
    "\n",
    "### üìä Example Walkthrough\n",
    "\n",
    "**CSV File (`cats_dogs.csv`):**\n",
    "```\n",
    "filename,label\n",
    "cat_001.jpg,0\n",
    "dog_001.jpg,1\n",
    "cat_002.jpg,0\n",
    "```\n",
    "\n",
    "**Code Flow:**\n",
    "```python\n",
    "dataset[1]  # Request item at index 1\n",
    "```\n",
    "\n",
    "**Step-by-step:**\n",
    "1. `index = 1`\n",
    "2. `img_path = \"cats_dogs/dog_001.jpg\"` (row 1, column 0)\n",
    "3. `image = io.imread(\"cats_dogs/dog_001.jpg\")` ‚Üí loads image as array\n",
    "4. `y_label = tensor(1)` (row 1, column 1)\n",
    "5. `image = transform(image)` ‚Üí resize to (128, 128) and convert to tensor\n",
    "6. `return (image_tensor, tensor(1))`\n",
    "\n",
    "---\n",
    "\n",
    "## üé¨ Step 4: Create Dataset Instance & Apply Transforms\n",
    "\n",
    "### üçï Simple Analogy: Setting Up Your Order System\n",
    "\n",
    "You're now opening your pizza shop:\n",
    "1. **Menu** = `cats_dogs.csv` (what pizzas you have)\n",
    "2. **Kitchen location** = `cats_dogs/` folder\n",
    "3. **Heating instructions** = transforms (resize, convert to tensor)\n",
    "\n",
    "### üîß What Each Transform Does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1db47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = torch.utils.data.random_split(dataset, [800, 200])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d6150",
   "metadata": {},
   "source": [
    "```python\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset = CatsandDogsDataset(\n",
    "    csv_file='cats_dogs.csv',      # Path to labels\n",
    "    root_dir='cats_dogs',          # Path to images folder\n",
    "    transform=transforms.Compose([  # Chain of transformations\n",
    "        transforms.ToPILImage(),    # 1. Convert numpy array to PIL Image\n",
    "        transforms.Resize((128, 128)),  # 2. Resize to 128x128 pixels\n",
    "        transforms.ToTensor()       # 3. Convert to tensor & normalize [0,1]\n",
    "    ])\n",
    ")\n",
    "```\n",
    "\n",
    "### üìä Transform Pipeline Explanation\n",
    "\n",
    "**Input:** Numpy array from `io.imread()` ‚Üí shape `(H, W, 3)`, values `[0-255]`\n",
    "\n",
    "**Step 1: `ToPILImage()`**\n",
    "- Converts numpy array to PIL Image\n",
    "- Why? Because `Resize()` works best with PIL Images\n",
    "\n",
    "**Step 2: `Resize((128, 128))`**\n",
    "- Resizes image to 128√ó128 pixels\n",
    "- Why? Neural networks need fixed-size inputs\n",
    "\n",
    "**Step 3: `ToTensor()`**\n",
    "- Converts PIL Image to PyTorch tensor\n",
    "- Changes shape from `(H, W, C)` to `(C, H, W)`\n",
    "- Normalizes values from `[0, 255]` to `[0.0, 1.0]`\n",
    "\n",
    "**Output:** Tensor of shape `(3, 128, 128)` with values in `[0.0, 1.0]`\n",
    "\n",
    "---\n",
    "\n",
    "## üîÄ Step 5: Split into Train/Test Sets\n",
    "\n",
    "### üçï Simple Analogy: Practice vs Competition\n",
    "\n",
    "You have 1000 pizzas:\n",
    "- **800 pizzas** = Practice cooking (training set) üë®‚Äçüç≥\n",
    "- **200 pizzas** = Competition cooking (test set) üèÜ\n",
    "\n",
    "You practice on 800, then test your skills on unseen 200 pizzas.\n",
    "\n",
    "### üîß Technical Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6969e62",
   "metadata": {},
   "source": [
    "This image shows the structure of the Flickr8k Image Captioning dataset, where each photo (in /images) is linked to multiple natural-language descriptions (in /captions). The task is to build a model that can look at a new image and describe it in words.\n",
    "\n",
    "\n",
    "###  Folder structure\n",
    "<figure>\n",
    "  <img src=\"asset/image_caption_directory.png\" alt=\"File Directory containing  image and text file with their caption\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "* **`images`** ‚Üí A folder containing all the image files.\n",
    "* **`captions`** ‚Üí A text file (`.txt`) containing all the image-caption pairs.\n",
    "  This setup is part of the **Flickr8k dataset**, which is widely used for *image captioning* tasks in deep learning.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Folder content preview\n",
    "<figure>\n",
    "  <img src=\"asset/image_caption_image_directory.png\" alt=\"inside of folder\" width=\"800\">\n",
    "</figure>\n",
    "* The path is `Desktop > customdata > flickr8k > images`.\n",
    "* It shows thumbnails of several **JPEG images**, each named with an ID like `667626_18933d13e`, `10815824_2997e03d76`, etc.\n",
    "* The images show various human and animal activities (children playing, dogs, people near water, etc.).\n",
    "  These are the **input images** used for training and evaluation in the captioning model.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  Captions file opened in Notepad\n",
    "<figure>\n",
    "  <img src=\"asset/image_caption_caption_file.png\" alt=\"inside of the txt\" width=\"800\">\n",
    "</figure>\n",
    "\n",
    "* The text file has **two columns**:\n",
    "\n",
    "  * **Column 1:** Image filename (e.g., `1000268201_693b08cb0e.jpg`)\n",
    "  * **Column 2:** The **caption** (natural language description).\n",
    "* Example lines:\n",
    "\n",
    "  * `1000268201_693b08cb0e.jpg, A child in a pink dress is climbing up a set of stairs in an entry way.`\n",
    "  * `1000268201_693b08cb0e.jpg, A girl going into a wooden building.`\n",
    "  * `1000268201_693b08cb0e.jpg, A little girl climbing into a wooden playhouse.`\n",
    "* Note that the same image appears multiple times, each with a **different caption** ‚Üí this is a common feature in Flickr8k: *five human-written captions per image*.\n",
    "\n",
    "\n",
    "**In short:**\n",
    "This image shows the structure of the **Flickr8k Image Captioning dataset**, where each photo (in `/images`) is linked to multiple natural-language descriptions (in `/captions`). The task is to build a model that can look at a new image and describe it in words.\n",
    "\n",
    "\n",
    "| Component       | Description                               | Purpose                              |\n",
    "| --------------- | ----------------------------------------- | ------------------------------------ |\n",
    "| `images` folder | Set of photos (8,000 total)               | Visual input                         |\n",
    "| `captions.txt`  | Image filename + 5 human captions         | Ground-truth text labels             |\n",
    "| Combined usage  | Each image paired with multiple sentences | For training Image Captioning models |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b04176",
   "metadata": {},
   "source": [
    "```python\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [800, 200])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=32, shuffle=True)\n",
    "```\n",
    "\n",
    "### üìä What Happens Here\n",
    "\n",
    "**1. `random_split(dataset, [800, 200])`**\n",
    "- Randomly splits 1000 images into 800 and 200\n",
    "- Returns two subset objects\n",
    "\n",
    "**2. `DataLoader` for Training:**\n",
    "- `batch_size=32`: Delivers 32 images at a time (not one by one)\n",
    "- `shuffle=True`: Randomizes order each epoch (prevents memorization)\n",
    "\n",
    "**3. `DataLoader` for Testing:**\n",
    "- Same batch size\n",
    "- Shuffle to avoid bias\n",
    "\n",
    "### üéØ Why Batching?\n",
    "\n",
    "**Without batching (batch_size=1):**\n",
    "```python\n",
    "for image, label in train_loader:  # One image at a time\n",
    "    # image.shape = (1, 3, 128, 128)\n",
    "    # Train on 1 image ‚Üí update weights ‚Üí repeat 800 times\n",
    "    # Super slow! üê¢\n",
    "```\n",
    "\n",
    "**With batching (batch_size=32):**\n",
    "```python\n",
    "for images, labels in train_loader:  # 32 images at once\n",
    "    # images.shape = (32, 3, 128, 128)\n",
    "    # Train on 32 images ‚Üí update weights ‚Üí repeat 25 times\n",
    "    # Much faster! ‚ö°\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üì∑ Part 2: Image Captioning Dataset\n",
    "\n",
    "## ü§î What's Different Here?\n",
    "\n",
    "**Cats & Dogs:**\n",
    "- Input: Image ‚Üí Output: Single number (0 or 1)\n",
    "\n",
    "**Image Captioning:**\n",
    "- Input: Image ‚Üí Output: **Sentence** (\"A dog playing in the park\")\n",
    "\n",
    "**New Challenge:** How do we handle text?\n",
    "- Need to convert words to numbers (vocabulary)\n",
    "- Need to handle variable-length captions\n",
    "- Need to pad sequences to same length\n",
    "\n",
    "Let's build it step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6969e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} #iots: index to string\n",
    "        self.stoi = {v: k for k, v in self.itos.items()} #stoi: string to index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    #        \"I love dogs\" -> ['i', 'love', 'dogs']\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] +=1\n",
    "                \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx +=1\n",
    "                    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9555ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìñ Step 6: Build a Vocabulary Class\n",
    "\n",
    "### üçï Simple Analogy: Restaurant Menu with Item Numbers\n",
    "\n",
    "Imagine a restaurant where:\n",
    "- Each dish has a **number** (easier for kitchen)\n",
    "- **Menu**: \"Pizza\" = 5, \"Burger\" = 10, \"Salad\" = 15\n",
    "\n",
    "**For neural networks:**\n",
    "- Can't process words directly (\"cat\", \"dog\", \"running\")\n",
    "- Need to convert words to numbers: {\"cat\": 4, \"dog\": 5, \"running\": 6}\n",
    "\n",
    "**Special tokens:**\n",
    "- `<PAD>` = 0: Padding (to make all sentences same length)\n",
    "- `<SOS>` = 1: Start of sentence\n",
    "- `<EOS>` = 2: End of sentence\n",
    "- `<UNK>` = 3: Unknown word (not in vocabulary)\n",
    "\n",
    "### üîß Technical Explanation: Vocabulary Class\n",
    "\n",
    "This class converts text to numbers and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e17dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81169e12",
   "metadata": {},
   "source": [
    "```python\n",
    "import spacy\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")  # English language model\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        \"\"\"\n",
    "        freq_threshold: Minimum word frequency to include in vocabulary\n",
    "                       (e.g., 5 means word must appear 5+ times)\n",
    "        \"\"\"\n",
    "        self.freq_threshold = freq_threshold\n",
    "        \n",
    "        # Initialize with special tokens\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}  # index to string\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}  # string to index\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return vocabulary size\"\"\"\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        \"\"\"\n",
    "        Split sentence into words (tokens)\n",
    "        Example: \"I love dogs\" ‚Üí ['i', 'love', 'dogs']\n",
    "        \"\"\"\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \"\"\"\n",
    "        Build vocabulary from list of sentences\n",
    "        Only include words that appear >= freq_threshold times\n",
    "        \"\"\"\n",
    "        frequencies = {}  # Count word occurrences\n",
    "        idx = 4  # Start from 4 (0-3 reserved for special tokens)\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                # Count word frequency\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] += 1\n",
    "                \n",
    "                # Add to vocabulary when threshold reached\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "    \n",
    "    def numericalize(self, text):\n",
    "        \"\"\"\n",
    "        Convert text to list of numbers\n",
    "        Example: \"I love dogs\" ‚Üí [45, 67, 89]\n",
    "        Unknown words ‚Üí 3 (<UNK>)\n",
    "        \"\"\"\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])  # Use <UNK> if word not in vocab\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "```\n",
    "\n",
    "### üìä Example Walkthrough\n",
    "\n",
    "**Input sentences:**\n",
    "```python\n",
    "sentences = [\n",
    "    \"A dog playing in park\",\n",
    "    \"A cat sleeping on sofa\",\n",
    "    \"A dog running in park\",\n",
    "    \"A bird flying in sky\",\n",
    "    \"A dog jumping in park\"\n",
    "]\n",
    "```\n",
    "\n",
    "**Step 1: Count frequencies**\n",
    "```python\n",
    "frequencies = {\n",
    "    'a': 5, 'dog': 3, 'playing': 1, 'in': 4, 'park': 3,\n",
    "    'cat': 1, 'sleeping': 1, 'on': 1, 'sofa': 1, ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 2: Build vocabulary (freq_threshold=3)**\n",
    "```python\n",
    "# Only words appearing 3+ times\n",
    "vocab.stoi = {\n",
    "    '<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3,\n",
    "    'a': 4, 'in': 5, 'dog': 6, 'park': 7\n",
    "}\n",
    "```\n",
    "\n",
    "**Step 3: Numericalize**\n",
    "```python\n",
    "vocab.numericalize(\"A dog playing in park\")\n",
    "# Output: [4, 6, 3, 5, 7]\n",
    "#         'a' 'dog' '<UNK>' 'in' 'park'\n",
    "#         (playing ‚Üí <UNK> because frequency < 3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Step 7: Import Additional Libraries for Image Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfbf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickerDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir=root_dir\n",
    "        self.df=pd.read_csv(captions_file)\n",
    "        self.transform=transform\n",
    "        \n",
    "        self.imgs=self.df['image']\n",
    "        self.captions=self.df['caption']\n",
    "        \n",
    "        self.vocab=Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption=self.captions[index]\n",
    "        img_id=self.imgs[index]\n",
    "        img_path=os.path.join(self.root_dir, img_id)\n",
    "        image=Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image=self.transform(image)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return image, torch.tensor(numericalized_caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8005c96",
   "metadata": {},
   "source": [
    "```python\n",
    "import spacy                        # For text tokenization\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence  # For padding variable-length sequences\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image              # Better image handling than skimage\n",
    "```\n",
    "\n",
    "**New libraries:**\n",
    "- **`pad_sequence`**: Makes all captions the same length (we'll see this later)\n",
    "- **`PIL`**: Handles images better, works well with torchvision transforms\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Step 8: Build Flickr Dataset Class\n",
    "\n",
    "### üçï Simple Analogy: Photo Album with Descriptions\n",
    "\n",
    "You have a photo album where:\n",
    "- **Photos** = Images in `flickr8k_images/` folder\n",
    "- **Descriptions** = Captions in `captions.txt` file\n",
    "- Each photo has 5 different descriptions written by different people\n",
    "\n",
    "When you request photo #10:\n",
    "1. Look up description #10 in the text file\n",
    "2. Find corresponding image\n",
    "3. Convert description to numbers using vocabulary\n",
    "4. Return (image_tensor, caption_numbers)\n",
    "\n",
    "### üîß Technical Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02788bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx=pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        images = [item[0].unsqueeze(0) for item in batch]\n",
    "        images = torch.cat(images, dim=0)\n",
    "        captions = [item[1] for item in batch]\n",
    "        captions = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        return images, captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221aef7c",
   "metadata": {},
   "source": [
    "```python\n",
    "class FlickerDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        \"\"\"\n",
    "        Initialize Flickr8k dataset\n",
    "        \n",
    "        Args:\n",
    "            root_dir: Folder containing images\n",
    "            captions_file: CSV file with [image, caption] columns\n",
    "            transform: Image transformations\n",
    "            freq_threshold: Min frequency for words in vocabulary\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)  # Read captions CSV\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract columns\n",
    "        self.imgs = self.df['image']      # Image filenames\n",
    "        self.captions = self.df['caption']  # Caption texts\n",
    "        \n",
    "        # Build vocabulary from all captions\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return total number of image-caption pairs\"\"\"\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get one image-caption pair\n",
    "        \n",
    "        Returns:\n",
    "            image: Transformed image tensor\n",
    "            numericalized_caption: Caption as list of numbers with <SOS> and <EOS>\n",
    "        \"\"\"\n",
    "        # 1. Get caption text\n",
    "        caption = self.captions[index]\n",
    "        \n",
    "        # 2. Get image filename and load image\n",
    "        img_id = self.imgs[index]\n",
    "        img_path = os.path.join(self.root_dir, img_id)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # 3. Apply transforms to image\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # 4. Convert caption to numbers with special tokens\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]          # Start token\n",
    "        numericalized_caption += self.vocab.numericalize(caption)   # Caption words\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])      # End token\n",
    "        \n",
    "        return image, torch.tensor(numericalized_caption)\n",
    "```\n",
    "\n",
    "### üìä Example Walkthrough\n",
    "\n",
    "**CSV file (`captions.txt`):**\n",
    "```\n",
    "image,caption\n",
    "dog_001.jpg,A brown dog running in the park\n",
    "cat_002.jpg,A white cat sleeping on a sofa\n",
    "```\n",
    "\n",
    "**Request item at index 0:**\n",
    "```python\n",
    "image, caption = dataset[0]\n",
    "```\n",
    "\n",
    "**Step-by-step:**\n",
    "1. `caption = \"A brown dog running in the park\"`\n",
    "2. `img_path = \"flickr8k_images/dog_001.jpg\"`\n",
    "3. `image = Image.open(...)` ‚Üí load and transform to tensor\n",
    "4. Convert caption to numbers:\n",
    "   ```python\n",
    "   # Tokenize: ['a', 'brown', 'dog', 'running', 'in', 'the', 'park']\n",
    "   # Numericalize: [1, 4, 45, 6, 89, 5, 12, 7, 2]\n",
    "   #                <SOS> a brown dog running in the park <EOS>\n",
    "   ```\n",
    "5. Return: `(image_tensor, tensor([1, 4, 45, 6, 89, 5, 12, 7, 2]))`\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Step 9: Custom Collate Function (Padding)\n",
    "\n",
    "### üçï Simple Analogy: Boxing Pizzas of Different Sizes\n",
    "\n",
    "You have pizzas of different sizes:\n",
    "- Small pizza: 6 inches\n",
    "- Medium pizza: 10 inches\n",
    "- Large pizza: 14 inches\n",
    "\n",
    "But your delivery boxes are all the same size (14 inches). So you:\n",
    "- Put small pizza in box + add padding (6 ‚Üí 14)\n",
    "- Put medium pizza in box + add padding (10 ‚Üí 14)\n",
    "- Put large pizza in box (already fits!)\n",
    "\n",
    "**For captions:**\n",
    "- Caption 1: 5 words\n",
    "- Caption 2: 12 words\n",
    "- Caption 3: 8 words\n",
    "\n",
    "Need to pad all to length 12 (longest) using `<PAD>` token (value = 0).\n",
    "\n",
    "### üîß Technical Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2cf748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    root_folder,\n",
    "    annotation_file,\n",
    "    transform,\n",
    "    batch_size=32,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    dataset = FlickerDataset(\n",
    "        root_dir=root_folder,\n",
    "        captions_file=annotation_file,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fdffb9",
   "metadata": {},
   "source": [
    "```python\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        \"\"\"\n",
    "        pad_idx: Index of <PAD> token (usually 0)\n",
    "        \"\"\"\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function to create batches with padded captions\n",
    "        \n",
    "        Args:\n",
    "            batch: List of tuples [(image1, caption1), (image2, caption2), ...]\n",
    "            \n",
    "        Returns:\n",
    "            images: Stacked tensor of images (batch_size, C, H, W)\n",
    "            captions: Padded captions (max_length, batch_size)\n",
    "        \"\"\"\n",
    "        # 1. Extract images and stack them\n",
    "        images = [item[0].unsqueeze(0) for item in batch]  # Add batch dimension\n",
    "        images = torch.cat(images, dim=0)  # Stack: (batch_size, C, H, W)\n",
    "        \n",
    "        # 2. Extract captions (variable lengths)\n",
    "        captions = [item[1] for item in batch]\n",
    "        \n",
    "        # 3. Pad captions to same length\n",
    "        captions = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        # Output shape: (max_length, batch_size)\n",
    "        \n",
    "        return images, captions\n",
    "```\n",
    "\n",
    "### üìä Example: How Padding Works\n",
    "\n",
    "**Batch of 3 samples:**\n",
    "```python\n",
    "batch = [\n",
    "    (image1, tensor([1, 4, 6, 2])),          # Length 4: <SOS> a dog <EOS>\n",
    "    (image2, tensor([1, 4, 5, 7, 8, 2])),    # Length 6: <SOS> a cat on sofa <EOS>\n",
    "    (image3, tensor([1, 4, 6, 5, 7, 2]))     # Length 6: <SOS> a dog in park <EOS>\n",
    "]\n",
    "```\n",
    "\n",
    "**After `pad_sequence`:**\n",
    "```python\n",
    "captions = tensor([\n",
    "    [1, 1, 1],      # <SOS> for all 3\n",
    "    [4, 4, 4],      # 'a' for all 3\n",
    "    [6, 5, 6],      # 'dog', 'cat', 'dog'\n",
    "    [2, 7, 5],      # <EOS>, 'on', 'in'\n",
    "    [0, 8, 7],      # <PAD>, 'sofa', 'park'\n",
    "    [0, 2, 2]       # <PAD>, <EOS>, <EOS>\n",
    "])\n",
    "# Shape: (6, 3) = (max_length, batch_size)\n",
    "```\n",
    "\n",
    "**Why `batch_first=False`?**\n",
    "- RNN/LSTM expects input as `(sequence_length, batch_size, embedding_dim)`\n",
    "- If `batch_first=True` ‚Üí shape would be `(batch_size, sequence_length)`\n",
    "\n",
    "---\n",
    "\n",
    "## üé¨ Step 10: Create DataLoader with Custom Collate\n",
    "\n",
    "### üçï Simple Analogy: Automated Pizza Delivery Service\n",
    "\n",
    "You set up an automated system that:\n",
    "1. Takes orders (reads images + captions)\n",
    "2. Boxes pizzas (pads captions)\n",
    "3. Delivers in batches of 32 orders at a time\n",
    "\n",
    "### üîß Technical Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59717788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "dataloader=get_loader(\n",
    "    root_folder=\"flickr8k_images\",\n",
    "    annotation_file=\"captions.txt\",\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    ),\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897099d",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_loader(\n",
    "    root_folder,\n",
    "    annotation_file,\n",
    "    transform,\n",
    "    batch_size=32,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a DataLoader for Flickr8k dataset\n",
    "    \n",
    "    Args:\n",
    "        root_folder: Path to images folder\n",
    "        annotation_file: Path to captions CSV\n",
    "        transform: Image transformations\n",
    "        batch_size: Number of samples per batch\n",
    "        num_workers: Number of CPU threads for data loading\n",
    "        shuffle: Randomize order each epoch\n",
    "        pin_memory: Speed up CPU ‚Üí GPU transfer\n",
    "        \n",
    "    Returns:\n",
    "        loader: PyTorch DataLoader object\n",
    "    \"\"\"\n",
    "    # 1. Create dataset instance\n",
    "    dataset = FlickerDataset(\n",
    "        root_dir=root_folder,\n",
    "        captions_file=annotation_file,\n",
    "        transform=transform,\n",
    "    )\n",
    "    \n",
    "    # 2. Get PAD token index\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    \n",
    "    # 3. Create DataLoader with custom collate function\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),  # Custom padding function\n",
    "    )\n",
    "    \n",
    "    return loader\n",
    "```\n",
    "\n",
    "### üìä What Each Parameter Does\n",
    "\n",
    "**`batch_size=32`**\n",
    "- Load 32 image-caption pairs at once\n",
    "- Larger batch = faster training but more memory\n",
    "\n",
    "**`num_workers=2`**\n",
    "- Use 2 CPU threads to load data in parallel\n",
    "- While GPU processes batch 1, CPU loads batch 2 (faster!)\n",
    "\n",
    "**`shuffle=True`**\n",
    "- Randomize order every epoch\n",
    "- Prevents model from memorizing order\n",
    "\n",
    "**`pin_memory=True`**\n",
    "- Allocates data in pinned memory (faster CPU ‚Üí GPU transfer)\n",
    "- Only use if you have a GPU\n",
    "\n",
    "**`collate_fn=MyCollate(pad_idx=pad_idx)`**\n",
    "- Use our custom function to pad captions\n",
    "- Default collate doesn't know how to handle variable-length sequences\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Step 11: Use the DataLoader\n",
    "\n",
    "### üçï Simple Analogy: Finally Opening Your Restaurant!\n",
    "\n",
    "You've set up everything:\n",
    "- ‚úÖ Menu (CSV)\n",
    "- ‚úÖ Kitchen (image folder)\n",
    "- ‚úÖ Cooking instructions (transforms)\n",
    "- ‚úÖ Delivery system (DataLoader)\n",
    "\n",
    "Now customers can order, and you deliver in batches!\n",
    "\n",
    "### üîß How to Use It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f15b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ea2ffdd",
   "metadata": {},
   "source": [
    "```python\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataloader = get_loader(\n",
    "    root_folder=\"flickr8k_images\",     # Where images are stored\n",
    "    annotation_file=\"captions.txt\",     # Where captions are stored\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to 224√ó224\n",
    "        transforms.ToTensor(),          # Convert to tensor [0, 1]\n",
    "    ]),\n",
    "    batch_size=32,\n",
    ")\n",
    "```\n",
    "\n",
    "### üéØ How to Loop Through Data\n",
    "\n",
    "```python\n",
    "for images, captions in dataloader:\n",
    "    # images.shape = (32, 3, 224, 224)\n",
    "    #   - 32 images\n",
    "    #   - 3 color channels (RGB)\n",
    "    #   - 224√ó224 pixels\n",
    "    \n",
    "    # captions.shape = (max_length, 32)\n",
    "    #   - max_length = longest caption in this batch\n",
    "    #   - 32 captions (one per image)\n",
    "    \n",
    "    # Train your model here!\n",
    "    pass\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è Note About Running\n",
    "\n",
    "**If you don't have the dataset**, running this cell will cause an error:\n",
    "```\n",
    "FileNotFoundError: [Errno 2] No such file or directory: 'flickr8k_images'\n",
    "```\n",
    "\n",
    "**This is expected!** The code structure is correct. To actually run it, you'd need to download the Flickr8k dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üéä Summary: What We Built\n",
    "\n",
    "### üê± Part 1: Cats & Dogs Classification\n",
    "1. ‚úÖ Built `CatsandDogsDataset` class\n",
    "2. ‚úÖ Loaded images and labels from CSV\n",
    "3. ‚úÖ Applied transforms (resize, convert to tensor)\n",
    "4. ‚úÖ Created train/test split\n",
    "5. ‚úÖ Created DataLoaders for batching\n",
    "\n",
    "### üì∑ Part 2: Image Captioning\n",
    "1. ‚úÖ Built `Vocabulary` class (text ‚Üí numbers)\n",
    "2. ‚úÖ Built `FlickerDataset` class (images + captions)\n",
    "3. ‚úÖ Added special tokens (`<SOS>`, `<EOS>`, `<PAD>`, `<UNK>`)\n",
    "4. ‚úÖ Built custom collate function for padding\n",
    "5. ‚úÖ Created DataLoader with variable-length sequences\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### 1Ô∏è‚É£ **Custom Datasets Need 3 Methods**\n",
    "```python\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):    # Setup\n",
    "    def __len__(self):     # Return total count\n",
    "    def __getitem__(self, idx):  # Return one item\n",
    "```\n",
    "\n",
    "### 2Ô∏è‚É£ **Text Must Be Converted to Numbers**\n",
    "- Build vocabulary from training data\n",
    "- Use special tokens for padding and sequence markers\n",
    "- Handle unknown words with `<UNK>`\n",
    "\n",
    "### 3Ô∏è‚É£ **Variable-Length Sequences Need Padding**\n",
    "- Use `pad_sequence` to make all captions same length\n",
    "- Custom collate function handles this automatically\n",
    "\n",
    "### 4Ô∏è‚É£ **DataLoader Makes Life Easy**\n",
    "- Batching: Load multiple samples at once\n",
    "- Shuffling: Randomize order for better training\n",
    "- Multi-threading: Load data while GPU trains\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you can load custom data, you can:\n",
    "1. Build a CNN classifier for cats & dogs\n",
    "2. Build an image captioning model (CNN encoder + RNN decoder)\n",
    "3. Train on your own custom datasets!\n",
    "\n",
    "**Happy coding! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
