{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumor Detection using CNN (PyTorch)\n",
    "\n",
    "Binary classification: Brain Tumor vs Healthy\n",
    "\n",
    "**Dataset**: Brain MRI images  \n",
    "**Goal**: Classify brain scans as tumor or healthy  \n",
    "**Framework**: PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "**Split Dataset**: Use `splitfolders` to split into train/val (80/20)  \n",
    "**Data Augmentation**: RandomHorizontalFlip, RandomRotation, Resize, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../input/brain-tumor-mri-dataset/'\n",
    "output_path = 'brain_tumor_splitted'\n",
    "\n",
    "splitfolders.ratio(input_path, output=output_path, seed=42, ratio=(0.8, 0.2))\n",
    "print(\"Data split completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Walkthrough:**\n",
    "- `splitfolders.ratio()` splits the dataset into train (80%) and validation (20%)\n",
    "- `seed=42` ensures reproducible splits\n",
    "- Creates folder structure: `brain_tumor_splitted/train/` and `brain_tumor_splitted/val/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Transforms\n",
    "\n",
    "Apply augmentation to training data and normalization to both train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Walkthrough:**\n",
    "- **Training transforms**: Resize to 224x224, flip horizontally (50% chance), rotate up to 10°, normalize\n",
    "- **Validation transforms**: Only resize and normalize (no augmentation for validation)\n",
    "- `Normalize(mean=[0.5], std=[0.5])` scales pixel values to [-1, 1] range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'brain_tumor_splitted/train'\n",
    "val_path = 'brain_tumor_splitted/val'\n",
    "\n",
    "train_set = ImageFolder(train_path, transform=train_transform)\n",
    "val_set = ImageFolder(val_path, transform=val_transform)\n",
    "\n",
    "print(f\"Training samples: {len(train_set)}\")\n",
    "print(f\"Validation samples: {len(val_set)}\")\n",
    "print(f\"Classes: {train_set.classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "for X, y in train_loader:\n",
    "    print(f\"Batch shape: {X.shape}\")  \n",
    "    print(f\"Label shape: {y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define CNN Model\n",
    "\n",
    "**Architecture**: 4 Convolutional blocks + 2 Fully Connected layers  \n",
    "**Features**: Batch Normalization, Dropout, MaxPooling  \n",
    "**Output**: Binary classification (Tumor vs Healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_TUMOR(\n",
      "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=12544, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN_TUMOR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_TUMOR, self).__init__()\n",
    "        \n",
    "        # Conv Block 1: 1 -> 8 channels\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        # Conv Block 2: 8 -> 16 channels\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        # Conv Block 3: 16 -> 32 channels\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Conv Block 4: 32 -> 64 channels\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # Fully Connected layers\n",
    "        # After 4 pooling layers: 224 -> 112 -> 56 -> 28 -> 14\n",
    "        # Feature map size: 14x14x64 = 12544\n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Block 4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = CNN_TUMOR().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Architecture:**\n",
    "\n",
    "| Layer | Input | Output | Operation |\n",
    "|-------|-------|--------|-----------|\n",
    "| Conv1 + Pool | 224×224×1 | 112×112×8 | Conv → BN → ReLU → MaxPool |\n",
    "| Conv2 + Pool | 112×112×8 | 56×56×16 | Conv → BN → ReLU → MaxPool |\n",
    "| Conv3 + Pool | 56×56×16 | 28×28×32 | Conv → BN → ReLU → MaxPool |\n",
    "| Conv4 + Pool | 28×28×32 | 14×14×64 | Conv → BN → ReLU → MaxPool |\n",
    "| Flatten | 14×14×64 | 12544 | Reshape to 1D |\n",
    "| FC1 | 12544 | 128 | Linear → ReLU → Dropout(0.5) |\n",
    "| FC2 | 128 | 2 | Linear (output logits) |\n",
    "\n",
    "**Code Walkthrough:**\n",
    "- **4 Conv blocks**: Each doubles the channels (1→8→16→32→64), halves spatial size with pooling\n",
    "- **Batch Normalization**: Stabilizes training by normalizing each layer's output\n",
    "- **Dropout (50%)**: Randomly drops neurons during training to prevent overfitting\n",
    "- **Output**: 2 classes (0=Healthy, 1=Tumor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary (Architecture Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 8, 224, 224]              80\n",
      "       BatchNorm2d-2          [-1, 8, 224, 224]              16\n",
      "         MaxPool2d-3          [-1, 8, 112, 112]               0\n",
      "            Conv2d-4         [-1, 16, 112, 112]           1,168\n",
      "       BatchNorm2d-5         [-1, 16, 112, 112]              32\n",
      "         MaxPool2d-6           [-1, 16, 56, 56]               0\n",
      "            Conv2d-7           [-1, 32, 56, 56]           4,640\n",
      "       BatchNorm2d-8           [-1, 32, 56, 56]              64\n",
      "         MaxPool2d-9           [-1, 32, 28, 28]               0\n",
      "           Conv2d-10           [-1, 64, 28, 28]          18,496\n",
      "      BatchNorm2d-11           [-1, 64, 28, 28]             128\n",
      "        MaxPool2d-12           [-1, 64, 14, 14]               0\n",
      "           Linear-13                  [-1, 128]       1,605,760\n",
      "          Dropout-14                  [-1, 128]               0\n",
      "           Linear-15                    [-1, 2]             258\n",
      "================================================================\n",
      "Total params: 1,630,642\n",
      "Trainable params: 1,630,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 12.92\n",
      "Params size (MB): 6.22\n",
      "Estimated Total Size (MB): 19.33\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(1, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary shows:**\n",
    "- Layer-by-layer architecture\n",
    "- Output shape at each layer\n",
    "- Number of parameters (weights and biases)\n",
    "- Total parameters and model size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "**Loss Function**: CrossEntropyLoss (for classification)  \n",
    "**Optimizer**: Adam with learning rate 0.001  \n",
    "**Scheduler**: ReduceLROnPlateau (reduces LR when validation loss plateaus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "    \n",
    "    # Calculate average validation loss\n",
    "    val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Print progress every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop Breakdown:**\n",
    "\n",
    "**Outer loop (epochs)**: Runs 30 times over the entire dataset\n",
    "\n",
    "**Training phase**:\n",
    "1. `model.train()`: Sets model to training mode (enables dropout, batch norm)\n",
    "2. `optimizer.zero_grad()`: Clears old gradients from previous iteration\n",
    "3. `outputs = model(images)`: Forward pass → computes predictions\n",
    "4. `loss = criterion(outputs, labels)`: Computes cross-entropy loss\n",
    "5. `loss.backward()`: Backpropagation → computes gradients\n",
    "6. `optimizer.step()`: Updates weights using gradients\n",
    "\n",
    "**Validation phase**:\n",
    "- `model.eval()`: Sets model to evaluation mode (disables dropout, batch norm)\n",
    "- `with torch.no_grad()`: Disables gradient calculation for faster computation\n",
    "- Computes validation loss to track model performance on unseen data\n",
    "\n",
    "**Print**: Shows training and validation loss every 5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate model performance using confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader):\n",
    "    \"\"\"Get all predictions and true labels\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "y_pred, y_true = get_predictions(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes):\n",
    "    \"\"\"Plot confusion matrix heatmap\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cm, train_set.classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Confusion Matrix:**\n",
    "- **Top-left (True Negatives)**: Correctly predicted as Healthy\n",
    "- **Top-right (False Positives)**: Incorrectly predicted as Tumor (false alarm)\n",
    "- **Bottom-left (False Negatives)**: Incorrectly predicted as Healthy (missed tumor)\n",
    "- **Bottom-right (True Positives)**: Correctly predicted as Tumor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=train_set.classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metrics Explained:**\n",
    "- **Precision**: Of all predicted tumors, how many are actually tumors? (TP / (TP + FP))\n",
    "- **Recall**: Of all actual tumors, how many did we detect? (TP / (TP + FN))\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced metric)\n",
    "- **Support**: Number of samples in each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions\n",
    "\n",
    "Show sample images with their predicted and actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of validation images\n",
    "dataiter = iter(val_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "    outputs = model(images)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "images = images.cpu()\n",
    "predictions = predictions.cpu()\n",
    "\n",
    "# Plot 8 sample images\n",
    "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(8):\n",
    "    # Convert from normalized tensor to image\n",
    "    img = images[i].squeeze()  # Remove channel dimension for grayscale\n",
    "    \n",
    "    # Denormalize: reverse the normalization (mean=0.5, std=0.5)\n",
    "    img = img * 0.5 + 0.5\n",
    "    \n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].set_title(f'True: {train_set.classes[labels[i]]}\\nPred: {train_set.classes[predictions[i]]}',\n",
    "                     color='green' if labels[i] == predictions[i] else 'red')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization shows:**\n",
    "- **Green titles**: Correct predictions\n",
    "- **Red titles**: Incorrect predictions (misclassifications)\n",
    "- Images are denormalized from [-1, 1] back to [0, 1] for proper display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'brain_tumor_cnn.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1343913,
     "sourceId": 2236708,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
