{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4d0ff1",
   "metadata": {},
   "source": [
    "# Image Captioning with TensorFlow/Keras\n",
    "\n",
    "## What is Image Captioning?\n",
    "\n",
    "**Goal:** Given an image, automatically generate a natural language description.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input:  [Photo of a dog playing with a ball]\n",
    "Output: \"a brown dog is playing with a red ball in the park\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "This tutorial uses a **CNN-RNN encoder-decoder architecture**:\n",
    "\n",
    "```\n",
    "Image ‚Üí [Xception CNN] ‚Üí Feature Vector (2048) ‚Üí [LSTM RNN] ‚Üí Caption\n",
    "  üì∑          üîç              üìä                    ‚úçÔ∏è           üìù\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **Xception CNN (Encoder)**: Extracts visual features from images (pre-trained on ImageNet)\n",
    "2. **LSTM RNN (Decoder)**: Generates caption word-by-word from image features\n",
    "3. **Embedding Layer**: Converts words to dense vectors\n",
    "4. **Merge Layer**: Combines image features with word embeddings\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset: Flickr8k\n",
    "\n",
    "**Structure:**\n",
    "- **8,000 images** with **5 captions each** = 40,000 image-caption pairs\n",
    "- **Caption format:** Each image has multiple captions in a text file\n",
    "\n",
    "**Example from `Flickr8k.token.txt`:**\n",
    "```\n",
    "1351764581_4d4fb1b40f.jpg#0    a fireman sprays water into the hood of a small white car\n",
    "1351764581_4d4fb1b40f.jpg#1    A fireman sprays inside the open hood of a small white car\n",
    "1351764581_4d4fb1b40f.jpg#2    A fireman uses a firehose on a car engine\n",
    "...\n",
    "```\n",
    "\n",
    "**Key difference from PyTorch version:**\n",
    "- PyTorch version: Simple CSV format (`image.jpg,caption text`)\n",
    "- TensorFlow version: Token format with `#0, #1, #2...` suffixes for multiple captions per image\n",
    "\n",
    "---\n",
    "\n",
    "## Complete Workflow\n",
    "\n",
    "```\n",
    "PHASE 1: DATA PREPARATION\n",
    "  ‚îú‚îÄ Load captions from Flickr8k.token.txt\n",
    "  ‚îú‚îÄ Clean text (lowercase, remove punctuation)\n",
    "  ‚îú‚îÄ Build vocabulary\n",
    "  ‚îî‚îÄ Save processed descriptions\n",
    "\n",
    "PHASE 2: FEATURE EXTRACTION\n",
    "  ‚îú‚îÄ Load Xception model (pre-trained)\n",
    "  ‚îú‚îÄ Extract 2048-dim features from all images\n",
    "  ‚îî‚îÄ Save features to pickle file\n",
    "\n",
    "PHASE 3: TOKENIZATION\n",
    "  ‚îú‚îÄ Create tokenizer (word ‚Üí number mapping)\n",
    "  ‚îú‚îÄ Calculate max caption length\n",
    "  ‚îî‚îÄ Save tokenizer\n",
    "\n",
    "PHASE 4: MODEL BUILDING\n",
    "  ‚îú‚îÄ Define CNN-RNN architecture\n",
    "  ‚îú‚îÄ Image features ‚Üí Dense(256)\n",
    "  ‚îú‚îÄ Word sequences ‚Üí Embedding(256) ‚Üí LSTM(256)\n",
    "  ‚îú‚îÄ Merge ‚Üí Dense ‚Üí Softmax\n",
    "  ‚îî‚îÄ Compile with categorical_crossentropy\n",
    "\n",
    "PHASE 5: TRAINING\n",
    "  ‚îú‚îÄ Create data generator (yields batches)\n",
    "  ‚îú‚îÄ Train for 10 epochs\n",
    "  ‚îî‚îÄ Save model checkpoints\n",
    "\n",
    "PHASE 6: INFERENCE\n",
    "  ‚îú‚îÄ Load trained model\n",
    "  ‚îú‚îÄ Extract features from new image\n",
    "  ‚îú‚îÄ Generate caption word-by-word\n",
    "  ‚îî‚îÄ Display result\n",
    "```\n",
    "\n",
    "Let's build each phase step by step!\n",
    "\n",
    "---\n",
    "\n",
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "340aa95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import os\n",
    "from pickle import dump, load\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bf32e",
   "metadata": {},
   "source": [
    "### Basic Libraries\n",
    "\n",
    "```python\n",
    "import string          # For text cleaning (remove punctuation)\n",
    "import numpy as np     # Numerical operations\n",
    "import os              # File system operations\n",
    "from pickle import dump, load  # Save/load Python objects\n",
    "import tensorflow as tf         # Deep learning framework\n",
    "import matplotlib.pyplot as plt # Visualization\n",
    "```\n",
    "\n",
    "**Purpose:** Essential utilities for data processing, model building, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb21f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PHASE 1: DATA PREPARATION\n",
    "\n",
    "This phase handles loading and preprocessing the Flickr8k dataset. The dataset has a unique token format where each image has 5 captions stored like this:\n",
    "\n",
    "```\n",
    "image.jpg#0    A dog runs across the grass\n",
    "image.jpg#1    A brown dog running in a field\n",
    "image.jpg#2    A dog playing outside\n",
    "...\n",
    "```\n",
    "\n",
    "**Workflow Steps:**\n",
    "1. Load token file ‚Üí Extract image names and captions\n",
    "2. Clean text ‚Üí Remove punctuation, lowercase, remove single chars\n",
    "3. Build vocabulary ‚Üí Create word-to-frequency dictionary\n",
    "4. Save to file ‚Üí Store processed captions for later use\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05c713",
   "metadata": {},
   "source": [
    "### TensorFlow & Keras Modules\n",
    "\n",
    "```python\n",
    "from PIL import Image                                      # Image loading\n",
    "from tensorflow.keras.applications.xception import Xception  # Pre-trained CNN\n",
    "from tensorflow.keras.applications.xception import preprocess_input  # Image preprocessing\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences    # Pad sequences to same length\n",
    "from tensorflow.keras.utils import to_categorical, plot_model        # One-hot encoding, model visualization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer            # Text to sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LSTM, Embedding, Dropout, add  # Model layers\n",
    ")\n",
    "from tensorflow.keras.models import Model  # Functional API\n",
    "```\n",
    "\n",
    "**Purpose:**\n",
    "- **Xception:** Pre-trained CNN for feature extraction (2048-dimensional vectors)\n",
    "- **Tokenizer:** Converts text captions to sequences of integers\n",
    "- **LSTM:** Recurrent layer for generating captions word-by-word\n",
    "- **Embedding:** Converts word indices to dense vectors\n",
    "\n",
    "**Why Xception?**\n",
    "- Efficient feature extraction (pre-trained on ImageNet)\n",
    "- Produces rich 2048-dimensional feature vectors\n",
    "- Faster than training CNN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100b9ad",
   "metadata": {},
   "source": [
    "### Function 1: `load_doc(filename)` - Load Text File\n",
    "\n",
    "```python\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "```\n",
    "\n",
    "**What it does:** Reads the entire content of a text file\n",
    "\n",
    "**Input:** `\"Flickr8k.token.txt\"` (contains all captions)\n",
    "\n",
    "**Output:** One large string with all lines\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input file content:\n",
    "    1000268201_693b08cb0e.jpg#0    A child in a pink dress is climbing up a set of stairs\n",
    "    1000268201_693b08cb0e.jpg#1    A girl going into a wooden building\n",
    "    \n",
    "Output: \"1000268201_693b08cb0e.jpg#0\\tA child in a pink dress...\\n1000268201_693b08cb0e.jpg#1\\tA girl going...\"\n",
    "```\n",
    "\n",
    "**Role in Workflow:** First step - loads raw caption data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29d3129",
   "metadata": {},
   "source": [
    "### Function 2: `all_img_captions(filename)` - Parse Token Format\n",
    "\n",
    "```python\n",
    "def all_img_captions(filename):\n",
    "    file = load_doc(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions = {}\n",
    "    \n",
    "    for caption in captions[:-1]:\n",
    "        img, caption_text = caption.split('\\t')\n",
    "        \n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [caption_text]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption_text)\n",
    "    \n",
    "    return descriptions\n",
    "```\n",
    "\n",
    "**What it does:** Converts token format to dictionary mapping\n",
    "\n",
    "**Key Logic:** `img[:-2]` removes the `#0`, `#1`, `#2` suffix to group captions by image\n",
    "\n",
    "**Data Transformation:**\n",
    "```\n",
    "INPUT (token format):\n",
    "    image.jpg#0    caption one\n",
    "    image.jpg#1    caption two\n",
    "    image.jpg#2    caption three\n",
    "\n",
    "OUTPUT (dictionary):\n",
    "    {\n",
    "        \"image.jpg\": [\n",
    "            \"caption one\",\n",
    "            \"caption two\", \n",
    "            \"caption three\"\n",
    "        ]\n",
    "    }\n",
    "```\n",
    "\n",
    "**Why This Matters:** Groups all 5 captions per image into a single entry\n",
    "\n",
    "**Role in Workflow:** Second step - structures raw text into usable dictionary format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1314bdf",
   "metadata": {},
   "source": [
    "### Function 3: `cleaning_text(descriptions)` - Text Preprocessing\n",
    "\n",
    "```python\n",
    "def cleaning_text(descriptions):\n",
    "    table = str.maketrans('', '', string.punctuation)  # Translation table for punctuation removal\n",
    "    \n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            desc = desc.split()                           # Split into words\n",
    "            desc = [word.lower() for word in desc]        # Lowercase\n",
    "            desc = [word.translate(table) for word in desc]  # Remove punctuation\n",
    "            desc = [word for word in desc if len(word) > 1]  # Remove single-character words\n",
    "            desc_list[i] = ' '.join(desc)\n",
    "    \n",
    "    return descriptions\n",
    "```\n",
    "\n",
    "**What it does:** Cleans and normalizes text for better model training\n",
    "\n",
    "**Cleaning Steps:**\n",
    "1. **Lowercase:** \"The Dog\" ‚Üí \"the dog\"\n",
    "2. **Remove punctuation:** \"dog!\" ‚Üí \"dog\"\n",
    "3. **Filter short words:** \"a\", \"I\" ‚Üí removed\n",
    "4. **Rejoin words:** [\"the\", \"dog\", \"runs\"] ‚Üí \"the dog runs\"\n",
    "\n",
    "**Example Transformation:**\n",
    "```\n",
    "BEFORE: \"A child in a pink dress is climbing up a set of stairs!\"\n",
    "AFTER:  \"child in pink dress is climbing up set of stairs\"\n",
    "\n",
    "BEFORE: \"The dog's running fast.\"\n",
    "AFTER:  \"the dogs running fast\"\n",
    "```\n",
    "\n",
    "**Why Clean Text?**\n",
    "- Reduces vocabulary size (fewer unique words)\n",
    "- Removes noise (punctuation doesn't help captioning)\n",
    "- Standardizes format (all lowercase)\n",
    "\n",
    "**Role in Workflow:** Third step - prepares text for tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8bc0db",
   "metadata": {},
   "source": [
    "### Function 4: `text_vocabulary(descriptions)` - Build Vocabulary Set\n",
    "\n",
    "```python\n",
    "def text_vocabulary(descriptions):\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "```\n",
    "\n",
    "**What it does:** Extracts all unique words from all captions\n",
    "\n",
    "**Data Flow:**\n",
    "```\n",
    "INPUT:\n",
    "    {\n",
    "        \"img1.jpg\": [\"dog runs fast\", \"brown dog running\"],\n",
    "        \"img2.jpg\": [\"cat sits quietly\"]\n",
    "    }\n",
    "\n",
    "PROCESSING:\n",
    "    img1.jpg captions ‚Üí [\"dog\", \"runs\", \"fast\", \"brown\", \"dog\", \"running\"]\n",
    "    img2.jpg captions ‚Üí [\"cat\", \"sits\", \"quietly\"]\n",
    "    \n",
    "OUTPUT (set):\n",
    "    {\"dog\", \"runs\", \"fast\", \"brown\", \"running\", \"cat\", \"sits\", \"quietly\"}\n",
    "```\n",
    "\n",
    "**Why Use a Set?**\n",
    "- Automatically removes duplicates\n",
    "- Fast lookup for checking if word exists\n",
    "- Gives vocabulary size: `len(vocabulary)`\n",
    "\n",
    "**Typical Vocabulary Size:** ~8,000 unique words in Flickr8k\n",
    "\n",
    "**Role in Workflow:** Fourth step - identifies all unique words needed for tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33261eb",
   "metadata": {},
   "source": [
    "### Function 5: `save_descriptions(descriptions, filename)` - Save Processed Data\n",
    "\n",
    "```python\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = []\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(f\"{key}\\t{desc}\")\n",
    "    \n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "```\n",
    "\n",
    "**What it does:** Saves cleaned captions to a file for later use\n",
    "\n",
    "**Output Format:** Simple tab-separated format (no more #0, #1, #2)\n",
    "\n",
    "**Example Output File (`descriptions.txt`):**\n",
    "```\n",
    "image1.jpg    child in pink dress is climbing up set of stairs\n",
    "image1.jpg    girl going into wooden building\n",
    "image1.jpg    little girl climbing into wooden playhouse\n",
    "image2.jpg    dog runs across the grass\n",
    "image2.jpg    brown dog running in field\n",
    "```\n",
    "\n",
    "**Why Save to File?**\n",
    "- Don't need to reprocess text every time\n",
    "- Can share cleaned data\n",
    "- Faster loading for training\n",
    "\n",
    "**Role in Workflow:** Final step of Phase 1 - persists cleaned data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7627e0",
   "metadata": {},
   "source": [
    "### Phase 1 Execution - Data Preparation Pipeline\n",
    "\n",
    "**This cell runs all Phase 1 functions in sequence:**\n",
    "\n",
    "```python\n",
    "# 1. Load raw token file\n",
    "descriptions = all_img_captions(\"Flickr8k.token.txt\")  \n",
    "# Output: {img: [cap1, cap2, cap3, cap4, cap5]} - 8,000 images\n",
    "\n",
    "# 2. Clean text\n",
    "clean_descriptions = cleaning_text(descriptions)\n",
    "# Output: Lowercase, no punctuation, no single chars\n",
    "\n",
    "# 3. Build vocabulary\n",
    "vocabulary = text_vocabulary(clean_descriptions)\n",
    "# Output: ~8,000 unique words\n",
    "\n",
    "# 4. Save to disk\n",
    "save_descriptions(clean_descriptions, \"descriptions.txt\")\n",
    "# Output: descriptions.txt file created\n",
    "```\n",
    "\n",
    "**Expected Output:**\n",
    "```\n",
    "Length of descriptions = 8091\n",
    "Length of vocabulary = 8763\n",
    "```\n",
    "\n",
    "**What You Get:**\n",
    "- `descriptions.txt` - cleaned captions ready for training\n",
    "- `vocabulary` - set of all unique words\n",
    "- Ready for Phase 2 (Feature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6427142",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PHASE 2: FEATURE EXTRACTION\n",
    "\n",
    "Now that text is prepared, we need to extract visual features from images using a pre-trained CNN (Xception).\n",
    "\n",
    "**Why Feature Extraction?**\n",
    "- Training a CNN from scratch is slow and requires huge data\n",
    "- Xception is pre-trained on ImageNet (1.4M images, 1000 classes)\n",
    "- We \"borrow\" its learned features (edges, textures, objects)\n",
    "\n",
    "**What Happens:**\n",
    "```\n",
    "Image (299x299x3) ‚Üí Xception CNN ‚Üí Feature Vector (2048)\n",
    "```\n",
    "\n",
    "Each image becomes a 2048-dimensional vector that captures its visual content.\n",
    "\n",
    "**Workflow:**\n",
    "1. Load Xception model (without top classification layer)\n",
    "2. Preprocess images to 299x299\n",
    "3. Extract features for all images\n",
    "4. Save features to pickle file\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc996809",
   "metadata": {},
   "source": [
    "### Phase 2 Execution - Feature Extraction with Xception\n",
    "\n",
    "**This cell downloads Xception weights and extracts features:**\n",
    "\n",
    "```python\n",
    "# 1. Download Xception weights (if not cached)\n",
    "weights_url = \"https://storage.googleapis.com/.../xception_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "weights_path = download_with_retry(weights_url, 'xception_weights.h5')\n",
    "\n",
    "# 2. Load Xception model\n",
    "model = Xception(include_top=False,    # Remove classification layer\n",
    "                 pooling='avg',         # Global average pooling\n",
    "                 weights=weights_path)  # Use downloaded weights\n",
    "\n",
    "# 3. Extract features from all images\n",
    "features = extract_features(dataset_images)\n",
    "# Output: {img_name: 2048-dim vector}\n",
    "\n",
    "# 4. Save features\n",
    "dump(features, open(\"features.p\", \"wb\"))\n",
    "```\n",
    "\n",
    "**Key Parameters:**\n",
    "- `include_top=False`: Removes final classification layer (we don't need 1000 ImageNet classes)\n",
    "- `pooling='avg'`: Adds global average pooling ‚Üí output shape (2048,)\n",
    "- `weights`: Pre-trained weights from ImageNet\n",
    "\n",
    "**What You Get:**\n",
    "- `features.p` - pickle file with all image features\n",
    "- Dictionary: `{image_name: numpy_array(2048)}`\n",
    "- Processing time: ~20-30 minutes for 8,000 images\n",
    "\n",
    "**Memory Note:** Features file is ~200MB (much smaller than raw images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a993442d",
   "metadata": {},
   "source": [
    "### Function 6: `extract_features(directory)` - CNN Feature Extraction\n",
    "\n",
    "```python\n",
    "def extract_features(directory):\n",
    "    features = {}\n",
    "    valid_images = ['.jpg', '.jpeg', '.png']\n",
    "    \n",
    "    for img in tqdm(os.listdir(directory)):\n",
    "        # Skip non-image files\n",
    "        ext = os.path.splitext(img)[1].lower()\n",
    "        if ext not in valid_images:\n",
    "            continue\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        filename = directory + \"/\" + img\n",
    "        image = Image.open(filename)\n",
    "        image = image.resize((299, 299))          # Xception input size\n",
    "        image = np.expand_dims(image, axis=0)     # Add batch dimension: (299,299,3) ‚Üí (1,299,299,3)\n",
    "        image = image / 127.5                      # Scale to [0, 2]\n",
    "        image = image - 1.0                        # Scale to [-1, 1]\n",
    "        \n",
    "        # Extract features using pre-trained model\n",
    "        feature = model.predict(image)             # Output: (1, 2048)\n",
    "        features[img] = feature\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "**What it does:** Converts images to feature vectors using Xception CNN\n",
    "\n",
    "**Image Preprocessing Steps:**\n",
    "1. **Resize:** Any size ‚Üí 299x299 (Xception requirement)\n",
    "2. **Add batch dimension:** (299,299,3) ‚Üí (1,299,299,3)\n",
    "3. **Normalize:** Pixel values [0,255] ‚Üí [-1,1]\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "INPUT: dog.jpg (640x480x3)\n",
    "‚Üì Resize\n",
    "(299x299x3)\n",
    "‚Üì Normalize\n",
    "(299x299x3) with values in [-1, 1]\n",
    "‚Üì Xception CNN\n",
    "(2048,) feature vector\n",
    "\n",
    "OUTPUT: features[\"dog.jpg\"] = array([0.234, -0.567, 0.891, ...])  # 2048 values\n",
    "```\n",
    "\n",
    "**Why [-1, 1] normalization?**\n",
    "- Xception was trained with this normalization\n",
    "- Matches training distribution = better features\n",
    "\n",
    "**Role in Workflow:** Converts raw images to dense feature vectors for the LSTM decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804816f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PHASE 3: TOKENIZATION & DATA LOADING\n",
    "\n",
    "Now we need to prepare the training data by:\n",
    "1. Loading train/test splits\n",
    "2. Adding `<start>` and `<end>` tokens to captions\n",
    "3. Creating a tokenizer (word ‚Üí integer mapping)\n",
    "4. Finding maximum caption length\n",
    "\n",
    "**Why Tokenization?**\n",
    "Neural networks work with numbers, not words. We need to convert:\n",
    "```\n",
    "\"dog runs fast\" ‚Üí [34, 156, 892]\n",
    "```\n",
    "\n",
    "**Special Tokens:**\n",
    "- `<start>`: Signals beginning of caption\n",
    "- `<end>`: Signals end of caption\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86973cd7",
   "metadata": {},
   "source": [
    "### Function 7: `load_photos(filename)` - Load Train/Test Split\n",
    "\n",
    "```python\n",
    "def load_photos(filename):\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"\\n\")[:-1]\n",
    "    photos_present = [photo for photo in photos if os.path.exists(os.path.join(dataset_images, photo))]\n",
    "    return photos_present\n",
    "```\n",
    "\n",
    "**What it does:** Loads list of image filenames for train/test split\n",
    "\n",
    "**Input:** `Flickr_8k.trainImages.txt` containing:\n",
    "```\n",
    "1000268201_693b08cb0e.jpg\n",
    "1001773457_577c3a7d70.jpg\n",
    "1002674143_1b742ab4b8.jpg\n",
    "...\n",
    "```\n",
    "\n",
    "**Output:** List of image filenames that exist in the dataset folder\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "train_imgs = load_photos(\"Flickr_8k.trainImages.txt\")\n",
    "# Output: ['1000268201_693b08cb0e.jpg', '1001773457_577c3a7d70.jpg', ...]\n",
    "# Length: ~6,000 images for training\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Separates training and testing images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b1c48",
   "metadata": {},
   "source": [
    "### Function 8: `load_clean_descriptions(filename, photos)` - Add Start/End Tokens\n",
    "\n",
    "```python\n",
    "def load_clean_descriptions(filename, photos): \n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    \n",
    "    for line in file.split(\"\\n\"):\n",
    "        words = line.split()\n",
    "        if len(words) < 1:\n",
    "            continue\n",
    "        \n",
    "        image, image_caption = words[0], words[1:]\n",
    "        \n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            \n",
    "            # Add special tokens\n",
    "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "    \n",
    "    return descriptions\n",
    "```\n",
    "\n",
    "**What it does:** Loads captions for specific images and adds start/end tokens\n",
    "\n",
    "**Caption Transformation:**\n",
    "```\n",
    "BEFORE: \"dog runs across the grass\"\n",
    "AFTER:  \"<start> dog runs across the grass <end>\"\n",
    "```\n",
    "\n",
    "**Why Add Tokens?**\n",
    "- `<start>`: Tells model when to begin generating\n",
    "- `<end>`: Tells model when to stop generating\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "# Output:\n",
    "{\n",
    "    \"dog.jpg\": [\n",
    "        \"<start> dog runs across the grass <end>\",\n",
    "        \"<start> brown dog running in field <end>\",\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Prepares captions for sequence-to-sequence training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b9e48",
   "metadata": {},
   "source": [
    "### Function 9: `load_features(photos)` - Load Pre-extracted Features\n",
    "\n",
    "```python\n",
    "def load_features(photos):\n",
    "    # Load all features\n",
    "    all_features = load(open(\"features.p\", \"rb\"))\n",
    "    # Select only needed features\n",
    "    features = {k: all_features[k] for k in photos}\n",
    "    return features\n",
    "```\n",
    "\n",
    "**What it does:** Loads only the features for training/test images\n",
    "\n",
    "**Why Filter?**\n",
    "- `features.p` contains features for ALL 8,000 images\n",
    "- We only need features for training set (~6,000) or test set (~2,000)\n",
    "- Saves memory\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# All features: 8,091 images\n",
    "all_features = load(open(\"features.p\", \"rb\"))\n",
    "\n",
    "# Filter for training set\n",
    "train_features = load_features(train_imgs)  # Only ~6,000 features\n",
    "```\n",
    "\n",
    "**Data Structure:**\n",
    "```python\n",
    "{\n",
    "    \"dog.jpg\": array([0.234, -0.567, ...]),  # Shape: (1, 2048)\n",
    "    \"cat.jpg\": array([0.891, 0.123, ...]),\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Loads pre-computed CNN features for efficient training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef8224",
   "metadata": {},
   "source": [
    "### Loading Training Data\n",
    "\n",
    "**This cell loads training split:**\n",
    "\n",
    "```python\n",
    "# 1. Load training image filenames\n",
    "train_imgs = load_photos(\"Flickr_8k.trainImages.txt\")\n",
    "# Output: List of ~6,000 image filenames\n",
    "\n",
    "# 2. Load captions with start/end tokens\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "# Output: {img: [\"<start> caption <end>\", ...]}\n",
    "\n",
    "# 3. Load pre-extracted CNN features\n",
    "train_features = load_features(train_imgs)\n",
    "# Output: {img: feature_vector(2048)}\n",
    "```\n",
    "\n",
    "**What You Get:**\n",
    "- `train_imgs`: List of training image names\n",
    "- `train_descriptions`: Dictionary of captions with special tokens\n",
    "- `train_features`: Dictionary of 2048-dim feature vectors\n",
    "\n",
    "**Ready for:** Tokenization and model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7741473",
   "metadata": {},
   "source": [
    "### Function 10: `dict_to_list(descriptions)` - Flatten Descriptions\n",
    "\n",
    "```python\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "```\n",
    "\n",
    "**What it does:** Converts dictionary of captions to a flat list\n",
    "\n",
    "**Transformation:**\n",
    "```\n",
    "INPUT (dictionary):\n",
    "{\n",
    "    \"img1.jpg\": [\"<start> dog runs <end>\", \"<start> brown dog <end>\"],\n",
    "    \"img2.jpg\": [\"<start> cat sits <end>\"]\n",
    "}\n",
    "\n",
    "OUTPUT (list):\n",
    "[\n",
    "    \"<start> dog runs <end>\",\n",
    "    \"<start> brown dog <end>\",\n",
    "    \"<start> cat sits <end>\"\n",
    "]\n",
    "```\n",
    "\n",
    "**Why Flatten?**\n",
    "- `Tokenizer.fit_on_texts()` expects a list of strings\n",
    "- We need all captions together to build the vocabulary\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "all_train_captions = dict_to_list(train_descriptions)\n",
    "# Output: List of ~30,000 captions (6,000 images √ó 5 captions each)\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Prepares data format for Keras Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3812312",
   "metadata": {},
   "source": [
    "### Creating Tokenizer - Word to Integer Mapping\n",
    "\n",
    "**This cell creates the tokenizer that converts words to numbers:**\n",
    "\n",
    "```python\n",
    "# 1. Flatten all captions to a list\n",
    "all_train_captions = dict_to_list(train_descriptions)\n",
    "# Output: [\"<start> dog runs <end>\", \"<start> cat sits <end>\", ...]\n",
    "\n",
    "# 2. Create tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_train_captions)\n",
    "# Learns vocabulary from all training captions\n",
    "\n",
    "# 3. Get vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "```\n",
    "\n",
    "**What the Tokenizer Does:**\n",
    "\n",
    "```python\n",
    "# Creates word_index dictionary:\n",
    "tokenizer.word_index = {\n",
    "    '<start>': 1,\n",
    "    '<end>': 2,\n",
    "    'dog': 3,\n",
    "    'runs': 4,\n",
    "    'cat': 5,\n",
    "    'sits': 6,\n",
    "    ...\n",
    "}\n",
    "\n",
    "# Can convert text to sequences:\n",
    "tokenizer.texts_to_sequences([\"dog runs\"])\n",
    "# Output: [[3, 4]]\n",
    "```\n",
    "\n",
    "**Why +1 for vocab_size?**\n",
    "- Index 0 is reserved for padding\n",
    "- Real vocabulary: indices 1 to vocab_size-1\n",
    "\n",
    "**Role in Workflow:** Creates word-to-integer mapping for model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b96fc",
   "metadata": {},
   "source": [
    "### Finding Maximum Caption Length\n",
    "\n",
    "**This cell calculates the longest caption length:**\n",
    "\n",
    "```python\n",
    "max_length = max(len(caption.split()) for caption in all_train_captions)\n",
    "```\n",
    "\n",
    "**Why Do We Need This?**\n",
    "- All sequences must be the same length for batch processing\n",
    "- Shorter captions will be padded to max_length\n",
    "- Longer captions cannot exceed max_length\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "captions = [\n",
    "    \"<start> dog runs <end>\",           # Length: 4\n",
    "    \"<start> brown dog running in field <end>\",  # Length: 7\n",
    "    \"<start> cat <end>\"                 # Length: 3\n",
    "]\n",
    "\n",
    "max_length = 7  # Longest caption\n",
    "\n",
    "# Padded sequences:\n",
    "[1, 3, 4, 2, 0, 0, 0]    # \"dog runs\" padded with 0s\n",
    "[1, 8, 3, 9, 10, 11, 2]  # \"brown dog running in field\"\n",
    "[1, 5, 2, 0, 0, 0, 0]    # \"cat\" padded with 0s\n",
    "```\n",
    "\n",
    "**Typical max_length:** ~34 words for Flickr8k\n",
    "\n",
    "**Role in Workflow:** Determines sequence padding length for model input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb57e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PHASE 4: MODEL ARCHITECTURE\n",
    "\n",
    "Time to build the image captioning model! We'll use an **Encoder-Decoder architecture**:\n",
    "\n",
    "**Architecture Overview:**\n",
    "\n",
    "```\n",
    "IMAGE FEATURES (2048)             CAPTION SEQUENCE\n",
    "       ‚Üì                                  ‚Üì\n",
    "   Dense(256)                    Embedding(vocab_size, 256)\n",
    "       ‚Üì                                  ‚Üì\n",
    "    Dropout                               ‚Üì\n",
    "       ‚Üì                                  ‚Üì\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí MERGE (add) ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚Üì\n",
    "                    LSTM(256)\n",
    "                         ‚Üì\n",
    "                    Dense(256)\n",
    "                         ‚Üì\n",
    "                Dense(vocab_size, softmax)\n",
    "                         ‚Üì\n",
    "                   NEXT WORD\n",
    "```\n",
    "\n",
    "**Two Input Branches:**\n",
    "1. **Image Encoder:** Dense layers to process CNN features\n",
    "2. **Text Decoder:** Embedding + LSTM to process caption sequences\n",
    "\n",
    "**They merge:** Combined features fed to LSTM for next-word prediction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a3bc1",
   "metadata": {},
   "source": [
    "### Function 11: `create_tokenizer(descriptions)` - Build Tokenizer\n",
    "\n",
    "```python\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)  # Flatten to list\n",
    "    tokenizer = Tokenizer()                  # Create tokenizer\n",
    "    tokenizer.fit_on_texts(desc_list)       # Learn vocabulary\n",
    "    return tokenizer\n",
    "```\n",
    "\n",
    "**What it does:** Wrapper function that creates and trains the tokenizer\n",
    "\n",
    "**Workflow:**\n",
    "```\n",
    "1. Flatten descriptions ‚Üí [\"<start> dog <end>\", \"<start> cat <end>\", ...]\n",
    "2. Create Tokenizer() object\n",
    "3. fit_on_texts() ‚Üí builds word_index dictionary\n",
    "4. Return trained tokenizer\n",
    "```\n",
    "\n",
    "**Example Usage:**\n",
    "```python\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))  # Save for later\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding index 0\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Creates reusable tokenizer object for converting text to sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee95cfba",
   "metadata": {},
   "source": [
    "### Function 12: `max_length(descriptions)` - Find Longest Caption\n",
    "\n",
    "```python\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "```\n",
    "\n",
    "**What it does:** Calculates the maximum caption length in the dataset\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "descriptions = {\n",
    "    \"img1.jpg\": [\"<start> dog runs <end>\", \"<start> brown dog <end>\"],\n",
    "    \"img2.jpg\": [\"<start> cat <end>\"]\n",
    "}\n",
    "\n",
    "max_length(descriptions)\n",
    "# Output: 4  (from \"<start> dog runs <end>\")\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Determines padding length for variable-length sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534c394",
   "metadata": {},
   "source": [
    "### Function 13: `create_sequences()` - Create Training Sequences\n",
    "\n",
    "This is the **MOST IMPORTANT** function - it creates input-output pairs for training!\n",
    "\n",
    "```python\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    \n",
    "    for desc in desc_list:\n",
    "        # Encode caption to integers\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        \n",
    "        # Create multiple training pairs from one caption\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]  # Partial caption, next word\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            \n",
    "            X1.append(feature)    # Image features\n",
    "            X2.append(in_seq)     # Partial caption\n",
    "            y.append(out_seq)     # Next word (one-hot)\n",
    "    \n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "```\n",
    "\n",
    "**How It Works - Example:**\n",
    "\n",
    "Given caption: `\"<start> dog runs <end>\"`\n",
    "\n",
    "**Step 1: Tokenize**\n",
    "```\n",
    "seq = [1, 23, 45, 2]  # <start>=1, dog=23, runs=45, <end>=2\n",
    "```\n",
    "\n",
    "**Step 2: Create Training Pairs**\n",
    "```\n",
    "i=1: in_seq=[1]         ‚Üí out_seq=23  (Given \"<start>\", predict \"dog\")\n",
    "i=2: in_seq=[1,23]      ‚Üí out_seq=45  (Given \"<start> dog\", predict \"runs\")\n",
    "i=3: in_seq=[1,23,45]   ‚Üí out_seq=2   (Given \"<start> dog runs\", predict \"<end>\")\n",
    "```\n",
    "\n",
    "**Step 3: Pad Sequences**\n",
    "```\n",
    "If max_length=10:\n",
    "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]         ‚Üí 23\n",
    "[1, 23, 0, 0, 0, 0, 0, 0, 0, 0]        ‚Üí 45\n",
    "[1, 23, 45, 0, 0, 0, 0, 0, 0, 0]       ‚Üí 2\n",
    "```\n",
    "\n",
    "**Final Output:**\n",
    "- `X1`: Image features (same for all 3 pairs)\n",
    "- `X2`: Padded caption sequences\n",
    "- `y`: Next word (one-hot encoded, size=vocab_size)\n",
    "\n",
    "**Why This Works:**\n",
    "- Model learns to predict next word given image + partial caption\n",
    "- One caption creates multiple training examples\n",
    "- At inference, we predict word-by-word using this same pattern\n",
    "\n",
    "**Role in Workflow:** Converts (image, caption) pairs into (image, partial_caption) ‚Üí next_word training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ca569",
   "metadata": {},
   "source": [
    "### Function 14: `data_generator()` - Batch Data Generator\n",
    "\n",
    "```python\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    def generator():\n",
    "        while True:  # Infinite loop for training\n",
    "            for key, description_list in descriptions.items():\n",
    "                feature = features[key][0]  # Get image features\n",
    "                \n",
    "                # Create sequences for this image\n",
    "                input_image, input_sequence, output_word = create_sequences(\n",
    "                    tokenizer, max_length, description_list, feature\n",
    "                )\n",
    "                \n",
    "                # Yield one sample at a time\n",
    "                for i in range(len(input_image)):\n",
    "                    yield {\n",
    "                        'input_1': input_image[i],      # Image features (2048)\n",
    "                        'input_2': input_sequence[i]    # Caption sequence (max_length)\n",
    "                    }, output_word[i]                    # Next word (vocab_size)\n",
    "    \n",
    "    # Define output shapes for TensorFlow\n",
    "    output_signature = (\n",
    "        {\n",
    "            'input_1': tf.TensorSpec(shape=(2048,), dtype=tf.float32),\n",
    "            'input_2': tf.TensorSpec(shape=(max_length,), dtype=tf.int32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    # Create TensorFlow dataset\n",
    "    dataset = tf.data.Dataset.from_generator(generator, output_signature=output_signature)\n",
    "    \n",
    "    return dataset.batch(32)  # Batch size 32\n",
    "```\n",
    "\n",
    "**What it does:** Creates a memory-efficient data pipeline for training\n",
    "\n",
    "**Why Use a Generator?**\n",
    "- Dataset is too large to fit in memory (6,000 images √ó 5 captions √ó 34 words = 1M+ samples)\n",
    "- Generates batches on-the-fly during training\n",
    "- Infinite loop ensures training never runs out of data\n",
    "\n",
    "**Output Format:**\n",
    "```python\n",
    "# Each batch contains:\n",
    "inputs = {\n",
    "    'input_1': (32, 2048),      # 32 image features\n",
    "    'input_2': (32, max_length) # 32 caption sequences\n",
    "}\n",
    "outputs = (32, vocab_size)      # 32 next-word predictions\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Provides batched training data to model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef79a533",
   "metadata": {},
   "source": [
    "### Function 15: `define_model()` - Build Caption Model Architecture\n",
    "\n",
    "```python\n",
    "def define_model(vocab_size, max_length):\n",
    "    # IMAGE ENCODER BRANCH\n",
    "    inputs1 = Input(shape=(2048,), name='input_1')  # Image features from Xception\n",
    "    fe1 = Dropout(0.5)(inputs1)                      # Dropout for regularization\n",
    "    fe2 = Dense(256, activation='relu')(fe1)         # Compress 2048 ‚Üí 256\n",
    "    \n",
    "    # TEXT DECODER BRANCH\n",
    "    inputs2 = Input(shape=(max_length,), name='input_2')  # Caption sequence\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)  # Word embeddings\n",
    "    se2 = Dropout(0.5)(se1)                                     # Dropout\n",
    "    se3 = LSTM(256)(se2)                                        # LSTM processes sequence ‚Üí 256\n",
    "    \n",
    "    # MERGE BRANCHES\n",
    "    decoder1 = add([fe2, se3])                       # Element-wise addition\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)  # Probability over words\n",
    "    \n",
    "    # BUILD MODEL\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "**Architecture Breakdown:**\n",
    "\n",
    "**1. Image Encoder (fe = feature encoder):**\n",
    "```\n",
    "(2048) ‚Üí Dropout(0.5) ‚Üí Dense(256) ‚Üí [256-dim vector]\n",
    "```\n",
    "\n",
    "**2. Text Decoder (se = sequence encoder):**\n",
    "```\n",
    "(max_length) ‚Üí Embedding(vocab_size, 256) ‚Üí Dropout(0.5) ‚Üí LSTM(256) ‚Üí [256-dim vector]\n",
    "```\n",
    "\n",
    "**3. Merge & Prediction:**\n",
    "```\n",
    "[Image 256] + [Text 256] ‚Üí Dense(256) ‚Üí Dense(vocab_size, softmax)\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "- **Embedding Layer:** Converts word indices to 256-dim vectors\n",
    "  - `mask_zero=True`: Ignores padding (index 0)\n",
    "  \n",
    "- **LSTM:** Processes sequence of word embeddings\n",
    "  - Captures context: \"dog\" means different things after \"brown\" vs \"hot\"\n",
    "  \n",
    "- **add([fe2, se3]):** Combines image and text features\n",
    "  - Both are 256-dim, so we can add them element-wise\n",
    "  \n",
    "- **Softmax Output:** Probability distribution over all words\n",
    "\n",
    "**Example Forward Pass:**\n",
    "```\n",
    "Image features: [0.23, -0.45, ..., 0.89]  (2048)\n",
    "Caption: \"<start> dog\"                     (tokenized, padded)\n",
    "\n",
    "‚Üì\n",
    "Image branch: [0.12, 0.89, ..., -0.34]    (256)\n",
    "Text branch:  [0.56, -0.23, ..., 0.78]    (256)\n",
    "\n",
    "‚Üì Merge (add)\n",
    "Combined: [0.68, 0.66, ..., 0.44]         (256)\n",
    "\n",
    "‚Üì Output\n",
    "Probabilities: [0.001, 0.003, ..., 0.24, ...]  (vocab_size)\n",
    "                                ‚Üë\n",
    "                            \"runs\" (highest probability)\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Defines the neural network architecture for image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd15575",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PHASE 5: TRAINING\n",
    "\n",
    "Now we train the model! This phase runs the training loop with our data generator.\n",
    "\n",
    "**Training Setup:**\n",
    "```python\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "\n",
    "dataset = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "model.fit(dataset, epochs=4, steps_per_epoch=steps, verbose=1)\n",
    "```\n",
    "\n",
    "**What Happens During Training:**\n",
    "\n",
    "Each step:\n",
    "1. Generator produces batch of (image_features, caption_sequence) ‚Üí next_word\n",
    "2. Model predicts next word probabilities\n",
    "3. Loss calculated: predicted vs actual next word\n",
    "4. Backpropagation updates weights\n",
    "5. Repeat for all batches\n",
    "\n",
    "**Training Progress:**\n",
    "- `steps_per_epoch`: Number of batches per epoch\n",
    "- Typical: ~2000-3000 steps/epoch for Flickr8k\n",
    "- Each epoch processes all training data once\n",
    "\n",
    "**Saving Models:**\n",
    "```python\n",
    "model.save(\"models2/model_0.h5\")  # After epoch 0\n",
    "model.save(\"models2/model_1.h5\")  # After epoch 1\n",
    "...\n",
    "```\n",
    "\n",
    "**Expected Training Time:** ~30-60 minutes per epoch on GPU\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517864c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PHASE 6: INFERENCE (GENERATING CAPTIONS)\n",
    "\n",
    "After training, we can generate captions for new images! This phase uses the trained model to predict captions word-by-word.\n",
    "\n",
    "**Inference Workflow:**\n",
    "```\n",
    "1. Load new image\n",
    "2. Extract features using Xception\n",
    "3. Start with \"<start>\" token\n",
    "4. Predict next word\n",
    "5. Append word to sequence\n",
    "6. Repeat until \"<end>\" or max_length\n",
    "```\n",
    "\n",
    "**Key Difference from Training:**\n",
    "- Training: Given full caption, predict each next word\n",
    "- Inference: Start with \"<start>\", generate word-by-word\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a2369",
   "metadata": {},
   "source": [
    "### Function 16: `word_for_id()` - Convert Index to Word\n",
    "\n",
    "```python\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "```\n",
    "\n",
    "**What it does:** Reverse lookup - converts word index back to word string\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Tokenizer word_index:\n",
    "{\n",
    "    '<start>': 1,\n",
    "    '<end>': 2,\n",
    "    'dog': 3,\n",
    "    'runs': 4\n",
    "}\n",
    "\n",
    "word_for_id(3, tokenizer)\n",
    "# Output: 'dog'\n",
    "\n",
    "word_for_id(999, tokenizer)\n",
    "# Output: None  (not in vocabulary)\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Converts model's integer predictions back to readable words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b593061",
   "metadata": {},
   "source": [
    "### Function 17: `generate_desc()` - Generate Caption for Image\n",
    "\n",
    "This is the **INFERENCE FUNCTION** - generates captions word-by-word!\n",
    "\n",
    "```python\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'start'  # Start with \"start\" token\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        # 1. Tokenize current caption\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        \n",
    "        # 2. Pad to max_length\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        \n",
    "        # 3. Predict next word\n",
    "        pred = model.predict([photo, sequence], verbose=0)\n",
    "        pred = np.argmax(pred)  # Get highest probability word index\n",
    "        \n",
    "        # 4. Convert index to word\n",
    "        word = word_for_id(pred, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        # 5. Append word to caption\n",
    "        in_text += ' ' + word\n",
    "        \n",
    "        # 6. Stop if we predict \"end\"\n",
    "        if word == 'end':\n",
    "            break\n",
    "    \n",
    "    return in_text\n",
    "```\n",
    "\n",
    "**How It Works - Step by Step:**\n",
    "\n",
    "**Given:** Image of a dog running\n",
    "\n",
    "**Iteration 1:**\n",
    "```\n",
    "in_text = \"start\"\n",
    "sequence = [1, 0, 0, ..., 0]  (padded to max_length)\n",
    "Prediction: [0.001, 0.002, 0.245, ...]  ‚Üí argmax = 23 ‚Üí \"dog\"\n",
    "in_text = \"start dog\"\n",
    "```\n",
    "\n",
    "**Iteration 2:**\n",
    "```\n",
    "in_text = \"start dog\"\n",
    "sequence = [1, 23, 0, ..., 0]\n",
    "Prediction: [0.003, 0.001, 0.189, ...]  ‚Üí argmax = 45 ‚Üí \"runs\"\n",
    "in_text = \"start dog runs\"\n",
    "```\n",
    "\n",
    "**Iteration 3:**\n",
    "```\n",
    "in_text = \"start dog runs\"\n",
    "sequence = [1, 23, 45, 0, ..., 0]\n",
    "Prediction: [0.001, 0.678, 0.002, ...]  ‚Üí argmax = 2 ‚Üí \"end\"\n",
    "in_text = \"start dog runs end\"\n",
    "STOP (word == \"end\")\n",
    "```\n",
    "\n",
    "**Final Output:** `\"start dog runs end\"`\n",
    "\n",
    "**Key Points:**\n",
    "- Autoregressive: Each prediction depends on previous predictions\n",
    "- Greedy search: Always picks highest probability word (not beam search)\n",
    "- Stops at \"end\" token or max_length\n",
    "\n",
    "**Example Usage:**\n",
    "```python\n",
    "# Load trained model\n",
    "model = load_model('models2/model_9.h5')\n",
    "\n",
    "# Extract features from new image\n",
    "photo = extract_features('dog.jpg', xception_model)\n",
    "\n",
    "# Generate caption\n",
    "caption = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(caption)\n",
    "# Output: \"start dog runs across the grass end\"\n",
    "```\n",
    "\n",
    "**Role in Workflow:** Core inference function that generates captions for new images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb27239",
   "metadata": {},
   "source": [
    "### Complete Inference Pipeline\n",
    "\n",
    "**This cell shows the full inference workflow:**\n",
    "\n",
    "```python\n",
    "# 1. Load pre-trained Xception for feature extraction\n",
    "xception_model = Xception(include_top=False, pooling='avg')\n",
    "\n",
    "# 2. Load trained caption model\n",
    "model = load_model('models2/model_9.h5')\n",
    "\n",
    "# 3. Load tokenizer\n",
    "tokenizer = load(open('tokenizer.p', 'rb'))\n",
    "\n",
    "# 4. Load new image and extract features\n",
    "img_path = 'test_image.jpg'\n",
    "photo = extract_features(img_path, xception_model)\n",
    "\n",
    "# 5. Generate caption\n",
    "caption = generate_desc(model, tokenizer, photo, max_length)\n",
    "\n",
    "# 6. Clean up (remove start/end tokens)\n",
    "caption = caption.replace('start', '').replace('end', '').strip()\n",
    "print(\"Caption:\", caption)\n",
    "```\n",
    "\n",
    "**Example Outputs:**\n",
    "```\n",
    "Image: dog_running.jpg\n",
    "Caption: \"dog runs across the grass\"\n",
    "\n",
    "Image: child_playing.jpg\n",
    "Caption: \"child in pink dress is climbing up set of stairs\"\n",
    "\n",
    "Image: beach_sunset.jpg\n",
    "Caption: \"person standing on beach at sunset\"\n",
    "```\n",
    "\n",
    "**Tips for Better Captions:**\n",
    "- Use model from later epochs (model_9.h5 > model_0.h5)\n",
    "- Images similar to training data work best\n",
    "- Model may struggle with unusual objects not in Flickr8k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f848483",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SUMMARY: Complete Workflow\n",
    "\n",
    "**PHASE 1: Data Preparation**\n",
    "- `load_doc()` ‚Üí Load token file\n",
    "- `all_img_captions()` ‚Üí Parse token format, group by image\n",
    "- `cleaning_text()` ‚Üí Remove punctuation, lowercase\n",
    "- `text_vocabulary()` ‚Üí Build word set\n",
    "- `save_descriptions()` ‚Üí Save cleaned captions\n",
    "\n",
    "**PHASE 2: Feature Extraction**\n",
    "- `Xception` ‚Üí Load pre-trained CNN\n",
    "- `extract_features()` ‚Üí Convert images to 2048-dim vectors\n",
    "- Save to `features.p`\n",
    "\n",
    "**PHASE 3: Tokenization**\n",
    "- `load_photos()` ‚Üí Load train/test split\n",
    "- `load_clean_descriptions()` ‚Üí Add `<start>` and `<end>` tokens\n",
    "- `load_features()` ‚Üí Filter features for train/test\n",
    "- `dict_to_list()` ‚Üí Flatten captions\n",
    "- `create_tokenizer()` ‚Üí Build word-to-index mapping\n",
    "- `max_length()` ‚Üí Find longest caption\n",
    "\n",
    "**PHASE 4: Model Building**\n",
    "- `define_model()` ‚Üí Build encoder-decoder architecture\n",
    "  - Image Encoder: Dense(256)\n",
    "  - Text Decoder: Embedding + LSTM(256)\n",
    "  - Merge: add() + softmax\n",
    "\n",
    "**PHASE 5: Training**\n",
    "- `create_sequences()` ‚Üí Create (image, partial_caption) ‚Üí next_word pairs\n",
    "- `data_generator()` ‚Üí Batch data generator\n",
    "- `model.fit()` ‚Üí Train the model\n",
    "\n",
    "**PHASE 6: Inference**\n",
    "- `extract_features()` ‚Üí Get features for new image\n",
    "- `word_for_id()` ‚Üí Convert index to word\n",
    "- `generate_desc()` ‚Üí Generate caption word-by-word\n",
    "\n",
    "**Key Differences from PyTorch:**\n",
    "- Token format: `image.jpg#0` (TensorFlow) vs `image.jpg,caption` (PyTorch)\n",
    "- Data loading: `tf.data.Dataset` generator vs PyTorch DataLoader\n",
    "- Model: Functional API vs Sequential/nn.Module\n",
    "- Training: `model.fit()` vs manual training loop\n",
    "\n",
    "**Dataset:** Flickr8k - 8,000 images, 5 captions each, ~8,700 unique words\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aa95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Model,load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, get_file\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd67de2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    \n",
    "    file=open(filename, 'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6e81c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_img_captions(filename):\n",
    "    file =load_doc(filename)\n",
    "    captions=file.split('\\n')\n",
    "    descriptions={}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption_text=caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]]=[caption_text]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption_text)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "540ec5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_text(descriptions):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc=desc_list[i]\n",
    "            desc=desc.split()\n",
    "            desc=[word.lower() for word in desc]\n",
    "            desc=[word.translate(table) for word in desc]\n",
    "            desc=[word for word in desc if len(word)>1]\n",
    "            desc_list[i]=' '.join(desc)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce7656b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_vocabulary(descriptions):\n",
    "    all_desc=set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947167e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions, filename):\n",
    "    lines=[]\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(f\"{key}\\t{desc}\")\n",
    "    data='\\n'.join(lines)\n",
    "    file=open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099a8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Set these path according to project folder in you system\n",
    "dataset_text = \"/Users/sreemanti/Documents/youtube/youtube-teach/image caption generator/Flickr8k_text\"\n",
    "dataset_images = \"/Users/sreemanti/Documents/youtube/youtube-teach/image caption generator/Flicker8k_Dataset\"\n",
    "\n",
    "#we prepare our text data\n",
    "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
    "#loading the file that contains all data\n",
    "#mapping them into descriptions dictionary img to 5 captions\n",
    "descriptions = all_img_captions(filename)\n",
    "print(\"Length of descriptions =\" ,len(descriptions))\n",
    "\n",
    "#cleaning the descriptions\n",
    "clean_descriptions = cleaning_text(descriptions)\n",
    "\n",
    "#building vocabulary \n",
    "vocabulary = text_vocabulary(clean_descriptions)\n",
    "print(\"Length of vocabulary = \", len(vocabulary))\n",
    "\n",
    "#saving each description to file \n",
    "save_descriptions(clean_descriptions, \"descriptions.txt\")\n",
    "\n",
    "def download_with_retry(url, filename, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return get_file(filename, url)\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise e\n",
    "            print(f\"Download attempt {attempt + 1} failed. Retrying in 5 seconds...\")\n",
    "            time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the Xception model initialization with:\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "weights_url = \"https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "weights_path = download_with_retry(weights_url, 'xception_weights.h5')\n",
    "model = Xception(include_top=False, pooling='avg', weights=weights_path)\n",
    "\n",
    "def extract_features(directory):\n",
    "    features = {}\n",
    "    valid_images = ['.jpg', '.jpeg', '.png']  # Add other formats if needed\n",
    "    \n",
    "    for img in tqdm(os.listdir(directory)):\n",
    "        # Skip files that don't end with valid image extensions\n",
    "        ext = os.path.splitext(img)[1].lower()\n",
    "        if ext not in valid_images:\n",
    "            continue\n",
    "            \n",
    "        filename = directory + \"/\" + img\n",
    "        image = Image.open(filename)\n",
    "        image = image.resize((299,299))\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "\n",
    "        feature = model.predict(image)\n",
    "        features[img] = feature\n",
    "    return features\n",
    "\n",
    "# 2048 feature vector\n",
    "features = extract_features(dataset_images)\n",
    "dump(features, open(\"features.p\",\"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = load(open(\"features.p\",\"rb\"))\n",
    "\n",
    "#load the data \n",
    "def load_photos(filename):\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"\\n\")[:-1]\n",
    "    photos_present = [photo for photo in photos if os.path.exists(os.path.join(dataset_images, photo))]\n",
    "    return photos_present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb5220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_clean_descriptions(filename, photos): \n",
    "    #loading clean_descriptions\n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"\\n\"):\n",
    "\n",
    "        words = line.split()\n",
    "        if len(words)<1 :\n",
    "            continue\n",
    "\n",
    "        image, image_caption = words[0], words[1:]\n",
    "\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
    "            descriptions[image].append(desc)\n",
    "\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffaffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_features(photos):\n",
    "    #loading all features\n",
    "    all_features = load(open(\"features.p\",\"rb\"))\n",
    "    #selecting only needed features\n",
    "    features = {k:all_features[k] for k in photos}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "\n",
    "#train = loading_data(filename)\n",
    "train_imgs = load_photos(filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#converting dictionary to clean list of descriptions\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dde79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating tokenizer class \n",
    "#this will vectorise text corpus\n",
    "#each integer will represent token in dictionary\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# give each word an index, and store that into tokenizer.p pickle file\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847624f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#calculate maximum length of descriptions\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "    \n",
    "max_length = max_length(train_descriptions)\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdb7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create input-output sequence pairs from the image description.\n",
    "\n",
    "#data generator, used by model.fit()\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    def generator():\n",
    "        while True:\n",
    "            for key, description_list in descriptions.items():\n",
    "                feature = features[key][0]\n",
    "                input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "                for i in range(len(input_image)):\n",
    "                    yield {'input_1': input_image[i], 'input_2': input_sequence[i]}, output_word[i]\n",
    "    \n",
    "    # Define the output signature for the generator\n",
    "    output_signature = (\n",
    "        {\n",
    "            'input_1': tf.TensorSpec(shape=(2048,), dtype=tf.float32),\n",
    "            'input_2': tf.TensorSpec(shape=(max_length,), dtype=tf.int32)\n",
    "        },\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)\n",
    "    )\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    return dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a659d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4138408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#You can check the shape of the input and output for your model\n",
    "dataset = data_generator(train_descriptions, features, tokenizer, max_length)\n",
    "for (a, b) in dataset.take(1):\n",
    "    print(a['input_1'].shape, a['input_2'].shape, b.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fefb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,), name='input_1')\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,), name='input_2')\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train our model\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d673c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "\n",
    "def get_steps_per_epoch(train_descriptions):\n",
    "    total_sequences = 0\n",
    "    for img_captions in train_descriptions.values():\n",
    "        for caption in img_captions:\n",
    "            words = caption.split()\n",
    "            total_sequences += len(words) - 1\n",
    "    # Ensure at least 1 step, even if sequences < batch_size\n",
    "    return max(1, total_sequences // 32)\n",
    "\n",
    "# Update training loop\n",
    "steps = get_steps_per_epoch(train_descriptions)\n",
    "\n",
    "# making a directory models to save our models\n",
    "os.mkdir(\"models2\")\n",
    "for i in range(epochs):\n",
    "    dataset = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "    model.fit(dataset, epochs=4, steps_per_epoch=steps, verbose=1)\n",
    "    model.save(\"models2/model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('-i', '--image', required=True, help=\"Image Path\")\n",
    "args = vars(ap.parse_args())\n",
    "img_path = args['image']\n",
    "\n",
    "def extract_features(filename, model):\n",
    "        try:\n",
    "            image = Image.open(filename)\n",
    "            \n",
    "        except:\n",
    "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
    "        image = image.resize((299,299))\n",
    "        image = np.array(image)\n",
    "        # for images that has 4 channels, we convert them into 3 channels\n",
    "        if image.shape[2] == 4: \n",
    "            image = image[..., :3]\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "        feature = model.predict(image)\n",
    "        return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ae196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    " for word, index in tokenizer.word_index.items():\n",
    "     if index == integer:\n",
    "         return word\n",
    " return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613009ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'start'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        pred = model.predict([photo,sequence], verbose=0)\n",
    "        pred = np.argmax(pred)\n",
    "        word = word_for_id(pred, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'end':\n",
    "            break\n",
    "    return in_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af669bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "\n",
    "    # features from the CNN model squeezed from 2048 to 256 nodes\n",
    "    inputs1 = Input(shape=(2048,), name='input_1')\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "    # LSTM sequence model\n",
    "    inputs2 = Input(shape=(max_length,), name='input_2')\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # Merging both models\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#path = 'Flicker8k_Dataset/111537222_07e56d5a30.jpg'\n",
    "max_length = 32\n",
    "tokenizer = load(open(\"tokenizer.p\",\"rb\"))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# First define the model architecture\n",
    "model = define_model(vocab_size, max_length)\n",
    "# Then load the weights\n",
    "model.load_weights('models/model_9.h5')\n",
    "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
    "\n",
    "photo = extract_features(img_path, xception_model)\n",
    "img = Image.open(img_path)\n",
    "\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(\"\\n\\n\")\n",
    "print(description)\n",
    "plt.imshow(img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
