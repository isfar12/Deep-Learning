{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef97e276",
   "metadata": {},
   "source": [
    "# üì∑ Image Captioning with CNN + RNN\n",
    "\n",
    "## üéØ What is Image Captioning?\n",
    "\n",
    "**Simple Definition:** Given an image, generate a natural language description.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input:  üñºÔ∏è [Photo of a dog playing with a ball in a park]\n",
    "Output: \"A brown dog is playing with a red ball in the park\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† The Big Idea: Two-Part System\n",
    "\n",
    "### üçï Simple Analogy: Restaurant Review System\n",
    "\n",
    "Imagine you're writing a restaurant review:\n",
    "\n",
    "**Part 1: LOOK at the food** üëÄ\n",
    "- Take a photo of the dish\n",
    "- Your eyes analyze: colors, shapes, ingredients\n",
    "- **This is the CNN Encoder** ‚Üí extracts visual features\n",
    "\n",
    "**Part 2: WRITE the review** ‚úçÔ∏è\n",
    "- Your brain converts visual information into words\n",
    "- Generates sentence word by word: \"This ‚Üí delicious ‚Üí pizza ‚Üí has ‚Üí fresh ‚Üí toppings\"\n",
    "- **This is the RNN Decoder** ‚Üí generates text sequence\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Overview\n",
    "\n",
    "```\n",
    "Image ‚Üí [CNN Encoder] ‚Üí Feature Vector ‚Üí [RNN Decoder] ‚Üí Caption\n",
    "  üñºÔ∏è         üîç              üìä              ‚úçÔ∏è            üìù\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **EncoderCNN**: Pre-trained Inception model (extracts image features)\n",
    "2. **DecoderRNN**: LSTM network (generates caption word by word)\n",
    "3. **CNNtoRNN**: Combines both parts\n",
    "\n",
    "Let's build each component!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e46743",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"asset/image_caption_overview.png\" alt=\"Architecture of Image Caption\" width=\"800\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab538168",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Image Captioning Architecture\n",
    "\n",
    "This diagram shows the complete pipeline:\n",
    "1. **Input Image** ‚Üí processed by CNN encoder\n",
    "2. **Feature Vector** ‚Üí extracted visual features\n",
    "3. **LSTM Decoder** ‚Üí generates words one by one\n",
    "4. **Output Caption** ‚Üí final sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45724ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a386c47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Step 1: Import Core Libraries\n",
    "\n",
    "### üîß What Each Library Does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7aa48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self,embed_size,train_CNN=False):\n",
    "        super(EncoderCNN,self).__init__()\n",
    "        \n",
    "        self.train_CNN=train_CNN\n",
    "        self.inception=models.inception_v3(pretrained=True,aux_logits=False)\n",
    "        self.inception.fc=nn.Linear(self.inception.fc.in_features,embed_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self,images):\n",
    "        features=self.inception(images)\n",
    "        \n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = self.train_CNN\n",
    "        return self.dropout(self.relu(features))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eb363",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch                    # Main PyTorch library\n",
    "import torch.nn as nn          # Neural network modules\n",
    "import torchvision.models as models  # Pre-trained models (Inception)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Step 2: Build CNN Encoder (Vision Part)\n",
    "\n",
    "### üçï Simple Analogy: Professional Food Photographer\n",
    "\n",
    "You hire a professional photographer to analyze dishes:\n",
    "- **Pre-trained eyes** (Inception model trained on ImageNet)\n",
    "- Converts photo into a **feature summary** (embed_size numbers)\n",
    "- Example: [0.5, 0.8, 0.2, ...] ‚Üí represents \"brown dog, green grass, blue sky\"\n",
    "\n",
    "### üîß Technical Explanation: EncoderCNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9612f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        \n",
    "        self.embed=nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm=nn.LSTM(embed_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.linear=nn.Linear(hidden_size,vocab_size)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, features,captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs=self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241aae9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Deep Dive: How Image Features Become \"Words\" for LSTM\n",
    "\n",
    "### üçï Simple Analogy: Restaurant Critic's First Impression\n",
    "\n",
    "Imagine a food critic:\n",
    "1. **Photographer shows photo** ‚Üí critic forms first impression (\"Ah, this looks like Italian food\")\n",
    "2. **Critic writes review** ‚Üí \"This\" ‚Üí \"delicious\" ‚Üí \"pizza\" ‚Üí \"has\" ‚Üí \"fresh\" ‚Üí \"toppings\"\n",
    "\n",
    "The photo analysis becomes the **starting point** for the review!\n",
    "\n",
    "### üîß Technical Explanation: Image Features as First \"Word\"\n",
    "\n",
    "In the decoder's forward pass:\n",
    "\n",
    "```python\n",
    "# 1. Image features from encoder\n",
    "features.shape = (batch_size, embed_size)\n",
    "# Example: (8, 256) - 8 images, each is 256 numbers\n",
    "\n",
    "# 2. Caption words converted to embeddings\n",
    "embeddings = self.embed(captions)\n",
    "# Example: (8, 10, 256) - 8 captions, 10 words each, 256-dim vectors\n",
    "\n",
    "# 3. THE MAGIC: Add sequence dimension to features\n",
    "features_with_seq = features.unsqueeze(1)\n",
    "# Shape changes: (8, 256) ‚Üí (8, 1, 256)\n",
    "# Now it looks like \"one word\" per image!\n",
    "\n",
    "# 4. Concatenate: Image feature becomes the first \"word\"\n",
    "embeddings = torch.cat((features_with_seq, embeddings), dim=1)\n",
    "# Result: (8, 11, 256) = 8 captions, 11 words (1 image + 10 real words), 256-dim\n",
    "```\n",
    "\n",
    "### üìä Visual Breakdown\n",
    "\n",
    "**Before concatenation:**\n",
    "```python\n",
    "Image features:  [0.5, 0.8, 0.2, ..., 0.7]  # 256 numbers (batch_size, 256)\n",
    "Caption embeddings:\n",
    "  Word 1 \"A\":    [0.1, 0.3, 0.5, ..., 0.2]  # 256 numbers\n",
    "  Word 2 \"dog\":  [0.7, 0.2, 0.1, ..., 0.9]  # 256 numbers\n",
    "  Word 3 \"is\":   [0.3, 0.6, 0.4, ..., 0.1]  # 256 numbers\n",
    "  ...\n",
    "```\n",
    "\n",
    "**After concatenation (what LSTM sees):**\n",
    "```python\n",
    "Sequence fed to LSTM:\n",
    "  Position 0 (image):  [0.5, 0.8, 0.2, ..., 0.7]  ‚Üê Image features\n",
    "  Position 1 \"A\":      [0.1, 0.3, 0.5, ..., 0.2]  ‚Üê Word embedding\n",
    "  Position 2 \"dog\":    [0.7, 0.2, 0.1, ..., 0.9]  ‚Üê Word embedding\n",
    "  Position 3 \"is\":     [0.3, 0.6, 0.4, ..., 0.1]  ‚Üê Word embedding\n",
    "  ...\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Image features and word embeddings are **both 256-dimensional vectors**\n",
    "- LSTM doesn't care if position 0 is an \"image\" or a \"word\"\n",
    "- It treats the image feature vector as the **context** for generating the caption\n",
    "\n",
    "### üéØ Complete Example\n",
    "\n",
    "**Input:**\n",
    "```python\n",
    "# Single image of a dog\n",
    "features = encoder(dog_image)  # Output: (1, 256)\n",
    "\n",
    "# Caption: \"A brown dog\"\n",
    "caption_indices = [1, 4, 45, 6]  # <SOS>=1, \"a\"=4, \"brown\"=45, \"dog\"=6\n",
    "```\n",
    "\n",
    "**Processing:**\n",
    "```python\n",
    "# 1. Embed caption words\n",
    "embeddings = self.embed(caption_indices)\n",
    "# Shape: (1, 4, 256) - 1 caption, 4 words, 256-dim each\n",
    "\n",
    "# 2. Add image as first position\n",
    "features_unsqueezed = features.unsqueeze(1)  # (1, 256) ‚Üí (1, 1, 256)\n",
    "embeddings = torch.cat((features_unsqueezed, embeddings), dim=1)\n",
    "# Shape: (1, 5, 256) - 1 caption, 5 positions (1 image + 4 words)\n",
    "\n",
    "# 3. LSTM sees this sequence:\n",
    "#    [Image_vector, <SOS>, \"a\", \"brown\", \"dog\"]\n",
    "#    \n",
    "# 4. LSTM generates predictions:\n",
    "#    Position 0 (after seeing image) ‚Üí predict <SOS>\n",
    "#    Position 1 (after seeing image + <SOS>) ‚Üí predict \"a\"\n",
    "#    Position 2 (after seeing image + <SOS> + \"a\") ‚Üí predict \"brown\"\n",
    "#    Position 3 (after seeing image + <SOS> + \"a\" + \"brown\") ‚Üí predict \"dog\"\n",
    "#    Position 4 (after seeing full context) ‚Üí predict <EOS>\n",
    "```\n",
    "\n",
    "### üí° Key Insight\n",
    "\n",
    "The image feature vector acts as **visual context** that influences every word generation:\n",
    "- **Without image context**: LSTM might generate random captions\n",
    "- **With image context**: LSTM generates captions that describe what's actually in the image!\n",
    "\n",
    "Think of it like this:\n",
    "```\n",
    "Image features = \"There's a brown furry animal in grass\"\n",
    "LSTM uses this context to generate: \"A brown dog is playing in the park\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74773a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        \"\"\"\n",
    "        CNN Encoder using pre-trained Inception v3\n",
    "        \n",
    "        Args:\n",
    "            embed_size: Size of feature vector (e.g., 256)\n",
    "            train_CNN: Whether to fine-tune CNN weights (usually False)\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        self.train_CNN = train_CNN\n",
    "        \n",
    "        # Load pre-trained Inception v3\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        \n",
    "        # Replace final layer to output embed_size features\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Convert images to feature vectors\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of images (batch_size, 3, 299, 299)\n",
    "            \n",
    "        Returns:\n",
    "            features: Feature vectors (batch_size, embed_size)\n",
    "        \"\"\"\n",
    "        features = self.inception(images)\n",
    "        \n",
    "        # Freeze/unfreeze CNN weights\n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True  # Always train final layer\n",
    "            else:\n",
    "                param.requires_grad = self.train_CNN  # Freeze other layers\n",
    "                \n",
    "        return self.dropout(self.relu(features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc2e52",
   "metadata": {},
   "source": [
    "### üìä What Happens Here\n",
    "\n",
    "**1. Load Pre-trained Inception**\n",
    "```python\n",
    "self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "```\n",
    "- **Pre-trained**: Already knows how to recognize objects (trained on ImageNet)\n",
    "- **aux_logits=False**: Disables auxiliary classifier (we don't need it)\n",
    "\n",
    "**2. Replace Final Layer**\n",
    "```python\n",
    "self.inception.fc = nn.Linear(in_features, embed_size)\n",
    "```\n",
    "- Original Inception outputs 1000 classes (ImageNet)\n",
    "- We replace it to output `embed_size` features (e.g., 256)\n",
    "- These features represent the image content\n",
    "\n",
    "**3. Freeze Layers**\n",
    "```python\n",
    "param.requires_grad = self.train_CNN\n",
    "```\n",
    "- **If `train_CNN=False`**: Freeze all layers (don't update weights)\n",
    "  - Why? Pre-trained features are already good!\n",
    "  - Faster training, less memory\n",
    "- **If `train_CNN=True`**: Fine-tune the entire network\n",
    "  - Slower but potentially better accuracy\n",
    "\n",
    "**4. Output**\n",
    "```python\n",
    "return self.dropout(self.relu(features))\n",
    "```\n",
    "- Apply ReLU activation\n",
    "- Apply dropout (50% chance to zero out neurons ‚Üí prevents overfitting)\n",
    "\n",
    "### üéØ Example Flow\n",
    "\n",
    "```python\n",
    "# Input: Batch of 8 images, 3 channels (RGB), 299√ó299 pixels\n",
    "images.shape = (8, 3, 299, 299)\n",
    "\n",
    "# Pass through encoder\n",
    "features = encoder(images)\n",
    "\n",
    "# Output: Feature vectors\n",
    "features.shape = (8, 256)  # 8 images, each represented by 256 numbers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è Step 3: Build RNN Decoder (Language Part)\n",
    "\n",
    "### üçï Simple Analogy: Food Critic Writing Reviews\n",
    "\n",
    "The food critic has two jobs:\n",
    "1. **Understand the photo analysis** (feature vector from CNN)\n",
    "2. **Write review word by word**: \"This\" ‚Üí \"delicious\" ‚Üí \"pizza\" ‚Üí \"has\" ‚Üí \"fresh\" ‚Üí \"toppings\"\n",
    "\n",
    "### üîß Technical Explanation: DecoderRNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e61e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, train_CNN=False):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def caption(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(image).unsqueeze(0)\n",
    "            states = None\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoder.lstm(x, states)\n",
    "                output = self.decoder.linear(hiddens.squeeze(1))\n",
    "                predicted = output.argmax(1)\n",
    "                \n",
    "                result_caption.append(predicted.item())\n",
    "                \n",
    "                x = self.decoder.embed(predicted).unsqueeze(1)\n",
    "                \n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "                    \n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608237e9",
   "metadata": {},
   "source": [
    "```python\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        \"\"\"\n",
    "        RNN Decoder using LSTM\n",
    "        \n",
    "        Args:\n",
    "            embed_size: Size of word embeddings (matches CNN output)\n",
    "            hidden_size: Size of LSTM hidden state\n",
    "            vocab_size: Number of words in vocabulary\n",
    "            num_layers: Number of LSTM layers\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # Word embedding layer (converts word indices to vectors)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer (generates sequences)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Output layer (converts LSTM output to vocabulary probabilities)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Generate caption predictions during training\n",
    "        \n",
    "        Args:\n",
    "            features: Image features from encoder (batch_size, embed_size)\n",
    "            captions: Ground truth captions (batch_size, caption_length)\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Predicted word probabilities (batch_size, caption_length, vocab_size)\n",
    "        \"\"\"\n",
    "        # 1. Convert caption words to embeddings\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        # Shape: (batch_size, caption_length, embed_size)\n",
    "        \n",
    "        # 2. Concatenate image features with caption embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        # Shape: (batch_size, caption_length+1, embed_size)\n",
    "        # Image features act as the first \"word\"\n",
    "        \n",
    "        # 3. Pass through LSTM\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        # Shape: (batch_size, caption_length+1, hidden_size)\n",
    "        \n",
    "        # 4. Convert to vocabulary predictions\n",
    "        outputs = self.linear(hiddens)\n",
    "        # Shape: (batch_size, caption_length+1, vocab_size)\n",
    "        \n",
    "        return outputs\n",
    "```\n",
    "\n",
    "### üìä Breaking Down Each Component\n",
    "\n",
    "**1. Word Embedding Layer**\n",
    "```python\n",
    "self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "```\n",
    "- **Purpose**: Convert word indices to dense vectors\n",
    "- **Example**:\n",
    "  ```python\n",
    "  vocab_size = 10000  # 10,000 words in vocabulary\n",
    "  embed_size = 256\n",
    "  \n",
    "  # Word \"dog\" has index 452\n",
    "  word_vector = embed(452)  # Shape: (256,)\n",
    "  # Output: [0.2, 0.8, -0.5, ..., 0.3]  # 256 numbers representing \"dog\"\n",
    "  ```\n",
    "\n",
    "**2. LSTM Layer**\n",
    "```python\n",
    "self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "```\n",
    "- **Purpose**: Generate sequences by remembering context\n",
    "- **batch_first=True**: Input shape is `(batch_size, sequence_length, embed_size)`\n",
    "\n",
    "**3. Linear Layer**\n",
    "```python\n",
    "self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "```\n",
    "- **Purpose**: Convert LSTM output to word predictions\n",
    "- **Output**: Probability for each word in vocabulary\n",
    "\n",
    "### üéØ Example Flow (Training Time)\n",
    "\n",
    "**Input:**\n",
    "```python\n",
    "features.shape = (8, 256)         # 8 images, 256 features each\n",
    "captions.shape = (8, 10)          # 8 captions, 10 words each (indices)\n",
    "```\n",
    "\n",
    "**Step-by-step:**\n",
    "```python\n",
    "# 1. Embed captions\n",
    "embeddings = self.embed(captions)  # (8, 10, 256)\n",
    "\n",
    "# 2. Add image features as first \"word\"\n",
    "embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "# Shape: (8, 11, 256)  # 11 = 1 image + 10 words\n",
    "\n",
    "# 3. Pass through LSTM\n",
    "hiddens, _ = self.lstm(embeddings)  # (8, 11, hidden_size)\n",
    "\n",
    "# 4. Convert to predictions\n",
    "outputs = self.linear(hiddens)  # (8, 11, 10000)\n",
    "# For each of 8 captions, for each of 11 positions, predict probability of each of 10000 words\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Step 4: Combine CNN + RNN (Complete Model)\n",
    "\n",
    "### üçï Simple Analogy: Restaurant Review System\n",
    "\n",
    "Now we connect both parts:\n",
    "1. **Photographer** analyzes the dish ‚Üí gives summary to critic\n",
    "2. **Critic** uses summary ‚Üí writes review word by word\n",
    "\n",
    "### üîß Technical Explanation: CNNtoRNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c20b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import spacy\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} #iots: index to string\n",
    "        self.stoi = {v: k for k, v in self.itos.items()} #stoi: string to index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    #        \"I love dogs\" -> ['i', 'love', 'dogs']\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] +=1\n",
    "                \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx +=1\n",
    "                    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb5ae4",
   "metadata": {},
   "source": [
    "```python\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import spacy\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "```\n",
    "\n",
    "### üì¶ Imports Explained\n",
    "\n",
    "**`spacy`**: Natural language processing library\n",
    "- Helps tokenize sentences (split into words)\n",
    "- Handles punctuation, contractions, etc.\n",
    "- **Example**: \"I'm running!\" ‚Üí [\"i\", \"'m\", \"running\", \"!\"]\n",
    "\n",
    "**`spacy_eng`**: English language model\n",
    "- Pre-trained on English text\n",
    "- Knows how to properly split English sentences\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Vocabulary Class: Complete Breakdown\n",
    "\n",
    "### Part 1: Initialization (`__init__`)\n",
    "\n",
    "```python\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Setting up the restaurant menu system\n",
    "\n",
    "**Line-by-line:**\n",
    "\n",
    "1. **`self.freq_threshold = freq_threshold`**\n",
    "   - **Stores**: Minimum word frequency to include\n",
    "   - **Example**: If `freq_threshold=5`, only words appearing 5+ times get a number\n",
    "   - **Why?**: Rare words (appearing 1-2 times) are noise ‚Üí replace with `<UNK>`\n",
    "\n",
    "2. **`self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}`**\n",
    "   - **itos** = \"index to string\" (number ‚Üí word)\n",
    "   - **Returns**: Dictionary mapping numbers to special tokens\n",
    "   - **Example**: `itos[0]` ‚Üí `\"<PAD>\"`\n",
    "   - **Special tokens**:\n",
    "     - `<PAD>` (0): Padding for short captions\n",
    "     - `<SOS>` (1): Start of sentence\n",
    "     - `<EOS>` (2): End of sentence\n",
    "     - `<UNK>` (3): Unknown word (not in vocabulary)\n",
    "\n",
    "3. **`self.stoi = {v: k for k, v in self.itos.items()}`**\n",
    "   - **stoi** = \"string to index\" (word ‚Üí number)\n",
    "   - **Returns**: Reverse dictionary (inverts `itos`)\n",
    "   - **Example**: `stoi[\"<PAD>\"]` ‚Üí `0`\n",
    "   - **How it works**:\n",
    "     ```python\n",
    "     # itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "     # Dictionary comprehension swaps keys and values:\n",
    "     # k (key) = 0, v (value) = \"<PAD>\"\n",
    "     # New: v (becomes key) = \"<PAD>\", k (becomes value) = 0\n",
    "     # Result: {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "     ```\n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Length Method (`__len__`)\n",
    "\n",
    "```python\n",
    "def __len__(self):\n",
    "    return len(self.itos)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Count how many dishes are on the menu\n",
    "\n",
    "**What it does:**\n",
    "- **Returns**: Total vocabulary size (number of unique words)\n",
    "- **Example**: After building vocab, `len(vocab)` might return `5000`\n",
    "- **Why needed?**: Neural network needs to know output size (how many possible words)\n",
    "\n",
    "**When called:**\n",
    "```python\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build_vocabulary(captions)\n",
    "print(len(vocab))  # Output: 5000 (4 special tokens + 4996 words)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Part 3: Tokenizer (Static Method)\n",
    "\n",
    "```python\n",
    "@staticmethod\n",
    "def tokenizer(text):\n",
    "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Recipe slicer that cuts instructions into steps\n",
    "\n",
    "**Line-by-line:**\n",
    "\n",
    "1. **`@staticmethod`**\n",
    "   - **What it means**: Method doesn't need `self` (doesn't use instance data)\n",
    "   - **Why use it?**: Tokenization is a utility function (doesn't depend on `freq_threshold`, `itos`, etc.)\n",
    "   - **Benefit**: Can call without creating instance: `Vocabulary.tokenizer(\"hello\")`\n",
    "   - **Comparison**:\n",
    "     ```python\n",
    "     # Regular method (needs instance):\n",
    "     vocab = Vocabulary(5)\n",
    "     vocab.tokenizer(\"hello\")  # Needs vocab object\n",
    "     \n",
    "     # Static method (no instance needed):\n",
    "     Vocabulary.tokenizer(\"hello\")  # Works directly!\n",
    "     ```\n",
    "\n",
    "2. **`spacy_eng.tokenizer(text)`**\n",
    "   - **Input**: String like `\"I love dogs!\"`\n",
    "   - **Returns**: Spacy tokens (special objects with word info)\n",
    "   - **Example**: `[Token(\"I\"), Token(\"love\"), Token(\"dogs\"), Token(\"!\")]`\n",
    "\n",
    "3. **`tok.text.lower()`**\n",
    "   - **tok.text**: Extracts actual text from spacy token\n",
    "   - **.lower()**: Converts to lowercase\n",
    "   - **Why lowercase?**: \"Dog\" and \"dog\" should be same word\n",
    "\n",
    "4. **`[... for tok in ...]`**\n",
    "   - **List comprehension**: Creates list of lowercase strings\n",
    "   - **Returns**: `[\"i\", \"love\", \"dogs\", \"!\"]`\n",
    "\n",
    "**Complete Example:**\n",
    "```python\n",
    "text = \"I love Dogs!\"\n",
    "tokens = Vocabulary.tokenizer(text)\n",
    "# Returns: [\"i\", \"love\", \"dogs\", \"!\"]\n",
    "\n",
    "# Without static method, would need:\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "tokens = vocab.tokenizer(text)  # Extra step!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Part 4: Build Vocabulary\n",
    "\n",
    "```python\n",
    "def build_vocabulary(self, sentence_list):\n",
    "    frequencies = {}\n",
    "    idx = 4\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        for word in self.tokenizer(sentence):\n",
    "            if word not in frequencies:\n",
    "                frequencies[word] = 1\n",
    "            else:\n",
    "                frequencies[word] += 1\n",
    "            \n",
    "            if frequencies[word] == self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Build menu by counting popular orders\n",
    "\n",
    "**Line-by-line:**\n",
    "\n",
    "1. **`frequencies = {}`**\n",
    "   - **Creates**: Empty dictionary to count word occurrences\n",
    "   - **Will become**: `{\"dog\": 10, \"cat\": 5, \"running\": 3, ...}`\n",
    "\n",
    "2. **`idx = 4`**\n",
    "   - **Starts**: At index 4 (0-3 reserved for special tokens)\n",
    "   - **Why 4?**: We already used 0=PAD, 1=SOS, 2=EOS, 3=UNK\n",
    "\n",
    "3. **`for sentence in sentence_list:`**\n",
    "   - **Loops**: Through all captions in dataset\n",
    "   - **Example**: `[\"A dog running\", \"A cat sleeping\", ...]`\n",
    "\n",
    "4. **`for word in self.tokenizer(sentence):`**\n",
    "   - **Tokenizes**: Each sentence into words\n",
    "   - **Example**: \"A dog running\" ‚Üí `[\"a\", \"dog\", \"running\"]`\n",
    "\n",
    "5. **`if word not in frequencies:`**\n",
    "   - **First occurrence**: Initialize count to 1\n",
    "   - **Example**: First time seeing \"dog\" ‚Üí `frequencies[\"dog\"] = 1`\n",
    "\n",
    "6. **`else: frequencies[word] += 1`**\n",
    "   - **Subsequent occurrences**: Increment count\n",
    "   - **Example**: Second time seeing \"dog\" ‚Üí `frequencies[\"dog\"] = 2`\n",
    "\n",
    "7. **`if frequencies[word] == self.freq_threshold:`**\n",
    "   - **Threshold reached**: Word appears enough times to include\n",
    "   - **Example**: If `freq_threshold=5` and \"dog\" appears 5th time ‚Üí add to vocab\n",
    "   - **Why \"==\"?**: Only add once (when count equals threshold)\n",
    "\n",
    "8. **`self.stoi[word] = idx`** and **`self.itos[idx] = word`**\n",
    "   - **Adds word**: To both dictionaries\n",
    "   - **Example**: `stoi[\"dog\"] = 4`, `itos[4] = \"dog\"`\n",
    "\n",
    "9. **`idx += 1`**\n",
    "   - **Increments**: Index for next word\n",
    "   - **Next word**: Will get index 5, then 6, etc.\n",
    "\n",
    "**Complete Example:**\n",
    "```python\n",
    "sentences = [\n",
    "    \"A dog running\",\n",
    "    \"A cat sleeping\", \n",
    "    \"A dog jumping\",\n",
    "    \"A dog eating\",\n",
    "    \"A dog playing\",  # \"dog\" appears 4 times now\n",
    "    \"A dog barking\"   # 5th time! Add to vocab\n",
    "]\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build_vocabulary(sentences)\n",
    "\n",
    "# After building:\n",
    "# frequencies = {\"a\": 6, \"dog\": 5, \"running\": 1, \"cat\": 1, ...}\n",
    "# Only \"a\" and \"dog\" reach threshold of 5\n",
    "# stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3, \"a\": 4, \"dog\": 5}\n",
    "# itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\", 4: \"a\", 5: \"dog\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Part 5: Numericalize\n",
    "\n",
    "```python\n",
    "def numericalize(self, text):\n",
    "    tokenized_text = self.tokenizer(text)\n",
    "    return [\n",
    "        self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "        for token in tokenized_text\n",
    "    ]\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Convert customer's order to kitchen codes\n",
    "\n",
    "**Line-by-line:**\n",
    "\n",
    "1. **`tokenized_text = self.tokenizer(text)`**\n",
    "   - **Splits**: Sentence into words\n",
    "   - **Returns**: List of lowercase words\n",
    "   - **Example**: \"A dog running\" ‚Üí `[\"a\", \"dog\", \"running\"]`\n",
    "\n",
    "2. **`self.stoi.get(token, self.stoi[\"<UNK>\"])`**\n",
    "   - **Looks up**: Word in dictionary\n",
    "   - **Returns**: Index if found, otherwise `<UNK>` index (3)\n",
    "   - **`.get(key, default)`**: Safe dictionary lookup\n",
    "     - If key exists ‚Üí return value\n",
    "     - If key missing ‚Üí return default\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     stoi = {\"dog\": 4, \"cat\": 5}\n",
    "     stoi.get(\"dog\", 3)      # Returns: 4 (found)\n",
    "     stoi.get(\"elephant\", 3)  # Returns: 3 (not found, use default)\n",
    "     ```\n",
    "\n",
    "3. **`[... for token in tokenized_text]`**\n",
    "   - **List comprehension**: Converts each word to number\n",
    "   - **Returns**: List of indices\n",
    "\n",
    "**Complete Example:**\n",
    "```python\n",
    "# Vocabulary built with:\n",
    "# stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3, \"a\": 4, \"dog\": 5, \"cat\": 6}\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build_vocabulary(captions)\n",
    "\n",
    "# Known words\n",
    "caption1 = \"A dog\"\n",
    "numbers1 = vocab.numericalize(caption1)\n",
    "# tokenized: [\"a\", \"dog\"]\n",
    "# numericalized: [4, 5]\n",
    "\n",
    "# Unknown word\n",
    "caption2 = \"A elephant\"\n",
    "numbers2 = vocab.numericalize(caption2)\n",
    "# tokenized: [\"a\", \"elephant\"]\n",
    "# \"elephant\" not in vocab ‚Üí use <UNK>\n",
    "# numericalized: [4, 3]  (3 = <UNK>)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why This Design?\n",
    "\n",
    "### ‚úÖ Separate Dictionaries (itos and stoi)\n",
    "**Why not just one?**\n",
    "```python\n",
    "# During training: Need string ‚Üí index\n",
    "word_idx = vocab.stoi[\"dog\"]  # Quick: O(1) lookup\n",
    "\n",
    "# During inference: Need index ‚Üí string\n",
    "word = vocab.itos[5]  # Quick: O(1) lookup\n",
    "\n",
    "# If only had one dictionary:\n",
    "# Would need to search values (slow: O(n))\n",
    "```\n",
    "\n",
    "### ‚úÖ Static Method for Tokenizer\n",
    "**Why not regular method?**\n",
    "```python\n",
    "# Static method benefits:\n",
    "1. Can call without instance: Vocabulary.tokenizer(\"text\")\n",
    "2. Shows it doesn't depend on object state\n",
    "3. Can be reused elsewhere in code\n",
    "4. More memory efficient (no self reference needed)\n",
    "```\n",
    "\n",
    "### ‚úÖ Frequency Threshold\n",
    "**Why not include all words?**\n",
    "```python\n",
    "# Without threshold (include all):\n",
    "vocab_size = 50,000  # Too large!\n",
    "- Slow training (huge output layer)\n",
    "- Memory problems\n",
    "- Rare words are noise (typos, names)\n",
    "\n",
    "# With threshold = 5:\n",
    "vocab_size = 5,000  # Manageable!\n",
    "- Faster training\n",
    "- Better generalization\n",
    "- Rare words ‚Üí <UNK> (acceptable loss)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Complete Workflow Example\n",
    "\n",
    "```python\n",
    "# 1. CREATE VOCABULARY\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "\n",
    "# 2. BUILD FROM CAPTIONS\n",
    "captions = [\n",
    "    \"A dog running in park\",\n",
    "    \"A cat sleeping on sofa\",\n",
    "    \"A dog jumping in park\",\n",
    "    \"A dog playing in park\",\n",
    "    \"A dog eating in park\",  # \"dog\" appears 4 times\n",
    "    \"A dog barking\"          # 5th time! Threshold reached\n",
    "]\n",
    "vocab.build_vocabulary(captions)\n",
    "\n",
    "# After building:\n",
    "# vocab.stoi = {\n",
    "#     \"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3,\n",
    "#     \"a\": 4, \"in\": 5, \"park\": 6, \"dog\": 7\n",
    "# }\n",
    "# (Words like \"running\", \"cat\" only appear 1-2 times ‚Üí not included)\n",
    "\n",
    "# 3. NUMERICALIZE NEW CAPTION\n",
    "new_caption = \"A dog running\"\n",
    "numbers = vocab.numericalize(new_caption)\n",
    "# Output: [4, 7, 3]  (\"a\"=4, \"dog\"=7, \"running\"=3 (UNK))\n",
    "\n",
    "# 4. REVERSE (numbers ‚Üí words)\n",
    "words = [vocab.itos[idx] for idx in numbers]\n",
    "# Output: [\"a\", \"dog\", \"<UNK>\"]\n",
    "\n",
    "# 5. USE IN TRAINING\n",
    "full_sequence = [vocab.stoi[\"<SOS>\"]] + numbers + [vocab.stoi[\"<EOS>\"]]\n",
    "# Output: [1, 4, 7, 3, 2]  (<SOS>, \"a\", \"dog\", <UNK>, <EOS>)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways: Vocabulary Class\n",
    "\n",
    "### ‚úÖ Purpose\n",
    "- Converts text ‚Üî numbers for neural networks\n",
    "- Handles unknown words gracefully\n",
    "- Reduces vocabulary size with frequency threshold\n",
    "\n",
    "### ‚úÖ Two-Way Mapping\n",
    "- **stoi**: String to index (training time)\n",
    "- **itos**: Index to string (inference time)\n",
    "\n",
    "### ‚úÖ Static Method\n",
    "- Tokenizer doesn't need instance data\n",
    "- Can call independently\n",
    "- More flexible and efficient\n",
    "\n",
    "### ‚úÖ Special Tokens\n",
    "- `<PAD>`: Fill short sequences\n",
    "- `<SOS>`: Mark sentence start\n",
    "- `<EOS>`: Mark sentence end\n",
    "- `<UNK>`: Replace rare/unknown words\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e4838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, train_CNN=False):\n",
    "        \"\"\"\n",
    "        Complete Image Captioning Model\n",
    "        \n",
    "        Args:\n",
    "            embed_size: Feature vector size (CNN output = RNN input)\n",
    "            hidden_size: LSTM hidden state size\n",
    "            vocab_size: Number of words in vocabulary\n",
    "            num_layers: Number of LSTM layers\n",
    "            train_CNN: Whether to fine-tune CNN\n",
    "        \"\"\"\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        \"\"\"\n",
    "        Training forward pass\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of images\n",
    "            captions: Ground truth captions\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Predicted word probabilities\n",
    "        \"\"\"\n",
    "        features = self.encoder(images)      # Image ‚Üí features\n",
    "        outputs = self.decoder(features, captions)  # Features ‚Üí caption\n",
    "        return outputs\n",
    "    \n",
    "    def caption(self, image, vocabulary, max_length=50):\n",
    "        \"\"\"\n",
    "        Generate caption for a single image (inference time)\n",
    "        \n",
    "        Args:\n",
    "            image: Single image tensor (1, 3, 299, 299)\n",
    "            vocabulary: Vocabulary object (for word lookups)\n",
    "            max_length: Maximum caption length\n",
    "            \n",
    "        Returns:\n",
    "            result_caption: List of words\n",
    "        \"\"\"\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():  # No gradient computation (inference only)\n",
    "            # 1. Get image features\n",
    "            x = self.encoder(image).unsqueeze(0)  # (1, 1, embed_size)\n",
    "            states = None  # Initial LSTM state\n",
    "            \n",
    "            # 2. Generate words one by one\n",
    "            for _ in range(max_length):\n",
    "                # Pass through LSTM\n",
    "                hiddens, states = self.decoder.lstm(x, states)\n",
    "                \n",
    "                # Predict next word\n",
    "                output = self.decoder.linear(hiddens.squeeze(1))\n",
    "                predicted = output.argmax(1)  # Get word with highest probability\n",
    "                \n",
    "                # Add to caption\n",
    "                result_caption.append(predicted.item())\n",
    "                \n",
    "                # Use predicted word as input for next step\n",
    "                x = self.decoder.embed(predicted).unsqueeze(1)\n",
    "                \n",
    "                # Stop if we predict <EOS> (end of sentence)\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "                    \n",
    "        # Convert indices to words\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bd949",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Deep Dive: CNNtoRNN Class - Line by Line\n",
    "\n",
    "### üçï Simple Analogy: Complete Restaurant Review System\n",
    "\n",
    "You hire:\n",
    "1. **Professional photographer** (EncoderCNN) ‚Üí analyzes food visually\n",
    "2. **Food critic** (DecoderRNN) ‚Üí writes reviews\n",
    "3. **Manager** (CNNtoRNN) ‚Üí coordinates both\n",
    "\n",
    "The manager tells photographer to analyze photo, then gives analysis to critic to write review.\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è CNNtoRNN Class: Complete Breakdown\n",
    "\n",
    "### Part 1: Initialization (`__init__`)\n",
    "\n",
    "```python\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, train_CNN=False):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "```\n",
    "\n",
    "**Line-by-line:**\n",
    "\n",
    "1. **`class CNNtoRNN(nn.Module):`**\n",
    "   - **Inherits**: From PyTorch's `nn.Module` (base class for all neural networks)\n",
    "   - **Why?**: Gets all functionality (forward pass, parameter tracking, GPU support)\n",
    "\n",
    "2. **`super(CNNtoRNN, self).__init__()`**\n",
    "   - **Calls**: Parent class initializer\n",
    "   - **Why needed?**: Properly sets up PyTorch module\n",
    "   - **Returns**: Nothing (just setup)\n",
    "\n",
    "3. **`self.encoder = EncoderCNN(embed_size, train_CNN)`**\n",
    "   - **Creates**: CNN encoder instance\n",
    "   - **Stores as**: Instance variable (accessible via `self.encoder`)\n",
    "   - **Parameters**:\n",
    "     - `embed_size=256`: Output 256-dimensional vectors\n",
    "     - `train_CNN=False`: Freeze Inception weights (use pre-trained)\n",
    "   - **Returns**: EncoderCNN object (Inception v3 network)\n",
    "\n",
    "4. **`self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)`**\n",
    "   - **Creates**: RNN decoder instance\n",
    "   - **Stores as**: Instance variable (accessible via `self.decoder`)\n",
    "   - **Parameters**:\n",
    "     - `embed_size=256`: Input dimension (matches encoder output)\n",
    "     - `hidden_size=256`: LSTM memory size\n",
    "     - `vocab_size=5000`: Number of possible words\n",
    "     - `num_layers=1`: Single LSTM layer\n",
    "   - **Returns**: DecoderRNN object (LSTM + linear layers)\n",
    "\n",
    "**Why separate encoder and decoder?**\n",
    "```python\n",
    "# Modularity benefits:\n",
    "1. Can train encoder separately\n",
    "2. Can swap encoders (ResNet, VGG, etc.)\n",
    "3. Can swap decoders (GRU, Transformer, etc.)\n",
    "4. Easier to debug (test each part independently)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Forward Pass (Training)\n",
    "\n",
    "```python\n",
    "def forward(self, images, captions):\n",
    "    features = self.encoder(images)\n",
    "    outputs = self.decoder(features, captions)\n",
    "    return outputs\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Process one batch of orders\n",
    "\n",
    "**Line-by-line:**\n",
    "\n",
    "1. **`def forward(self, images, captions):`**\n",
    "   - **Input**:\n",
    "     - `images`: Batch of photos (32, 3, 224, 224)\n",
    "     - `captions`: Ground truth captions (max_len, 32)\n",
    "   - **Called**: Automatically when you do `model(images, captions)`\n",
    "\n",
    "2. **`features = self.encoder(images)`**\n",
    "   - **Calls**: `EncoderCNN.forward(images)`\n",
    "   - **Process**: Images ‚Üí Inception network ‚Üí feature vectors\n",
    "   - **Returns**: Tensor of shape (32, 256)\n",
    "     - 32 images\n",
    "     - Each represented by 256 numbers\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     images.shape = (32, 3, 224, 224)  # Input\n",
    "     features = self.encoder(images)\n",
    "     features.shape = (32, 256)  # Output\n",
    "     ```\n",
    "\n",
    "3. **`outputs = self.decoder(features, captions)`**\n",
    "   - **Calls**: `DecoderRNN.forward(features, captions)`\n",
    "   - **Process**: Features + captions ‚Üí LSTM ‚Üí predictions\n",
    "   - **Returns**: Tensor of shape (32, max_len, 5000)\n",
    "     - 32 captions\n",
    "     - max_len words per caption\n",
    "     - 5000 probabilities per word (one for each vocabulary word)\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     features.shape = (32, 256)        # Image features\n",
    "     captions.shape = (32, 15)         # Caption word indices\n",
    "     outputs = self.decoder(features, captions)\n",
    "     outputs.shape = (32, 15, 5000)    # Word predictions\n",
    "     ```\n",
    "\n",
    "4. **`return outputs`**\n",
    "   - **Returns**: Prediction tensor to training loop\n",
    "   - **Used for**: Calculating loss (compare with ground truth)\n",
    "\n",
    "**Complete Flow:**\n",
    "```python\n",
    "# Training example\n",
    "model = CNNtoRNN(embed_size=256, hidden_size=256, vocab_size=5000, num_layers=1)\n",
    "\n",
    "images = load_images()  # (32, 3, 224, 224)\n",
    "captions = load_captions()  # (32, 15)\n",
    "\n",
    "# Call model (invokes forward automatically)\n",
    "predictions = model(images, captions)\n",
    "\n",
    "# predictions.shape = (32, 15, 5000)\n",
    "# For each of 32 images:\n",
    "#   For each of 15 word positions:\n",
    "#     Probability distribution over 5000 words\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Part 3: Caption Generation (Inference)\n",
    "\n",
    "```python\n",
    "def caption(self, image, vocabulary, max_length=50):\n",
    "    result_caption = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        x = self.encoder(image).unsqueeze(0)\n",
    "        states = None\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            hiddens, states = self.decoder.lstm(x, states)\n",
    "            output = self.decoder.linear(hiddens.squeeze(1))\n",
    "            predicted = output.argmax(1)\n",
    "            \n",
    "            result_caption.append(predicted.item())\n",
    "            \n",
    "            x = self.decoder.embed(predicted).unsqueeze(1)\n",
    "            \n",
    "            if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                break\n",
    "                \n",
    "    return [vocabulary.itos[idx] for idx in result_caption]\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Critic writes review word by word\n",
    "\n",
    "**Line-by-line:**\n",
    "\n",
    "1. **`result_caption = []`**\n",
    "   - **Creates**: Empty list to store word indices\n",
    "   - **Will become**: `[4, 7, 89, 5, 12, 2]` (word indices)\n",
    "\n",
    "2. **`with torch.no_grad():`**\n",
    "   - **Disables**: Gradient computation\n",
    "   - **Why?**: Inference only (not training), saves memory\n",
    "   - **Effect**: PyTorch won't track operations for backpropagation\n",
    "\n",
    "3. **`x = self.encoder(image).unsqueeze(0)`**\n",
    "   - **`self.encoder(image)`**: Get image features\n",
    "     - Input: (1, 3, 299, 299)\n",
    "     - Output: (1, 256)\n",
    "   - **`.unsqueeze(0)`**: Add sequence dimension\n",
    "     - Changes: (1, 256) ‚Üí (1, 1, 256)\n",
    "     - Why? LSTM expects (batch, sequence, features)\n",
    "   - **Returns**: Tensor (1, 1, 256)\n",
    "\n",
    "4. **`states = None`**\n",
    "   - **Initializes**: LSTM hidden state\n",
    "   - **None means**: LSTM starts with zero state\n",
    "   - **Will become**: Tuple of (hidden, cell) after first LSTM call\n",
    "\n",
    "5. **`for _ in range(max_length):`**\n",
    "   - **Loop**: Up to max_length times (e.g., 50)\n",
    "   - **Why?**: Generate up to 50 words\n",
    "   - **Can exit early**: If `<EOS>` predicted\n",
    "\n",
    "6. **`hiddens, states = self.decoder.lstm(x, states)`**\n",
    "   - **Calls**: LSTM with current input and previous state\n",
    "   - **Input**:\n",
    "     - `x`: Current word embedding (1, 1, 256)\n",
    "     - `states`: Previous hidden state (None for first step)\n",
    "   - **Returns**:\n",
    "     - `hiddens`: LSTM output (1, 1, 256)\n",
    "     - `states`: Updated hidden state (for next step)\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     # Step 1: x = image features, states = None\n",
    "     hiddens, states = lstm(x, None)\n",
    "     \n",
    "     # Step 2: x = embedding of predicted word, states = previous states\n",
    "     hiddens, states = lstm(x, states)\n",
    "     ```\n",
    "\n",
    "7. **`output = self.decoder.linear(hiddens.squeeze(1))`**\n",
    "   - **`hiddens.squeeze(1)`**: Remove sequence dimension\n",
    "     - Changes: (1, 1, 256) ‚Üí (1, 256)\n",
    "   - **`self.decoder.linear(...)`**: Convert to vocabulary scores\n",
    "     - Input: (1, 256)\n",
    "     - Output: (1, 5000) - score for each word\n",
    "   - **Returns**: Tensor (1, 5000)\n",
    "\n",
    "8. **`predicted = output.argmax(1)`**\n",
    "   - **`.argmax(1)`**: Get index of highest score\n",
    "     - Dimension 1 = vocabulary dimension\n",
    "   - **Returns**: Tensor with single integer (0-4999)\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     output = tensor([[0.1, 0.8, 0.3, 0.2, ...]])  # 5000 scores\n",
    "     predicted = output.argmax(1)\n",
    "     # Returns: tensor([1])  # Index 1 has highest score (0.8)\n",
    "     ```\n",
    "\n",
    "9. **`result_caption.append(predicted.item())`**\n",
    "   - **`.item()`**: Extract Python integer from tensor\n",
    "     - tensor([1]) ‚Üí 1\n",
    "   - **`.append()`**: Add to result list\n",
    "   - **Example**: `result_caption = [1, 4, 7, 89, ...]`\n",
    "\n",
    "10. **`x = self.decoder.embed(predicted).unsqueeze(1)`**\n",
    "    - **`self.decoder.embed(predicted)`**: Convert word index to embedding\n",
    "      - Input: tensor([4]) (word index)\n",
    "      - Output: (1, 256) (word vector)\n",
    "    - **`.unsqueeze(1)`**: Add sequence dimension\n",
    "      - Changes: (1, 256) ‚Üí (1, 1, 256)\n",
    "    - **Purpose**: Use predicted word as input for next step\n",
    "    - **Returns**: Tensor (1, 1, 256)\n",
    "\n",
    "11. **`if vocabulary.itos[predicted.item()] == \"<EOS>\":`**\n",
    "    - **`predicted.item()`**: Get word index (e.g., 2)\n",
    "    - **`vocabulary.itos[2]`**: Convert to word (e.g., \"<EOS>\")\n",
    "    - **Check**: If end-of-sentence predicted\n",
    "    - **If true**: Exit loop early\n",
    "\n",
    "12. **`break`**\n",
    "    - **Exits**: For loop\n",
    "    - **Why?**: Caption complete (no need to generate more words)\n",
    "\n",
    "13. **`return [vocabulary.itos[idx] for idx in result_caption]`**\n",
    "    - **List comprehension**: Convert all indices to words\n",
    "    - **Example**:\n",
    "      ```python\n",
    "      result_caption = [1, 4, 7, 89, 5, 2]\n",
    "      # Convert using vocabulary:\n",
    "      words = [vocabulary.itos[1], vocabulary.itos[4], ...]\n",
    "      # Returns: [\"<SOS>\", \"a\", \"dog\", \"running\", \"in\", \"<EOS>\"]\n",
    "      ```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Complete Example: Step-by-Step\n",
    "\n",
    "```python\n",
    "# 1. SETUP\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "vocab.build_vocabulary(captions)\n",
    "model = CNNtoRNN(embed_size=256, hidden_size=256, vocab_size=len(vocab), num_layers=1)\n",
    "\n",
    "# 2. LOAD IMAGE\n",
    "image = load_image(\"dog.jpg\")  # (1, 3, 299, 299)\n",
    "\n",
    "# 3. GENERATE CAPTION\n",
    "caption_words = model.caption(image, vocab, max_length=20)\n",
    "\n",
    "# STEP-BY-STEP GENERATION:\n",
    "\n",
    "# Initialization:\n",
    "x = encoder(image).unsqueeze(0)  # (1, 1, 256) - image features\n",
    "states = None\n",
    "result = []\n",
    "\n",
    "# Iteration 1:\n",
    "hiddens, states = lstm(x, None)  # Input: image features\n",
    "output = linear(hiddens.squeeze(1))  # (1, 5000)\n",
    "predicted = output.argmax(1)  # tensor([1]) = <SOS>\n",
    "result.append(1)  # [1]\n",
    "x = embed(tensor([1])).unsqueeze(1)  # Embed <SOS>\n",
    "\n",
    "# Iteration 2:\n",
    "hiddens, states = lstm(x, states)  # Input: <SOS> embedding + previous state\n",
    "output = linear(hiddens.squeeze(1))\n",
    "predicted = output.argmax(1)  # tensor([4]) = \"a\"\n",
    "result.append(4)  # [1, 4]\n",
    "x = embed(tensor([4])).unsqueeze(1)  # Embed \"a\"\n",
    "\n",
    "# Iteration 3:\n",
    "hiddens, states = lstm(x, states)  # Input: \"a\" embedding + previous state\n",
    "output = linear(hiddens.squeeze(1))\n",
    "predicted = output.argmax(1)  # tensor([7]) = \"dog\"\n",
    "result.append(7)  # [1, 4, 7]\n",
    "x = embed(tensor([7])).unsqueeze(1)  # Embed \"dog\"\n",
    "\n",
    "# ... continue until <EOS> or max_length ...\n",
    "\n",
    "# Iteration 6:\n",
    "predicted = tensor([2])  # <EOS>\n",
    "result.append(2)  # [1, 4, 7, 89, 5, 2]\n",
    "if vocab.itos[2] == \"<EOS>\":\n",
    "    break  # Stop generating\n",
    "\n",
    "# 4. CONVERT TO WORDS\n",
    "caption = [vocab.itos[idx] for idx in result]\n",
    "# [\"<SOS>\", \"a\", \"dog\", \"running\", \"in\", \"<EOS>\"]\n",
    "\n",
    "# 5. CLEAN UP\n",
    "caption = \" \".join(caption[1:-1])  # Remove <SOS> and <EOS>\n",
    "# \"a dog running in\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Training vs Inference: Key Differences\n",
    "\n",
    "### üéì Training Mode (`forward` method)\n",
    "\n",
    "```python\n",
    "# PARALLEL PROCESSING (Teacher Forcing)\n",
    "outputs = model(images, captions)\n",
    "\n",
    "# All words predicted at once:\n",
    "# Input:  [<SOS>, \"a\", \"dog\", \"running\"]\n",
    "# Output: [\"a\", \"dog\", \"running\", <EOS>]  ‚Üê All predicted simultaneously\n",
    "\n",
    "# Uses ground truth words as input\n",
    "# Fast (one forward pass)\n",
    "# Used for training only\n",
    "```\n",
    "\n",
    "### üöÄ Inference Mode (`caption` method)\n",
    "\n",
    "```python\n",
    "# SEQUENTIAL PROCESSING (Autoregressive)\n",
    "caption = model.caption(image, vocab)\n",
    "\n",
    "# Words predicted one by one:\n",
    "# Step 1: [] ‚Üí predict \"a\"\n",
    "# Step 2: [\"a\"] ‚Üí predict \"dog\"\n",
    "# Step 3: [\"a\", \"dog\"] ‚Üí predict \"running\"\n",
    "# Step 4: [\"a\", \"dog\", \"running\"] ‚Üí predict <EOS>\n",
    "\n",
    "# Uses model's own predictions as input\n",
    "# Slower (multiple forward passes)\n",
    "# Used for inference only\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways: CNNtoRNN Class\n",
    "\n",
    "### ‚úÖ Architecture\n",
    "- **Encoder**: Converts image ‚Üí feature vector\n",
    "- **Decoder**: Converts features ‚Üí text sequence\n",
    "- **Manager**: Coordinates both parts\n",
    "\n",
    "### ‚úÖ Two Modes\n",
    "- **forward()**: Training with teacher forcing\n",
    "- **caption()**: Inference with autoregressive generation\n",
    "\n",
    "### ‚úÖ Key Methods\n",
    "- **`.encoder()`**: Returns image features\n",
    "- **`.decoder()`**: Returns word predictions\n",
    "- **`.argmax()`**: Picks most likely word\n",
    "- **`.item()`**: Converts tensor to Python number\n",
    "- **`.unsqueeze()`**: Adds dimension for LSTM\n",
    "\n",
    "### ‚úÖ Why This Design?\n",
    "- Modular (easy to swap components)\n",
    "- Flexible (different modes for train/test)\n",
    "- Efficient (GPU-optimized operations)\n",
    "- Clean (separates concerns)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f1689",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üìä Two Modes: Training vs Inference\n",
    "\n",
    "**TRAINING MODE** (`forward` method):\n",
    "```python\n",
    "# We have ground truth captions\n",
    "images = [img1, img2, ..., img8]  # Batch of 8 images\n",
    "captions = [\"A dog ...\", \"A cat ...\", ...]  # Known captions\n",
    "\n",
    "outputs = model(images, captions)\n",
    "# Model predicts all words at once (teacher forcing)\n",
    "# Compare predictions with ground truth ‚Üí calculate loss ‚Üí update weights\n",
    "```\n",
    "\n",
    "**INFERENCE MODE** (`caption` method):\n",
    "```python\n",
    "# We DON'T know the caption\n",
    "image = single_image  # One image\n",
    "\n",
    "caption = model.caption(image, vocabulary)\n",
    "# Model generates word by word:\n",
    "# Step 1: [Image] ‚Üí \"A\"\n",
    "# Step 2: [Image, \"A\"] ‚Üí \"dog\"\n",
    "# Step 3: [Image, \"A\", \"dog\"] ‚Üí \"is\"\n",
    "# ...until <EOS> or max_length reached\n",
    "```\n",
    "\n",
    "### üéØ Inference Example\n",
    "\n",
    "```python\n",
    "# Given image of dog in park\n",
    "image = load_image(\"dog_park.jpg\")  # Shape: (1, 3, 299, 299)\n",
    "\n",
    "# Generate caption\n",
    "caption = model.caption(image, vocab, max_length=20)\n",
    "\n",
    "# Output: ['a', 'brown', 'dog', 'is', 'playing', 'in', 'the', 'park', '<EOS>']\n",
    "# As sentence: \"A brown dog is playing in the park\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Step 5: Vocabulary Class (Same as Custom Dataset)\n",
    "\n",
    "### üçï Simple Analogy: Menu with Item Numbers\n",
    "\n",
    "(See explanation in Custom_Dataset_Build.ipynb)\n",
    "\n",
    "**Quick Summary:**\n",
    "- Converts words ‚Üî numbers\n",
    "- Special tokens: `<PAD>`, `<SOS>`, `<EOS>`, `<UNK>`\n",
    "- Only includes words appearing `freq_threshold` times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c008650",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Deep Dive: Vocabulary Class - Line by Line\n",
    "\n",
    "### üçï Simple Analogy: Restaurant Menu System\n",
    "\n",
    "Imagine a restaurant with a coding system:\n",
    "- **Menu board**: Shows \"Pizza = 5\", \"Burger = 10\"\n",
    "- **Kitchen knows**: 5 means make pizza, 10 means make burger\n",
    "- **This is the Vocabulary class**: Converts words ‚Üî numbers\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Complete Vocabulary Class Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d893d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Step 6: Import Dataset Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb3fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickerDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir=root_dir\n",
    "        self.df=pd.read_csv(captions_file)\n",
    "        self.transform=transform\n",
    "        \n",
    "        self.imgs=self.df['image']\n",
    "        self.captions=self.df['caption']\n",
    "        \n",
    "        self.vocab=Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption=self.captions[index]\n",
    "        img_id=self.imgs[index]\n",
    "        img_path=os.path.join(self.root_dir, img_id)\n",
    "        image=Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image=self.transform(image)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return image, torch.tensor(numericalized_caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba1b6f",
   "metadata": {},
   "source": [
    "```python\n",
    "import spacy                        # Text processing\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence  # Padding variable-length sequences\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image              # Image loading\n",
    "import pandas as pd                # CSV reading\n",
    "import os                          # File paths\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Step 7: Flickr Dataset Class\n",
    "\n",
    "(See detailed explanation in Custom_Dataset_Build.ipynb)\n",
    "\n",
    "**Quick Summary:**\n",
    "- Loads image + caption pairs\n",
    "- Builds vocabulary from all captions\n",
    "- Returns (image_tensor, numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a4c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx=pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        images = [item[0].unsqueeze(0) for item in batch]\n",
    "        images = torch.cat(images, dim=0)\n",
    "        captions = [item[1] for item in batch]\n",
    "        captions = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        return images, captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e2cb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Step 8: Custom Collate Function\n",
    "\n",
    "(See detailed explanation in Custom_Dataset_Build.ipynb)\n",
    "\n",
    "**Quick Summary:**\n",
    "- Pads captions to same length\n",
    "- Uses `<PAD>` token (index 0)\n",
    "- Returns batched images + padded captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    root_folder,\n",
    "    annotation_file,\n",
    "    transform,\n",
    "    batch_size=32,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    dataset = FlickerDataset(\n",
    "        root_dir=root_folder,\n",
    "        captions_file=annotation_file,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb208a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé¨ Step 9: DataLoader Function\n",
    "\n",
    "**Modified to return both loader AND dataset:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b260d40",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_loader(...):\n",
    "    dataset = FlickerDataset(...)\n",
    "    loader = DataLoader(...)\n",
    "    return loader, dataset  # ‚úÖ Returns BOTH (needed for vocab_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Step 10: Complete Training Loop\n",
    "\n",
    "### üçï Simple Analogy: Learning to Describe Food\n",
    "\n",
    "You're training someone to describe dishes:\n",
    "1. **Show photo** (image)\n",
    "2. **Show correct description** (caption)\n",
    "3. **They try to describe** (model prediction)\n",
    "4. **You correct them** (calculate loss)\n",
    "5. **They improve** (update weights)\n",
    "6. **Repeat** for thousands of photos\n",
    "\n",
    "### üîß Technical Explanation: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step\n",
    "\n",
    "def train():\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "    train_loader,dataset=get_loader(\n",
    "        root_folder=\"data/flickr8k/images\",\n",
    "        annotation_file=\"data/flickr8k/captions.txt\",\n",
    "        transform=transform,\n",
    "        batch_size=32,\n",
    "    )\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    embed_size=256\n",
    "    hidden_size=256\n",
    "    vocab_size=len(dataset.vocab)\n",
    "    num_layers=1\n",
    "    learning_rate=3e-4\n",
    "    num_epochs=10\n",
    "    load_model=False\n",
    "    save_model=True\n",
    "    \n",
    "    \n",
    "    writer=SummaryWriter(\"runs/flickr8k_experiment_1\")\n",
    "    step=0\n",
    "    \n",
    "    model=CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    if load_model:\n",
    "        step=load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model,optimizer)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if save_model:\n",
    "            checkpoint={\n",
    "                \"state_dict\":model.state_dict(),\n",
    "                \"optimizer\":optimizer.state_dict(),\n",
    "                \"step\":step,\n",
    "            }\n",
    "            save_checkpoint(checkpoint,\"my_checkpoint.pth.tar\")\n",
    "            \n",
    "            \n",
    "        for idx, (imgs, captions) in enumerate(train_loader):\n",
    "            imgs=imgs.to(device)\n",
    "            captions=captions.to(device)\n",
    "            \n",
    "            outputs=model(imgs,captions[:-1])\n",
    "            \n",
    "            loss=criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]),\n",
    "                captions.reshape(-1),\n",
    "            )\n",
    "            \n",
    "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step+=1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if idx % 100 ==0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}] Batch {idx}/{len(train_loader)} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6cd10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Step 10: Training Function - Complete Breakdown\n",
    "\n",
    "The training function is large, so let's break it into **digestible chunks** with explanations!\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Part 1: Setup Transforms\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224√ó224\n",
    "    transforms.ToTensor(),          # Convert to tensor [0, 1]\n",
    "    transforms.Normalize(           # Normalize for Inception\n",
    "        (0.485, 0.456, 0.406),     # ImageNet mean\n",
    "        (0.229, 0.224, 0.225)      # ImageNet std\n",
    "    ),\n",
    "])\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Preparing ingredients before cooking\n",
    "- **Resize**: Cut all vegetables to same size\n",
    "- **ToTensor**: Put ingredients in containers (PIL ‚Üí Tensor)\n",
    "- **Normalize**: Season consistently (match what Inception was trained on)\n",
    "\n",
    "**üîß Technical:**\n",
    "- Inception v3 was trained on ImageNet with specific normalization\n",
    "- Using the same normalization ensures best performance\n",
    "- Mean/std values are ImageNet dataset statistics\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ Part 2: Load Dataset\n",
    "\n",
    "```python\n",
    "train_loader, dataset = get_loader(\n",
    "    root_folder=\"data/flickr8k/images\",\n",
    "    annotation_file=\"data/flickr8k/captions.txt\",\n",
    "    transform=transform,\n",
    "    batch_size=32,\n",
    ")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Opening restaurant and bringing in supplies\n",
    "- **root_folder**: Kitchen pantry (where images are stored)\n",
    "- **annotation_file**: Menu with descriptions\n",
    "- **batch_size=32**: Serve 32 customers at once (not one by one)\n",
    "\n",
    "**üîß Technical:**\n",
    "- Creates DataLoader that yields batches of (images, captions)\n",
    "- Returns dataset too (we need its vocabulary)\n",
    "- Handles padding automatically via MyCollate\n",
    "\n",
    "---\n",
    "\n",
    "### üñ•Ô∏è Part 3: Setup Device\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Choose your vehicle\n",
    "- GPU = Sports car üèéÔ∏è (fast, for heavy work)\n",
    "- CPU = Bicycle üö≤ (slow, but always available)\n",
    "\n",
    "**üîß Technical:**\n",
    "- Checks if NVIDIA GPU is available\n",
    "- All tensors and models will move to this device\n",
    "- Training on GPU is 10-100x faster than CPU\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Part 4: Hyperparameters\n",
    "\n",
    "```python\n",
    "embed_size = 256        # Size of feature vectors\n",
    "hidden_size = 256       # Size of LSTM hidden state\n",
    "vocab_size = len(dataset.vocab)  # Number of words in vocabulary\n",
    "num_layers = 1          # Number of LSTM layers\n",
    "learning_rate = 3e-4    # Learning rate for optimizer\n",
    "num_epochs = 10         # Number of complete passes through dataset\n",
    "load_model = False      # Whether to load saved checkpoint\n",
    "save_model = True       # Whether to save checkpoints\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Recipe settings\n",
    "- **embed_size**: Size of flavor profile (how many taste dimensions)\n",
    "- **hidden_size**: Chef's memory capacity (how much context to remember)\n",
    "- **vocab_size**: Number of dishes on menu\n",
    "- **learning_rate**: How fast chef learns (big steps vs small adjustments)\n",
    "- **num_epochs**: How many times to practice entire recipe book\n",
    "\n",
    "**üîß Technical:**\n",
    "- **embed_size=256**: Matches CNN output and word embeddings\n",
    "- **hidden_size=256**: Larger = more memory but slower\n",
    "- **learning_rate=3e-4** (0.0003): Adam optimizer works well with this\n",
    "- **num_epochs=10**: Typically need 10-20 epochs for convergence\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Part 5: Setup TensorBoard\n",
    "\n",
    "```python\n",
    "writer = SummaryWriter(\"runs/flickr8k_experiment_1\")\n",
    "step = 0\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Restaurant quality tracker\n",
    "- Records how good each dish tastes over time\n",
    "- Can review later to see improvement\n",
    "\n",
    "**üîß Technical:**\n",
    "- TensorBoard logs training metrics\n",
    "- View with: `tensorboard --logdir=runs`\n",
    "- `step` counts total batches processed\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Part 6: Create Model\n",
    "\n",
    "```python\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Hire photographer + critic\n",
    "- Photographer (CNN) analyzes images\n",
    "- Critic (RNN) writes descriptions\n",
    "\n",
    "**üîß Technical:**\n",
    "- Creates complete model (Encoder + Decoder)\n",
    "- `.to(device)` moves model to GPU/CPU\n",
    "- Model has ~50M parameters (Inception) + ~5M (LSTM)\n",
    "\n",
    "---\n",
    "\n",
    "### üí• Part 7: Loss Function\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Quality inspector\n",
    "- Compares chef's dish with expected dish\n",
    "- Ignores placeholder dishes (padding)\n",
    "\n",
    "**üîß Technical:**\n",
    "- **CrossEntropyLoss**: Measures prediction accuracy for classification\n",
    "- **ignore_index**: Don't penalize model for `<PAD>` tokens\n",
    "- Lower loss = better predictions\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Part 8: Optimizer\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Cooking coach\n",
    "- Watches chef's mistakes\n",
    "- Suggests improvements\n",
    "\n",
    "**üîß Technical:**\n",
    "- **Adam**: Adaptive learning rate optimizer (better than SGD)\n",
    "- Updates model weights to minimize loss\n",
    "- `lr=3e-4`: Step size for weight updates\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Part 9: Load Checkpoint (Optional)\n",
    "\n",
    "```python\n",
    "if load_model:\n",
    "    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Resume from saved progress\n",
    "- Like loading a video game save file\n",
    "- Continue training from where you left off\n",
    "\n",
    "**üîß Technical:**\n",
    "- Only runs if `load_model=True`\n",
    "- Restores model weights, optimizer state, and step counter\n",
    "- Useful if training was interrupted\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Part 10: Training Mode\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Open restaurant for practice\n",
    "- Turn on all training features (dropout, batch norm)\n",
    "- vs `model.eval()` which turns them off for testing\n",
    "\n",
    "**üîß Technical:**\n",
    "- Enables dropout (randomly zeros neurons)\n",
    "- Enables batch normalization training mode\n",
    "- Required before training loop\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Part 11: Epoch Loop\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # Save checkpoint at start of each epoch\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"step\": step,\n",
    "        }\n",
    "        save_checkpoint(checkpoint, \"my_checkpoint.pth.tar\")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Practice recipe book 10 times\n",
    "- Each epoch = one complete pass through all recipes\n",
    "- Save progress after each complete pass\n",
    "\n",
    "**üîß Technical:**\n",
    "- **Epoch**: One complete pass through entire dataset\n",
    "- Saves checkpoint at start (can resume if crash)\n",
    "- `state_dict()` contains all model weights\n",
    "\n",
    "---\n",
    "\n",
    "### üé≤ Part 12: Batch Loop\n",
    "\n",
    "```python\n",
    "for idx, (imgs, captions) in enumerate(train_loader):\n",
    "    imgs = imgs.to(device)\n",
    "    captions = captions.to(device)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Process one table of customers at a time\n",
    "- 32 customers (batch_size=32)\n",
    "- Bring their orders to the kitchen (GPU/CPU)\n",
    "\n",
    "**üîß Technical:**\n",
    "- `enumerate(train_loader)`: Loop through batches\n",
    "- `imgs.shape = (32, 3, 224, 224)`: 32 images\n",
    "- `captions.shape = (max_len, 32)`: 32 captions (padded)\n",
    "- `.to(device)`: Move data to GPU/CPU\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Part 13: Forward Pass\n",
    "\n",
    "```python\n",
    "outputs = model(imgs, captions[:-1])\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Chef cooks the dish\n",
    "- Look at ingredients (images)\n",
    "- Follow recipe (captions[:-1])\n",
    "- Produce dish (outputs)\n",
    "\n",
    "**üîß Technical:**\n",
    "- **Input**: Images + captions (without last word)\n",
    "- **Output**: Predicted words for each position\n",
    "- **Shape**: `(32, max_len, vocab_size)` = probability for each word\n",
    "- **Teacher forcing**: Use ground truth words during training\n",
    "\n",
    "**Why `captions[:-1]`?**\n",
    "```python\n",
    "Original caption: [<SOS>, \"A\", \"dog\", \"running\", <EOS>]\n",
    "Feed to model:    [<SOS>, \"A\", \"dog\", \"running\"]        # Remove last\n",
    "Model predicts:   [\"A\",   \"dog\", \"running\", <EOS>]      # Predict next word\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí• Part 14: Calculate Loss\n",
    "\n",
    "```python\n",
    "loss = criterion(\n",
    "    outputs.reshape(-1, outputs.shape[2]),  # Flatten to (batch*seq, vocab)\n",
    "    captions.reshape(-1),                    # Flatten to (batch*seq)\n",
    ")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Judge tastes dish and scores it\n",
    "- Compare chef's output with expected recipe\n",
    "- Lower score = better match\n",
    "\n",
    "**üîß Technical:**\n",
    "- **Reshape outputs**: `(32, 15, 10000)` ‚Üí `(480, 10000)`\n",
    "  - 32 captions √ó 15 words = 480 predictions\n",
    "  - Each prediction has 10000 possibilities (vocab_size)\n",
    "- **Reshape captions**: `(32, 15)` ‚Üí `(480,)`\n",
    "  - 480 ground truth word indices\n",
    "- **CrossEntropyLoss**: Compares predicted probabilities with true words\n",
    "- **Result**: Single number (e.g., 2.5) - lower is better\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Part 15: Log to TensorBoard\n",
    "\n",
    "```python\n",
    "writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "step += 1\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Write in quality log book\n",
    "- Record: \"Batch #1234, Loss = 2.5\"\n",
    "- Track improvement over time\n",
    "\n",
    "**üîß Technical:**\n",
    "- Logs loss value for visualization\n",
    "- `step` counts total batches (not just per epoch)\n",
    "- View graph: `tensorboard --logdir=runs`\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Part 16: Backward Pass (Learning!)\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()  # Reset gradients\n",
    "loss.backward()        # Calculate gradients\n",
    "optimizer.step()       # Update weights\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Coach gives feedback and chef improves\n",
    "1. **zero_grad()**: Clear old feedback notes\n",
    "2. **loss.backward()**: Calculate: \"What went wrong and by how much?\"\n",
    "3. **optimizer.step()**: Chef adjusts technique\n",
    "\n",
    "**üîß Technical:**\n",
    "1. **zero_grad()**: Clear gradient buffers (they accumulate by default)\n",
    "2. **loss.backward()**: Backpropagation - calculates gradient for each parameter\n",
    "3. **optimizer.step()**: Updates weights using gradients:\n",
    "   ```python\n",
    "   weight_new = weight_old - learning_rate * gradient\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### üñ®Ô∏è Part 17: Print Progress\n",
    "\n",
    "```python\n",
    "if idx % 100 == 0:\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Batch {idx}/{len(train_loader)} Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Check progress every 100 dishes\n",
    "- Don't print after every dish (too much!)\n",
    "- Check occasionally to ensure quality improving\n",
    "\n",
    "**üîß Technical:**\n",
    "- Prints every 100 batches\n",
    "- Shows: which epoch, which batch, current loss\n",
    "- Example output:\n",
    "  ```\n",
    "  Epoch [0/10] Batch 0/250 Loss: 9.2134\n",
    "  Epoch [0/10] Batch 100/250 Loss: 4.5678\n",
    "  Epoch [0/10] Batch 200/250 Loss: 3.1234\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## üéä Complete Training Flow Summary\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ FOR EACH EPOCH (10 times)                           ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Save checkpoint                                 ‚îÇ\n",
    "‚îÇ  ‚îÇ                                                   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ FOR EACH BATCH (250 batches)                   ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 1. Get 32 images + captions                ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 2. Move to GPU                             ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 3. Forward pass (predict captions)         ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 4. Calculate loss (how wrong?)             ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 5. Log to TensorBoard                      ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 6. Backward pass (calculate gradients)     ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 7. Update weights (improve model)          ‚îÇ\n",
    "‚îÇ      ‚îî‚îÄ 8. Print progress (every 100 batches)      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Training Timeline Example\n",
    "\n",
    "**Dataset:** 8,000 image-caption pairs, batch_size=32\n",
    "\n",
    "**One Epoch:**\n",
    "- Number of batches = 8,000 / 32 = 250 batches\n",
    "- Time per batch ‚âà 0.5 seconds (on GPU)\n",
    "- **Total time per epoch** ‚âà 125 seconds ‚âà **2 minutes**\n",
    "\n",
    "**Complete Training (10 epochs):**\n",
    "- **Total time** ‚âà 10 √ó 2 min = **20 minutes** (on GPU)\n",
    "- On CPU: 10-20x slower ‚âà **3-6 hours**\n",
    "\n",
    "**Expected Loss Progress:**\n",
    "```\n",
    "Epoch 0: Loss starts at ~9.0\n",
    "Epoch 1: Loss drops to ~5.0\n",
    "Epoch 3: Loss drops to ~3.0\n",
    "Epoch 5: Loss drops to ~2.0\n",
    "Epoch 10: Loss around ~1.5 (good!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "### üíæ Checkpoint Saving\n",
    "- Saves after each epoch (every ~2 minutes)\n",
    "- Can resume if training crashes\n",
    "- File: `my_checkpoint.pth.tar` (~200MB)\n",
    "\n",
    "### üéØ Teacher Forcing\n",
    "- During training: Use ground truth words (faster learning)\n",
    "- During inference: Use model's own predictions (more realistic)\n",
    "\n",
    "### üìä TensorBoard Monitoring\n",
    "```bash\n",
    "# In terminal:\n",
    "tensorboard --logdir=runs\n",
    "\n",
    "# Open browser:\n",
    "http://localhost:6006\n",
    "```\n",
    "\n",
    "### üö® Common Issues\n",
    "1. **Out of memory**: Reduce batch_size (32 ‚Üí 16)\n",
    "2. **Loss not decreasing**: Check learning rate (try 1e-4)\n",
    "3. **NaN loss**: Gradient explosion (add gradient clipping)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### ‚úÖ Training is a Loop\n",
    "```python\n",
    "for epoch in epochs:\n",
    "    for batch in batches:\n",
    "        predict ‚Üí calculate loss ‚Üí backpropagate ‚Üí update weights\n",
    "```\n",
    "\n",
    "### ‚úÖ Three Key Steps\n",
    "1. **Forward pass**: Model makes predictions\n",
    "2. **Loss calculation**: Measure how wrong\n",
    "3. **Backward pass**: Learn from mistakes\n",
    "\n",
    "### ‚úÖ Monitoring\n",
    "- Loss should decrease over time\n",
    "- Save checkpoints regularly\n",
    "- Use TensorBoard for visualization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ After Training\n",
    "\n",
    "Once training completes:\n",
    "\n",
    "```python\n",
    "# Load best checkpoint\n",
    "model.load_state_dict(torch.load(\"my_checkpoint.pth.tar\")[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Generate caption for new image\n",
    "image = load_and_transform_image(\"dog.jpg\")\n",
    "caption = model.caption(image, vocabulary)\n",
    "print(\" \".join(caption))\n",
    "# Output: \"a brown dog is playing with a ball in the park\"\n",
    "```\n",
    "\n",
    "**Congratulations! You've trained an image captioning model! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
