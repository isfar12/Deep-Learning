{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef97e276",
   "metadata": {},
   "source": [
    "# üì∑ Image Captioning with CNN + RNN\n",
    "\n",
    "## üéØ What is Image Captioning?\n",
    "\n",
    "**Simple Definition:** Given an image, generate a natural language description.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input:  üñºÔ∏è [Photo of a dog playing with a ball in a park]\n",
    "Output: \"A brown dog is playing with a red ball in the park\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† The Big Idea: Two-Part System\n",
    "\n",
    "### üçï Simple Analogy: Restaurant Review System\n",
    "\n",
    "Imagine you're writing a restaurant review:\n",
    "\n",
    "**Part 1: LOOK at the food** üëÄ\n",
    "- Take a photo of the dish\n",
    "- Your eyes analyze: colors, shapes, ingredients\n",
    "- **This is the CNN Encoder** ‚Üí extracts visual features\n",
    "\n",
    "**Part 2: WRITE the review** ‚úçÔ∏è\n",
    "- Your brain converts visual information into words\n",
    "- Generates sentence word by word: \"This ‚Üí delicious ‚Üí pizza ‚Üí has ‚Üí fresh ‚Üí toppings\"\n",
    "- **This is the RNN Decoder** ‚Üí generates text sequence\n",
    "\n",
    "---\n",
    "\n",
    "## üèóÔ∏è Architecture Overview\n",
    "\n",
    "```\n",
    "Image ‚Üí [CNN Encoder] ‚Üí Feature Vector ‚Üí [RNN Decoder] ‚Üí Caption\n",
    "  üñºÔ∏è         üîç              üìä              ‚úçÔ∏è            üìù\n",
    "```\n",
    "\n",
    "**Components:**\n",
    "1. **EncoderCNN**: Pre-trained Inception model (extracts image features)\n",
    "2. **DecoderRNN**: LSTM network (generates caption word by word)\n",
    "3. **CNNtoRNN**: Combines both parts\n",
    "\n",
    "Let's build each component!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e46743",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"asset/image_caption_overview.png\" alt=\"Architecture of Image Caption\" width=\"800\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab538168",
   "metadata": {},
   "source": [
    "## üèõÔ∏è Image Captioning Architecture\n",
    "\n",
    "This diagram shows the complete pipeline:\n",
    "1. **Input Image** ‚Üí processed by CNN encoder\n",
    "2. **Feature Vector** ‚Üí extracted visual features\n",
    "3. **LSTM Decoder** ‚Üí generates words one by one\n",
    "4. **Output Caption** ‚Üí final sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45724ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a386c47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Step 1: Import Core Libraries\n",
    "\n",
    "### üîß What Each Library Does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7aa48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self,embed_size,train_CNN=False):\n",
    "        super(EncoderCNN,self).__init__()\n",
    "        \n",
    "        self.train_CNN=train_CNN\n",
    "        self.inception=models.inception_v3(pretrained=True,aux_logits=False)\n",
    "        self.inception.fc=nn.Linear(self.inception.fc.in_features,embed_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self,images):\n",
    "        features=self.inception(images)\n",
    "        \n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = self.train_CNN\n",
    "        return self.dropout(self.relu(features))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329eb363",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch                    # Main PyTorch library\n",
    "import torch.nn as nn          # Neural network modules\n",
    "import torchvision.models as models  # Pre-trained models (Inception)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Step 2: Build CNN Encoder (Vision Part)\n",
    "\n",
    "### üçï Simple Analogy: Professional Food Photographer\n",
    "\n",
    "You hire a professional photographer to analyze dishes:\n",
    "- **Pre-trained eyes** (Inception model trained on ImageNet)\n",
    "- Converts photo into a **feature summary** (embed_size numbers)\n",
    "- Example: [0.5, 0.8, 0.2, ...] ‚Üí represents \"brown dog, green grass, blue sky\"\n",
    "\n",
    "### üîß Technical Explanation: EncoderCNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9612f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers):\n",
    "        super(DecoderRNN,self).__init__()\n",
    "        \n",
    "        self.embed=nn.Embedding(vocab_size,embed_size)\n",
    "        self.lstm=nn.LSTM(embed_size,hidden_size,num_layers,batch_first=True)\n",
    "        self.linear=nn.Linear(hidden_size,vocab_size)\n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, features,captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs=self.linear(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241aae9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîó Deep Dive: How Image Features Become \"Words\" for LSTM\n",
    "\n",
    "### üçï Simple Analogy: Restaurant Critic's First Impression\n",
    "\n",
    "Imagine a food critic:\n",
    "1. **Photographer shows photo** ‚Üí critic forms first impression (\"Ah, this looks like Italian food\")\n",
    "2. **Critic writes review** ‚Üí \"This\" ‚Üí \"delicious\" ‚Üí \"pizza\" ‚Üí \"has\" ‚Üí \"fresh\" ‚Üí \"toppings\"\n",
    "\n",
    "The photo analysis becomes the **starting point** for the review!\n",
    "\n",
    "### üîß Technical Explanation: Image Features as First \"Word\"\n",
    "\n",
    "In the decoder's forward pass:\n",
    "\n",
    "```python\n",
    "# 1. Image features from encoder\n",
    "features.shape = (batch_size, embed_size)\n",
    "# Example: (8, 256) - 8 images, each is 256 numbers\n",
    "\n",
    "# 2. Caption words converted to embeddings\n",
    "embeddings = self.embed(captions)\n",
    "# Example: (8, 10, 256) - 8 captions, 10 words each, 256-dim vectors\n",
    "\n",
    "# 3. THE MAGIC: Add sequence dimension to features\n",
    "features_with_seq = features.unsqueeze(1)\n",
    "# Shape changes: (8, 256) ‚Üí (8, 1, 256)\n",
    "# Now it looks like \"one word\" per image!\n",
    "\n",
    "# 4. Concatenate: Image feature becomes the first \"word\"\n",
    "embeddings = torch.cat((features_with_seq, embeddings), dim=1)\n",
    "# Result: (8, 11, 256) = 8 captions, 11 words (1 image + 10 real words), 256-dim\n",
    "```\n",
    "\n",
    "### üìä Visual Breakdown\n",
    "\n",
    "**Before concatenation:**\n",
    "```python\n",
    "Image features:  [0.5, 0.8, 0.2, ..., 0.7]  # 256 numbers (batch_size, 256)\n",
    "Caption embeddings:\n",
    "  Word 1 \"A\":    [0.1, 0.3, 0.5, ..., 0.2]  # 256 numbers\n",
    "  Word 2 \"dog\":  [0.7, 0.2, 0.1, ..., 0.9]  # 256 numbers\n",
    "  Word 3 \"is\":   [0.3, 0.6, 0.4, ..., 0.1]  # 256 numbers\n",
    "  ...\n",
    "```\n",
    "\n",
    "**After concatenation (what LSTM sees):**\n",
    "```python\n",
    "Sequence fed to LSTM:\n",
    "  Position 0 (image):  [0.5, 0.8, 0.2, ..., 0.7]  ‚Üê Image features\n",
    "  Position 1 \"A\":      [0.1, 0.3, 0.5, ..., 0.2]  ‚Üê Word embedding\n",
    "  Position 2 \"dog\":    [0.7, 0.2, 0.1, ..., 0.9]  ‚Üê Word embedding\n",
    "  Position 3 \"is\":     [0.3, 0.6, 0.4, ..., 0.1]  ‚Üê Word embedding\n",
    "  ...\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Image features and word embeddings are **both 256-dimensional vectors**\n",
    "- LSTM doesn't care if position 0 is an \"image\" or a \"word\"\n",
    "- It treats the image feature vector as the **context** for generating the caption\n",
    "\n",
    "### üéØ Complete Example\n",
    "\n",
    "**Input:**\n",
    "```python\n",
    "# Single image of a dog\n",
    "features = encoder(dog_image)  # Output: (1, 256)\n",
    "\n",
    "# Caption: \"A brown dog\"\n",
    "caption_indices = [1, 4, 45, 6]  # <SOS>=1, \"a\"=4, \"brown\"=45, \"dog\"=6\n",
    "```\n",
    "\n",
    "**Processing:**\n",
    "```python\n",
    "# 1. Embed caption words\n",
    "embeddings = self.embed(caption_indices)\n",
    "# Shape: (1, 4, 256) - 1 caption, 4 words, 256-dim each\n",
    "\n",
    "# 2. Add image as first position\n",
    "features_unsqueezed = features.unsqueeze(1)  # (1, 256) ‚Üí (1, 1, 256)\n",
    "embeddings = torch.cat((features_unsqueezed, embeddings), dim=1)\n",
    "# Shape: (1, 5, 256) - 1 caption, 5 positions (1 image + 4 words)\n",
    "\n",
    "# 3. LSTM sees this sequence:\n",
    "#    [Image_vector, <SOS>, \"a\", \"brown\", \"dog\"]\n",
    "#    \n",
    "# 4. LSTM generates predictions:\n",
    "#    Position 0 (after seeing image) ‚Üí predict <SOS>\n",
    "#    Position 1 (after seeing image + <SOS>) ‚Üí predict \"a\"\n",
    "#    Position 2 (after seeing image + <SOS> + \"a\") ‚Üí predict \"brown\"\n",
    "#    Position 3 (after seeing image + <SOS> + \"a\" + \"brown\") ‚Üí predict \"dog\"\n",
    "#    Position 4 (after seeing full context) ‚Üí predict <EOS>\n",
    "```\n",
    "\n",
    "### üí° Key Insight\n",
    "\n",
    "The image feature vector acts as **visual context** that influences every word generation:\n",
    "- **Without image context**: LSTM might generate random captions\n",
    "- **With image context**: LSTM generates captions that describe what's actually in the image!\n",
    "\n",
    "Think of it like this:\n",
    "```\n",
    "Image features = \"There's a brown furry animal in grass\"\n",
    "LSTM uses this context to generate: \"A brown dog is playing in the park\"\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74773a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        \"\"\"\n",
    "        CNN Encoder using pre-trained Inception v3\n",
    "        \n",
    "        Args:\n",
    "            embed_size: Size of feature vector (e.g., 256)\n",
    "            train_CNN: Whether to fine-tune CNN weights (usually False)\n",
    "        \"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        \n",
    "        self.train_CNN = train_CNN\n",
    "        \n",
    "        # Load pre-trained Inception v3\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        \n",
    "        # Replace final layer to output embed_size features\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        \n",
    "        # Activation and regularization\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Convert images to feature vectors\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of images (batch_size, 3, 299, 299)\n",
    "            \n",
    "        Returns:\n",
    "            features: Feature vectors (batch_size, embed_size)\n",
    "        \"\"\"\n",
    "        features = self.inception(images)\n",
    "        \n",
    "        # Freeze/unfreeze CNN weights\n",
    "        for name, param in self.inception.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True  # Always train final layer\n",
    "            else:\n",
    "                param.requires_grad = self.train_CNN  # Freeze other layers\n",
    "                \n",
    "        return self.dropout(self.relu(features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bc2e52",
   "metadata": {},
   "source": [
    "### üìä What Happens Here\n",
    "\n",
    "**1. Load Pre-trained Inception**\n",
    "```python\n",
    "self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "```\n",
    "- **Pre-trained**: Already knows how to recognize objects (trained on ImageNet)\n",
    "- **aux_logits=False**: Disables auxiliary classifier (we don't need it)\n",
    "\n",
    "**2. Replace Final Layer**\n",
    "```python\n",
    "self.inception.fc = nn.Linear(in_features, embed_size)\n",
    "```\n",
    "- Original Inception outputs 1000 classes (ImageNet)\n",
    "- We replace it to output `embed_size` features (e.g., 256)\n",
    "- These features represent the image content\n",
    "\n",
    "**3. Freeze Layers**\n",
    "```python\n",
    "param.requires_grad = self.train_CNN\n",
    "```\n",
    "- **If `train_CNN=False`**: Freeze all layers (don't update weights)\n",
    "  - Why? Pre-trained features are already good!\n",
    "  - Faster training, less memory\n",
    "- **If `train_CNN=True`**: Fine-tune the entire network\n",
    "  - Slower but potentially better accuracy\n",
    "\n",
    "**4. Output**\n",
    "```python\n",
    "return self.dropout(self.relu(features))\n",
    "```\n",
    "- Apply ReLU activation\n",
    "- Apply dropout (50% chance to zero out neurons ‚Üí prevents overfitting)\n",
    "\n",
    "### üéØ Example Flow\n",
    "\n",
    "```python\n",
    "# Input: Batch of 8 images, 3 channels (RGB), 299√ó299 pixels\n",
    "images.shape = (8, 3, 299, 299)\n",
    "\n",
    "# Pass through encoder\n",
    "features = encoder(images)\n",
    "\n",
    "# Output: Feature vectors\n",
    "features.shape = (8, 256)  # 8 images, each represented by 256 numbers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è Step 3: Build RNN Decoder (Language Part)\n",
    "\n",
    "### üçï Simple Analogy: Food Critic Writing Reviews\n",
    "\n",
    "The food critic has two jobs:\n",
    "1. **Understand the photo analysis** (feature vector from CNN)\n",
    "2. **Write review word by word**: \"This\" ‚Üí \"delicious\" ‚Üí \"pizza\" ‚Üí \"has\" ‚Üí \"fresh\" ‚Üí \"toppings\"\n",
    "\n",
    "### üîß Technical Explanation: DecoderRNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e61e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, train_CNN=False):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def caption(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(image).unsqueeze(0)\n",
    "            states = None\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoder.lstm(x, states)\n",
    "                output = self.decoder.linear(hiddens.squeeze(1))\n",
    "                predicted = output.argmax(1)\n",
    "                \n",
    "                result_caption.append(predicted.item())\n",
    "                \n",
    "                x = self.decoder.embed(predicted).unsqueeze(1)\n",
    "                \n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "                    \n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608237e9",
   "metadata": {},
   "source": [
    "```python\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        \"\"\"\n",
    "        RNN Decoder using LSTM\n",
    "        \n",
    "        Args:\n",
    "            embed_size: Size of word embeddings (matches CNN output)\n",
    "            hidden_size: Size of LSTM hidden state\n",
    "            vocab_size: Number of words in vocabulary\n",
    "            num_layers: Number of LSTM layers\n",
    "        \"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        # Word embedding layer (converts word indices to vectors)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # LSTM layer (generates sequences)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Output layer (converts LSTM output to vocabulary probabilities)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"\n",
    "        Generate caption predictions during training\n",
    "        \n",
    "        Args:\n",
    "            features: Image features from encoder (batch_size, embed_size)\n",
    "            captions: Ground truth captions (batch_size, caption_length)\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Predicted word probabilities (batch_size, caption_length, vocab_size)\n",
    "        \"\"\"\n",
    "        # 1. Convert caption words to embeddings\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        # Shape: (batch_size, caption_length, embed_size)\n",
    "        \n",
    "        # 2. Concatenate image features with caption embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "        # Shape: (batch_size, caption_length+1, embed_size)\n",
    "        # Image features act as the first \"word\"\n",
    "        \n",
    "        # 3. Pass through LSTM\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        # Shape: (batch_size, caption_length+1, hidden_size)\n",
    "        \n",
    "        # 4. Convert to vocabulary predictions\n",
    "        outputs = self.linear(hiddens)\n",
    "        # Shape: (batch_size, caption_length+1, vocab_size)\n",
    "        \n",
    "        return outputs\n",
    "```\n",
    "\n",
    "### üìä Breaking Down Each Component\n",
    "\n",
    "**1. Word Embedding Layer**\n",
    "```python\n",
    "self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "```\n",
    "- **Purpose**: Convert word indices to dense vectors\n",
    "- **Example**:\n",
    "  ```python\n",
    "  vocab_size = 10000  # 10,000 words in vocabulary\n",
    "  embed_size = 256\n",
    "  \n",
    "  # Word \"dog\" has index 452\n",
    "  word_vector = embed(452)  # Shape: (256,)\n",
    "  # Output: [0.2, 0.8, -0.5, ..., 0.3]  # 256 numbers representing \"dog\"\n",
    "  ```\n",
    "\n",
    "**2. LSTM Layer**\n",
    "```python\n",
    "self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "```\n",
    "- **Purpose**: Generate sequences by remembering context\n",
    "- **batch_first=True**: Input shape is `(batch_size, sequence_length, embed_size)`\n",
    "\n",
    "**3. Linear Layer**\n",
    "```python\n",
    "self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "```\n",
    "- **Purpose**: Convert LSTM output to word predictions\n",
    "- **Output**: Probability for each word in vocabulary\n",
    "\n",
    "### üéØ Example Flow (Training Time)\n",
    "\n",
    "**Input:**\n",
    "```python\n",
    "features.shape = (8, 256)         # 8 images, 256 features each\n",
    "captions.shape = (8, 10)          # 8 captions, 10 words each (indices)\n",
    "```\n",
    "\n",
    "**Step-by-step:**\n",
    "```python\n",
    "# 1. Embed captions\n",
    "embeddings = self.embed(captions)  # (8, 10, 256)\n",
    "\n",
    "# 2. Add image features as first \"word\"\n",
    "embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1)\n",
    "# Shape: (8, 11, 256)  # 11 = 1 image + 10 words\n",
    "\n",
    "# 3. Pass through LSTM\n",
    "hiddens, _ = self.lstm(embeddings)  # (8, 11, hidden_size)\n",
    "\n",
    "# 4. Convert to predictions\n",
    "outputs = self.linear(hiddens)  # (8, 11, 10000)\n",
    "# For each of 8 captions, for each of 11 positions, predict probability of each of 10000 words\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Step 4: Combine CNN + RNN (Complete Model)\n",
    "\n",
    "### üçï Simple Analogy: Restaurant Review System\n",
    "\n",
    "Now we connect both parts:\n",
    "1. **Photographer** analyzes the dish ‚Üí gives summary to critic\n",
    "2. **Critic** uses summary ‚Üí writes review word by word\n",
    "\n",
    "### üîß Technical Explanation: CNNtoRNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c20b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import spacy\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} #iots: index to string\n",
    "        self.stoi = {v: k for k, v in self.itos.items()} #stoi: string to index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tokenizer(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "    #        \"I love dogs\" -> ['i', 'love', 'dogs']\n",
    "    \n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 1\n",
    "                else:\n",
    "                    frequencies[word] +=1\n",
    "                \n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx +=1\n",
    "                    \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63f1689",
   "metadata": {},
   "source": [
    "```python\n",
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, train_CNN=False):\n",
    "        \"\"\"\n",
    "        Complete Image Captioning Model\n",
    "        \n",
    "        Args:\n",
    "            embed_size: Feature vector size (CNN output = RNN input)\n",
    "            hidden_size: LSTM hidden state size\n",
    "            vocab_size: Number of words in vocabulary\n",
    "            num_layers: Number of LSTM layers\n",
    "            train_CNN: Whether to fine-tune CNN\n",
    "        \"\"\"\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size, train_CNN)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "        \n",
    "    def forward(self, images, captions):\n",
    "        \"\"\"\n",
    "        Training forward pass\n",
    "        \n",
    "        Args:\n",
    "            images: Batch of images\n",
    "            captions: Ground truth captions\n",
    "            \n",
    "        Returns:\n",
    "            outputs: Predicted word probabilities\n",
    "        \"\"\"\n",
    "        features = self.encoder(images)      # Image ‚Üí features\n",
    "        outputs = self.decoder(features, captions)  # Features ‚Üí caption\n",
    "        return outputs\n",
    "    \n",
    "    def caption(self, image, vocabulary, max_length=50):\n",
    "        \"\"\"\n",
    "        Generate caption for a single image (inference time)\n",
    "        \n",
    "        Args:\n",
    "            image: Single image tensor (1, 3, 299, 299)\n",
    "            vocabulary: Vocabulary object (for word lookups)\n",
    "            max_length: Maximum caption length\n",
    "            \n",
    "        Returns:\n",
    "            result_caption: List of words\n",
    "        \"\"\"\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():  # No gradient computation (inference only)\n",
    "            # 1. Get image features\n",
    "            x = self.encoder(image).unsqueeze(0)  # (1, 1, embed_size)\n",
    "            states = None  # Initial LSTM state\n",
    "            \n",
    "            # 2. Generate words one by one\n",
    "            for _ in range(max_length):\n",
    "                # Pass through LSTM\n",
    "                hiddens, states = self.decoder.lstm(x, states)\n",
    "                \n",
    "                # Predict next word\n",
    "                output = self.decoder.linear(hiddens.squeeze(1))\n",
    "                predicted = output.argmax(1)  # Get word with highest probability\n",
    "                \n",
    "                # Add to caption\n",
    "                result_caption.append(predicted.item())\n",
    "                \n",
    "                # Use predicted word as input for next step\n",
    "                x = self.decoder.embed(predicted).unsqueeze(1)\n",
    "                \n",
    "                # Stop if we predict <EOS> (end of sentence)\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "                    \n",
    "        # Convert indices to words\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]\n",
    "```\n",
    "\n",
    "### üìä Two Modes: Training vs Inference\n",
    "\n",
    "**TRAINING MODE** (`forward` method):\n",
    "```python\n",
    "# We have ground truth captions\n",
    "images = [img1, img2, ..., img8]  # Batch of 8 images\n",
    "captions = [\"A dog ...\", \"A cat ...\", ...]  # Known captions\n",
    "\n",
    "outputs = model(images, captions)\n",
    "# Model predicts all words at once (teacher forcing)\n",
    "# Compare predictions with ground truth ‚Üí calculate loss ‚Üí update weights\n",
    "```\n",
    "\n",
    "**INFERENCE MODE** (`caption` method):\n",
    "```python\n",
    "# We DON'T know the caption\n",
    "image = single_image  # One image\n",
    "\n",
    "caption = model.caption(image, vocabulary)\n",
    "# Model generates word by word:\n",
    "# Step 1: [Image] ‚Üí \"A\"\n",
    "# Step 2: [Image, \"A\"] ‚Üí \"dog\"\n",
    "# Step 3: [Image, \"A\", \"dog\"] ‚Üí \"is\"\n",
    "# ...until <EOS> or max_length reached\n",
    "```\n",
    "\n",
    "### üéØ Inference Example\n",
    "\n",
    "```python\n",
    "# Given image of dog in park\n",
    "image = load_image(\"dog_park.jpg\")  # Shape: (1, 3, 299, 299)\n",
    "\n",
    "# Generate caption\n",
    "caption = model.caption(image, vocab, max_length=20)\n",
    "\n",
    "# Output: ['a', 'brown', 'dog', 'is', 'playing', 'in', 'the', 'park', '<EOS>']\n",
    "# As sentence: \"A brown dog is playing in the park\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Step 5: Vocabulary Class (Same as Custom Dataset)\n",
    "\n",
    "### üçï Simple Analogy: Menu with Item Numbers\n",
    "\n",
    "(See explanation in Custom_Dataset_Build.ipynb)\n",
    "\n",
    "**Quick Summary:**\n",
    "- Converts words ‚Üî numbers\n",
    "- Special tokens: `<PAD>`, `<SOS>`, `<EOS>`, `<UNK>`\n",
    "- Only includes words appearing `freq_threshold` times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fd571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d893d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì¶ Step 6: Import Dataset Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efb3fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickerDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir=root_dir\n",
    "        self.df=pd.read_csv(captions_file)\n",
    "        self.transform=transform\n",
    "        \n",
    "        self.imgs=self.df['image']\n",
    "        self.captions=self.df['caption']\n",
    "        \n",
    "        self.vocab=Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        caption=self.captions[index]\n",
    "        img_id=self.imgs[index]\n",
    "        img_path=os.path.join(self.root_dir, img_id)\n",
    "        image=Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image=self.transform(image)\n",
    "        \n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        \n",
    "        return image, torch.tensor(numericalized_caption)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ba1b6f",
   "metadata": {},
   "source": [
    "```python\n",
    "import spacy                        # Text processing\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence  # Padding variable-length sequences\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image              # Image loading\n",
    "import pandas as pd                # CSV reading\n",
    "import os                          # File paths\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üóÇÔ∏è Step 7: Flickr Dataset Class\n",
    "\n",
    "(See detailed explanation in Custom_Dataset_Build.ipynb)\n",
    "\n",
    "**Quick Summary:**\n",
    "- Loads image + caption pairs\n",
    "- Builds vocabulary from all captions\n",
    "- Returns (image_tensor, numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a4c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx=pad_idx\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        images = [item[0].unsqueeze(0) for item in batch]\n",
    "        images = torch.cat(images, dim=0)\n",
    "        captions = [item[1] for item in batch]\n",
    "        captions = pad_sequence(captions, batch_first=False, padding_value=self.pad_idx)\n",
    "        \n",
    "        return images, captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e2cb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Step 8: Custom Collate Function\n",
    "\n",
    "(See detailed explanation in Custom_Dataset_Build.ipynb)\n",
    "\n",
    "**Quick Summary:**\n",
    "- Pads captions to same length\n",
    "- Uses `<PAD>` token (index 0)\n",
    "- Returns batched images + padded captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(\n",
    "    root_folder,\n",
    "    annotation_file,\n",
    "    transform,\n",
    "    batch_size=32,\n",
    "    num_workers=2,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    dataset = FlickerDataset(\n",
    "        root_dir=root_folder,\n",
    "        captions_file=annotation_file,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb208a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé¨ Step 9: DataLoader Function\n",
    "\n",
    "**Modified to return both loader AND dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2371d56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b260d40",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_loader(...):\n",
    "    dataset = FlickerDataset(...)\n",
    "    loader = DataLoader(...)\n",
    "    return loader, dataset  # ‚úÖ Returns BOTH (needed for vocab_size)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Step 10: Complete Training Loop\n",
    "\n",
    "### üçï Simple Analogy: Learning to Describe Food\n",
    "\n",
    "You're training someone to describe dishes:\n",
    "1. **Show photo** (image)\n",
    "2. **Show correct description** (caption)\n",
    "3. **They try to describe** (model prediction)\n",
    "4. **You correct them** (calculate loss)\n",
    "5. **They improve** (update weights)\n",
    "6. **Repeat** for thousands of photos\n",
    "\n",
    "### üîß Technical Explanation: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step\n",
    "\n",
    "def train():\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "        ]\n",
    "    )\n",
    "    train_loader,dataset=get_loader(\n",
    "        root_folder=\"data/flickr8k/images\",\n",
    "        annotation_file=\"data/flickr8k/captions.txt\",\n",
    "        transform=transform,\n",
    "        batch_size=32,\n",
    "    )\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    embed_size=256\n",
    "    hidden_size=256\n",
    "    vocab_size=len(dataset.vocab)\n",
    "    num_layers=1\n",
    "    learning_rate=3e-4\n",
    "    num_epochs=10\n",
    "    load_model=False\n",
    "    save_model=True\n",
    "    \n",
    "    \n",
    "    writer=SummaryWriter(\"runs/flickr8k_experiment_1\")\n",
    "    step=0\n",
    "    \n",
    "    model=CNNtoRNN(embed_size,hidden_size,vocab_size,num_layers).to(device)\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    if load_model:\n",
    "        step=load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"),model,optimizer)\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        if save_model:\n",
    "            checkpoint={\n",
    "                \"state_dict\":model.state_dict(),\n",
    "                \"optimizer\":optimizer.state_dict(),\n",
    "                \"step\":step,\n",
    "            }\n",
    "            save_checkpoint(checkpoint,\"my_checkpoint.pth.tar\")\n",
    "            \n",
    "            \n",
    "        for idx, (imgs, captions) in enumerate(train_loader):\n",
    "            imgs=imgs.to(device)\n",
    "            captions=captions.to(device)\n",
    "            \n",
    "            outputs=model(imgs,captions[:-1])\n",
    "            \n",
    "            loss=criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]),\n",
    "                captions.reshape(-1),\n",
    "            )\n",
    "            \n",
    "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step+=1\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if idx % 100 ==0:\n",
    "                print(f\"Epoch [{epoch}/{num_epochs}] Batch {idx}/{len(train_loader)} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6cd10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ Step 10: Training Function - Complete Breakdown\n",
    "\n",
    "The training function is large, so let's break it into **digestible chunks** with explanations!\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Part 1: Setup Transforms\n",
    "\n",
    "```python\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224√ó224\n",
    "    transforms.ToTensor(),          # Convert to tensor [0, 1]\n",
    "    transforms.Normalize(           # Normalize for Inception\n",
    "        (0.485, 0.456, 0.406),     # ImageNet mean\n",
    "        (0.229, 0.224, 0.225)      # ImageNet std\n",
    "    ),\n",
    "])\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Preparing ingredients before cooking\n",
    "- **Resize**: Cut all vegetables to same size\n",
    "- **ToTensor**: Put ingredients in containers (PIL ‚Üí Tensor)\n",
    "- **Normalize**: Season consistently (match what Inception was trained on)\n",
    "\n",
    "**üîß Technical:**\n",
    "- Inception v3 was trained on ImageNet with specific normalization\n",
    "- Using the same normalization ensures best performance\n",
    "- Mean/std values are ImageNet dataset statistics\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ Part 2: Load Dataset\n",
    "\n",
    "```python\n",
    "train_loader, dataset = get_loader(\n",
    "    root_folder=\"data/flickr8k/images\",\n",
    "    annotation_file=\"data/flickr8k/captions.txt\",\n",
    "    transform=transform,\n",
    "    batch_size=32,\n",
    ")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Opening restaurant and bringing in supplies\n",
    "- **root_folder**: Kitchen pantry (where images are stored)\n",
    "- **annotation_file**: Menu with descriptions\n",
    "- **batch_size=32**: Serve 32 customers at once (not one by one)\n",
    "\n",
    "**üîß Technical:**\n",
    "- Creates DataLoader that yields batches of (images, captions)\n",
    "- Returns dataset too (we need its vocabulary)\n",
    "- Handles padding automatically via MyCollate\n",
    "\n",
    "---\n",
    "\n",
    "### üñ•Ô∏è Part 3: Setup Device\n",
    "\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Choose your vehicle\n",
    "- GPU = Sports car üèéÔ∏è (fast, for heavy work)\n",
    "- CPU = Bicycle üö≤ (slow, but always available)\n",
    "\n",
    "**üîß Technical:**\n",
    "- Checks if NVIDIA GPU is available\n",
    "- All tensors and models will move to this device\n",
    "- Training on GPU is 10-100x faster than CPU\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Part 4: Hyperparameters\n",
    "\n",
    "```python\n",
    "embed_size = 256        # Size of feature vectors\n",
    "hidden_size = 256       # Size of LSTM hidden state\n",
    "vocab_size = len(dataset.vocab)  # Number of words in vocabulary\n",
    "num_layers = 1          # Number of LSTM layers\n",
    "learning_rate = 3e-4    # Learning rate for optimizer\n",
    "num_epochs = 10         # Number of complete passes through dataset\n",
    "load_model = False      # Whether to load saved checkpoint\n",
    "save_model = True       # Whether to save checkpoints\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Recipe settings\n",
    "- **embed_size**: Size of flavor profile (how many taste dimensions)\n",
    "- **hidden_size**: Chef's memory capacity (how much context to remember)\n",
    "- **vocab_size**: Number of dishes on menu\n",
    "- **learning_rate**: How fast chef learns (big steps vs small adjustments)\n",
    "- **num_epochs**: How many times to practice entire recipe book\n",
    "\n",
    "**üîß Technical:**\n",
    "- **embed_size=256**: Matches CNN output and word embeddings\n",
    "- **hidden_size=256**: Larger = more memory but slower\n",
    "- **learning_rate=3e-4** (0.0003): Adam optimizer works well with this\n",
    "- **num_epochs=10**: Typically need 10-20 epochs for convergence\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Part 5: Setup TensorBoard\n",
    "\n",
    "```python\n",
    "writer = SummaryWriter(\"runs/flickr8k_experiment_1\")\n",
    "step = 0\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Restaurant quality tracker\n",
    "- Records how good each dish tastes over time\n",
    "- Can review later to see improvement\n",
    "\n",
    "**üîß Technical:**\n",
    "- TensorBoard logs training metrics\n",
    "- View with: `tensorboard --logdir=runs`\n",
    "- `step` counts total batches processed\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è Part 6: Create Model\n",
    "\n",
    "```python\n",
    "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Hire photographer + critic\n",
    "- Photographer (CNN) analyzes images\n",
    "- Critic (RNN) writes descriptions\n",
    "\n",
    "**üîß Technical:**\n",
    "- Creates complete model (Encoder + Decoder)\n",
    "- `.to(device)` moves model to GPU/CPU\n",
    "- Model has ~50M parameters (Inception) + ~5M (LSTM)\n",
    "\n",
    "---\n",
    "\n",
    "### üí• Part 7: Loss Function\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Quality inspector\n",
    "- Compares chef's dish with expected dish\n",
    "- Ignores placeholder dishes (padding)\n",
    "\n",
    "**üîß Technical:**\n",
    "- **CrossEntropyLoss**: Measures prediction accuracy for classification\n",
    "- **ignore_index**: Don't penalize model for `<PAD>` tokens\n",
    "- Lower loss = better predictions\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Part 8: Optimizer\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Cooking coach\n",
    "- Watches chef's mistakes\n",
    "- Suggests improvements\n",
    "\n",
    "**üîß Technical:**\n",
    "- **Adam**: Adaptive learning rate optimizer (better than SGD)\n",
    "- Updates model weights to minimize loss\n",
    "- `lr=3e-4`: Step size for weight updates\n",
    "\n",
    "---\n",
    "\n",
    "### üíæ Part 9: Load Checkpoint (Optional)\n",
    "\n",
    "```python\n",
    "if load_model:\n",
    "    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Resume from saved progress\n",
    "- Like loading a video game save file\n",
    "- Continue training from where you left off\n",
    "\n",
    "**üîß Technical:**\n",
    "- Only runs if `load_model=True`\n",
    "- Restores model weights, optimizer state, and step counter\n",
    "- Useful if training was interrupted\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Part 10: Training Mode\n",
    "\n",
    "```python\n",
    "model.train()\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Open restaurant for practice\n",
    "- Turn on all training features (dropout, batch norm)\n",
    "- vs `model.eval()` which turns them off for testing\n",
    "\n",
    "**üîß Technical:**\n",
    "- Enables dropout (randomly zeros neurons)\n",
    "- Enables batch normalization training mode\n",
    "- Required before training loop\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ Part 11: Epoch Loop\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # Save checkpoint at start of each epoch\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"step\": step,\n",
    "        }\n",
    "        save_checkpoint(checkpoint, \"my_checkpoint.pth.tar\")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Practice recipe book 10 times\n",
    "- Each epoch = one complete pass through all recipes\n",
    "- Save progress after each complete pass\n",
    "\n",
    "**üîß Technical:**\n",
    "- **Epoch**: One complete pass through entire dataset\n",
    "- Saves checkpoint at start (can resume if crash)\n",
    "- `state_dict()` contains all model weights\n",
    "\n",
    "---\n",
    "\n",
    "### üé≤ Part 12: Batch Loop\n",
    "\n",
    "```python\n",
    "for idx, (imgs, captions) in enumerate(train_loader):\n",
    "    imgs = imgs.to(device)\n",
    "    captions = captions.to(device)\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Process one table of customers at a time\n",
    "- 32 customers (batch_size=32)\n",
    "- Bring their orders to the kitchen (GPU/CPU)\n",
    "\n",
    "**üîß Technical:**\n",
    "- `enumerate(train_loader)`: Loop through batches\n",
    "- `imgs.shape = (32, 3, 224, 224)`: 32 images\n",
    "- `captions.shape = (max_len, 32)`: 32 captions (padded)\n",
    "- `.to(device)`: Move data to GPU/CPU\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Part 13: Forward Pass\n",
    "\n",
    "```python\n",
    "outputs = model(imgs, captions[:-1])\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Chef cooks the dish\n",
    "- Look at ingredients (images)\n",
    "- Follow recipe (captions[:-1])\n",
    "- Produce dish (outputs)\n",
    "\n",
    "**üîß Technical:**\n",
    "- **Input**: Images + captions (without last word)\n",
    "- **Output**: Predicted words for each position\n",
    "- **Shape**: `(32, max_len, vocab_size)` = probability for each word\n",
    "- **Teacher forcing**: Use ground truth words during training\n",
    "\n",
    "**Why `captions[:-1]`?**\n",
    "```python\n",
    "Original caption: [<SOS>, \"A\", \"dog\", \"running\", <EOS>]\n",
    "Feed to model:    [<SOS>, \"A\", \"dog\", \"running\"]        # Remove last\n",
    "Model predicts:   [\"A\",   \"dog\", \"running\", <EOS>]      # Predict next word\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üí• Part 14: Calculate Loss\n",
    "\n",
    "```python\n",
    "loss = criterion(\n",
    "    outputs.reshape(-1, outputs.shape[2]),  # Flatten to (batch*seq, vocab)\n",
    "    captions.reshape(-1),                    # Flatten to (batch*seq)\n",
    ")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Judge tastes dish and scores it\n",
    "- Compare chef's output with expected recipe\n",
    "- Lower score = better match\n",
    "\n",
    "**üîß Technical:**\n",
    "- **Reshape outputs**: `(32, 15, 10000)` ‚Üí `(480, 10000)`\n",
    "  - 32 captions √ó 15 words = 480 predictions\n",
    "  - Each prediction has 10000 possibilities (vocab_size)\n",
    "- **Reshape captions**: `(32, 15)` ‚Üí `(480,)`\n",
    "  - 480 ground truth word indices\n",
    "- **CrossEntropyLoss**: Compares predicted probabilities with true words\n",
    "- **Result**: Single number (e.g., 2.5) - lower is better\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Part 15: Log to TensorBoard\n",
    "\n",
    "```python\n",
    "writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "step += 1\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Write in quality log book\n",
    "- Record: \"Batch #1234, Loss = 2.5\"\n",
    "- Track improvement over time\n",
    "\n",
    "**üîß Technical:**\n",
    "- Logs loss value for visualization\n",
    "- `step` counts total batches (not just per epoch)\n",
    "- View graph: `tensorboard --logdir=runs`\n",
    "\n",
    "---\n",
    "\n",
    "### üîÑ Part 16: Backward Pass (Learning!)\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()  # Reset gradients\n",
    "loss.backward()        # Calculate gradients\n",
    "optimizer.step()       # Update weights\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Coach gives feedback and chef improves\n",
    "1. **zero_grad()**: Clear old feedback notes\n",
    "2. **loss.backward()**: Calculate: \"What went wrong and by how much?\"\n",
    "3. **optimizer.step()**: Chef adjusts technique\n",
    "\n",
    "**üîß Technical:**\n",
    "1. **zero_grad()**: Clear gradient buffers (they accumulate by default)\n",
    "2. **loss.backward()**: Backpropagation - calculates gradient for each parameter\n",
    "3. **optimizer.step()**: Updates weights using gradients:\n",
    "   ```python\n",
    "   weight_new = weight_old - learning_rate * gradient\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### üñ®Ô∏è Part 17: Print Progress\n",
    "\n",
    "```python\n",
    "if idx % 100 == 0:\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] Batch {idx}/{len(train_loader)} Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "**üçï Analogy:** Check progress every 100 dishes\n",
    "- Don't print after every dish (too much!)\n",
    "- Check occasionally to ensure quality improving\n",
    "\n",
    "**üîß Technical:**\n",
    "- Prints every 100 batches\n",
    "- Shows: which epoch, which batch, current loss\n",
    "- Example output:\n",
    "  ```\n",
    "  Epoch [0/10] Batch 0/250 Loss: 9.2134\n",
    "  Epoch [0/10] Batch 100/250 Loss: 4.5678\n",
    "  Epoch [0/10] Batch 200/250 Loss: 3.1234\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## üéä Complete Training Flow Summary\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ FOR EACH EPOCH (10 times)                           ‚îÇ\n",
    "‚îÇ  ‚îú‚îÄ Save checkpoint                                 ‚îÇ\n",
    "‚îÇ  ‚îÇ                                                   ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ FOR EACH BATCH (250 batches)                   ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 1. Get 32 images + captions                ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 2. Move to GPU                             ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 3. Forward pass (predict captions)         ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 4. Calculate loss (how wrong?)             ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 5. Log to TensorBoard                      ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 6. Backward pass (calculate gradients)     ‚îÇ\n",
    "‚îÇ      ‚îú‚îÄ 7. Update weights (improve model)          ‚îÇ\n",
    "‚îÇ      ‚îî‚îÄ 8. Print progress (every 100 batches)      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚è±Ô∏è Training Timeline Example\n",
    "\n",
    "**Dataset:** 8,000 image-caption pairs, batch_size=32\n",
    "\n",
    "**One Epoch:**\n",
    "- Number of batches = 8,000 / 32 = 250 batches\n",
    "- Time per batch ‚âà 0.5 seconds (on GPU)\n",
    "- **Total time per epoch** ‚âà 125 seconds ‚âà **2 minutes**\n",
    "\n",
    "**Complete Training (10 epochs):**\n",
    "- **Total time** ‚âà 10 √ó 2 min = **20 minutes** (on GPU)\n",
    "- On CPU: 10-20x slower ‚âà **3-6 hours**\n",
    "\n",
    "**Expected Loss Progress:**\n",
    "```\n",
    "Epoch 0: Loss starts at ~9.0\n",
    "Epoch 1: Loss drops to ~5.0\n",
    "Epoch 3: Loss drops to ~3.0\n",
    "Epoch 5: Loss drops to ~2.0\n",
    "Epoch 10: Loss around ~1.5 (good!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Important Notes\n",
    "\n",
    "### üíæ Checkpoint Saving\n",
    "- Saves after each epoch (every ~2 minutes)\n",
    "- Can resume if training crashes\n",
    "- File: `my_checkpoint.pth.tar` (~200MB)\n",
    "\n",
    "### üéØ Teacher Forcing\n",
    "- During training: Use ground truth words (faster learning)\n",
    "- During inference: Use model's own predictions (more realistic)\n",
    "\n",
    "### üìä TensorBoard Monitoring\n",
    "```bash\n",
    "# In terminal:\n",
    "tensorboard --logdir=runs\n",
    "\n",
    "# Open browser:\n",
    "http://localhost:6006\n",
    "```\n",
    "\n",
    "### üö® Common Issues\n",
    "1. **Out of memory**: Reduce batch_size (32 ‚Üí 16)\n",
    "2. **Loss not decreasing**: Check learning rate (try 1e-4)\n",
    "3. **NaN loss**: Gradient explosion (add gradient clipping)\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Key Takeaways\n",
    "\n",
    "### ‚úÖ Training is a Loop\n",
    "```python\n",
    "for epoch in epochs:\n",
    "    for batch in batches:\n",
    "        predict ‚Üí calculate loss ‚Üí backpropagate ‚Üí update weights\n",
    "```\n",
    "\n",
    "### ‚úÖ Three Key Steps\n",
    "1. **Forward pass**: Model makes predictions\n",
    "2. **Loss calculation**: Measure how wrong\n",
    "3. **Backward pass**: Learn from mistakes\n",
    "\n",
    "### ‚úÖ Monitoring\n",
    "- Loss should decrease over time\n",
    "- Save checkpoints regularly\n",
    "- Use TensorBoard for visualization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ After Training\n",
    "\n",
    "Once training completes:\n",
    "\n",
    "```python\n",
    "# Load best checkpoint\n",
    "model.load_state_dict(torch.load(\"my_checkpoint.pth.tar\")[\"state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Generate caption for new image\n",
    "image = load_and_transform_image(\"dog.jpg\")\n",
    "caption = model.caption(image, vocabulary)\n",
    "print(\" \".join(caption))\n",
    "# Output: \"a brown dog is playing with a ball in the park\"\n",
    "```\n",
    "\n",
    "**Congratulations! You've trained an image captioning model! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
