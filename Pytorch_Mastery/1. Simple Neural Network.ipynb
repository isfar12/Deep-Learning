{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3587e9",
   "metadata": {},
   "source": [
    "# PyTorch Mastery — 1. Simple Neural Network\n",
    "\n",
    "Welcome! In this mini-tutorial, we build a simple fully connected (feed-forward) neural network for binary classification using synthetic data. The flow:\n",
    "\n",
    "1) Import libraries\n",
    "2) Generate a toy dataset\n",
    "3) Train/test split and feature scaling\n",
    "4) Convert to PyTorch tensors\n",
    "5) Create TensorDataset and DataLoader\n",
    "6) Define a small neural network\n",
    "7) Choose loss function and optimizer\n",
    "8) Train the model\n",
    "9) Evaluate accuracy on the test set\n",
    "\n",
    "Skim the code, then read the explanation cells that follow each code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf88a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708b5df",
   "metadata": {},
   "source": [
    "## Imports explained\n",
    "\n",
    "- `import torch`: Core PyTorch library for tensors and autograd.\n",
    "- `import torch.nn as nn`: Neural network building blocks (layers, loss functions).\n",
    "- `import torch.optim as optim`: Optimizers like SGD/Adam for updating weights.\n",
    "- `import torch.nn.functional as F`: Functional API (we’ll use `torch.relu` directly here, F also has activations).\n",
    "- `from torch.utils.data import TensorDataset, DataLoader, Dataset`:\n",
    "  - `TensorDataset`: Wraps feature and label tensors into indexable pairs.\n",
    "  - `DataLoader`: Batches and shuffles data, and iterates efficiently.\n",
    "  - `Dataset`: Base class to build custom datasets (e.g., `MyDataset`) when you need per-item logic, transforms, or reading from disk.\n",
    "- `from sklearn.datasets import make_classification`: Generates a synthetic classification dataset.\n",
    "- `from sklearn.model_selection import train_test_split`: Splits data into train and test sets.\n",
    "- `from sklearn.preprocessing import StandardScaler`: Standardizes features to zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf88a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=10, n_informative=5, \n",
    "    n_redundant=2, n_classes=2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4849c58",
   "metadata": {},
   "source": [
    "## Create a synthetic dataset\n",
    "\n",
    "- `make_classification(...)` creates `X` (features) and `y` (labels) for a binary classification problem.\n",
    "- `n_samples=1000`: number of rows.\n",
    "- `n_features=10`: total input features.\n",
    "- `n_informative=5`: features that actually matter for the classes.\n",
    "- `n_redundant=2`: linear combinations of informative features.\n",
    "- `n_classes=2`: binary classification.\n",
    "- `random_state=42`: makes the random generation reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce893a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.125100</td>\n",
       "      <td>1.178124</td>\n",
       "      <td>0.493516</td>\n",
       "      <td>0.790880</td>\n",
       "      <td>-0.614278</td>\n",
       "      <td>1.347020</td>\n",
       "      <td>1.419515</td>\n",
       "      <td>1.357325</td>\n",
       "      <td>0.966041</td>\n",
       "      <td>-1.981139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.564641</td>\n",
       "      <td>3.638629</td>\n",
       "      <td>-1.522415</td>\n",
       "      <td>-1.541705</td>\n",
       "      <td>1.616697</td>\n",
       "      <td>4.781310</td>\n",
       "      <td>3.190292</td>\n",
       "      <td>-0.890254</td>\n",
       "      <td>1.438826</td>\n",
       "      <td>-3.828748</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.516313</td>\n",
       "      <td>2.165426</td>\n",
       "      <td>-0.628486</td>\n",
       "      <td>-0.386923</td>\n",
       "      <td>0.492518</td>\n",
       "      <td>1.442381</td>\n",
       "      <td>1.332905</td>\n",
       "      <td>-1.958175</td>\n",
       "      <td>-0.348803</td>\n",
       "      <td>-1.804124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.537282</td>\n",
       "      <td>0.966618</td>\n",
       "      <td>-0.115420</td>\n",
       "      <td>0.670755</td>\n",
       "      <td>-0.958516</td>\n",
       "      <td>0.871440</td>\n",
       "      <td>0.508186</td>\n",
       "      <td>-1.034471</td>\n",
       "      <td>-1.654176</td>\n",
       "      <td>-1.910503</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.278385</td>\n",
       "      <td>1.065828</td>\n",
       "      <td>-1.724917</td>\n",
       "      <td>-2.235667</td>\n",
       "      <td>0.715107</td>\n",
       "      <td>0.731249</td>\n",
       "      <td>-0.674119</td>\n",
       "      <td>0.598330</td>\n",
       "      <td>-0.524283</td>\n",
       "      <td>1.047610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0   1.125100   1.178124   0.493516   0.790880  -0.614278   1.347020   \n",
       "1  -0.564641   3.638629  -1.522415  -1.541705   1.616697   4.781310   \n",
       "2   0.516313   2.165426  -0.628486  -0.386923   0.492518   1.442381   \n",
       "3   0.537282   0.966618  -0.115420   0.670755  -0.958516   0.871440   \n",
       "4   0.278385   1.065828  -1.724917  -2.235667   0.715107   0.731249   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  target  \n",
       "0   1.419515   1.357325   0.966041  -1.981139       1  \n",
       "1   3.190292  -0.890254   1.438826  -3.828748       0  \n",
       "2   1.332905  -1.958175  -0.348803  -1.804124       0  \n",
       "3   0.508186  -1.034471  -1.654176  -1.910503       1  \n",
       "4  -0.674119   0.598330  -0.524283   1.047610       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataframe=pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])\n",
    "dataframe['target'] = y\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "771176b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf762e9b",
   "metadata": {},
   "source": [
    "## Split into train and test\n",
    "\n",
    "- `train_test_split(...)` splits arrays into training and testing subsets.\n",
    "- `test_size=0.2`: 20% of data goes to testing.\n",
    "- `random_state=42`: reproducible split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8412f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43acde1",
   "metadata": {},
   "source": [
    "## Standardize features\n",
    "\n",
    "- `StandardScaler` shifts each feature to mean 0 and scales to unit variance.\n",
    "- Fit on `X_train` only, then apply the learned scaling to both train and test.\n",
    "- This prevents train/test leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad7ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbd35a",
   "metadata": {},
   "source": [
    "## Convert NumPy arrays to PyTorch tensors\n",
    "\n",
    "- `torch.FloatTensor(X_train)`: features as 32-bit floats (required for `nn.Linear`).\n",
    "- `torch.LongTensor(y_train)`: labels as integer class indices (required by `CrossEntropyLoss`).\n",
    "- Do the same for test data.\n",
    "\n",
    "Note: `CrossEntropyLoss` expects raw, unnormalized scores (logits) as `FloatTensor` and target labels as `LongTensor` with values in `[0, num_classes-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d32198d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ec2f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Initialize dataset with features and labels.\n",
    "        Converts numpy arrays to PyTorch tensors.\n",
    "        \"\"\"\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieve a single sample at index `idx`.\n",
    "        Must return (feature, label).\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d66206",
   "metadata": {},
   "source": [
    "## Optional: Custom Dataset class (`MyDataset`)\n",
    "\n",
    "This custom dataset mirrors what `TensorDataset` does for in-memory tensors, but gives you flexibility to add preprocessing, on-the-fly transforms, or load from disk.\n",
    "\n",
    "- `__init__(self, X, y)`: stores features and labels as tensors (`FloatTensor` for features, `LongTensor` for integer class labels).\n",
    "- `__len__(self)`: returns the total number of samples so the `DataLoader` knows how many batches to create.\n",
    "- `__getitem__(self, idx)`: returns a single `(feature, label)` pair at index `idx`. You could augment data, normalize per item, or apply custom logic here.\n",
    "\n",
    "How to use it instead of `TensorDataset`:\n",
    "\n",
    "- Replace:\n",
    "  - `train_dataset = TensorDataset(X_train_tensor, y_train_tensor)`\n",
    "  - `test_dataset = TensorDataset(X_test_tensor, y_test_tensor)`\n",
    "- With:\n",
    "  - `train_dataset = MyDataset(X_train, y_train)`\n",
    "  - `test_dataset = MyDataset(X_test, y_test)`\n",
    "\n",
    "Note: In this version `MyDataset` converts NumPy arrays to tensors inside `__init__`. If you already have tensors (like `X_train_tensor`), you could modify `MyDataset` to accept tensors directly to avoid double conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6f885",
   "metadata": {},
   "source": [
    "## Create datasets and data loaders\n",
    "\n",
    "- Option A (used in this notebook):\n",
    "  - `TensorDataset(features, labels)`: pairs up tensors so each index returns `(X[i], y[i])`.\n",
    "  - `DataLoader(..., batch_size=32, shuffle=True)`: mini-batch iterator; shuffles training data each epoch to improve generalization.\n",
    "- Option B (optional custom dataset):\n",
    "  - Use `MyDataset(X, y)` when you need to apply transforms or custom logic in `__getitem__`.\n",
    "  - `DataLoader(MyDataset(...), batch_size=32, shuffle=True)` works the same.\n",
    "\n",
    "The test loader typically does not need shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb0ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)  # 2 classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet(input_dim=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d437e",
   "metadata": {},
   "source": [
    "## Define the neural network\n",
    "\n",
    "- Subclass `nn.Module` to define layers and the forward pass.\n",
    "- `fc1`: Linear layer mapping input_dim → 32 hidden units.\n",
    "- `fc2`: Linear layer mapping 32 → 16.\n",
    "- `fc3`: Linear layer mapping 16 → 2 output logits (for 2 classes).\n",
    "- In `forward`:\n",
    "  - Apply `ReLU` after `fc1` and `fc2` for non-linearity.\n",
    "  - `fc3` returns raw scores (logits). Don’t apply `softmax` here because `CrossEntropyLoss` handles it internally.\n",
    "- Instantiate the model with `input_dim=10` to match `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca398f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0f120",
   "metadata": {},
   "source": [
    "## Loss function and optimizer\n",
    "\n",
    "- `nn.CrossEntropyLoss()`:\n",
    "  - Expects logits of shape `(batch_size, num_classes)` and targets of shape `(batch_size,)` with class indices.\n",
    "  - Internally applies `log_softmax` + `nll_loss`.\n",
    "- `optim.Adam(model.parameters(), lr=0.001)`: Adam optimizer with a small learning rate for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb7e1032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.6911\n",
      "Epoch [2/20], Loss: 0.6419\n",
      "Epoch [3/20], Loss: 0.5868\n",
      "Epoch [4/20], Loss: 0.5190\n",
      "Epoch [5/20], Loss: 0.4507\n",
      "Epoch [6/20], Loss: 0.3891\n",
      "Epoch [7/20], Loss: 0.3462\n",
      "Epoch [8/20], Loss: 0.3207\n",
      "Epoch [9/20], Loss: 0.3006\n",
      "Epoch [10/20], Loss: 0.2865\n",
      "Epoch [11/20], Loss: 0.2712\n",
      "Epoch [12/20], Loss: 0.2595\n",
      "Epoch [13/20], Loss: 0.2481\n",
      "Epoch [14/20], Loss: 0.2376\n",
      "Epoch [15/20], Loss: 0.2283\n",
      "Epoch [16/20], Loss: 0.2192\n",
      "Epoch [17/20], Loss: 0.2122\n",
      "Epoch [18/20], Loss: 0.2050\n",
      "Epoch [19/20], Loss: 0.1998\n",
      "Epoch [20/20], Loss: 0.1932\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e595cb85",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "- `epochs = 20`: number of full passes through the training data.\n",
    "- `model.train()`: sets the model to training mode (affects layers like dropout/batchnorm; none here but good practice).\n",
    "- For each batch:\n",
    "  - `optimizer.zero_grad()`: clears old gradients.\n",
    "  - `outputs = model(X_batch)`: forward pass to get logits.\n",
    "  - `loss = criterion(outputs, y_batch)`: compute loss.\n",
    "  - `loss.backward()`: backpropagates gradients.\n",
    "  - `optimizer.step()`: updates weights.\n",
    "  - Track and print average loss per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c6de6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 94.00%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5f1b1",
   "metadata": {},
   "source": [
    "## Evaluation on the test set\n",
    "\n",
    "- `model.eval()`: evaluation mode (disables dropout/batchnorm behaviors if present).\n",
    "- `torch.no_grad()`: turns off gradient tracking for faster inference and lower memory use.\n",
    "- For each batch in `test_loader`:\n",
    "  - `outputs = model(X_batch)`: logits.\n",
    "  - `_, predicted = torch.max(outputs, 1)`: class with highest logit per sample.\n",
    "  - Accumulate `correct` and `total` to compute accuracy.\n",
    "- Print final test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aec84b7",
   "metadata": {},
   "source": [
    "## Wrap-up and next steps\n",
    "\n",
    "You trained a small feed-forward neural network on a synthetic binary classification dataset. Key takeaways:\n",
    "\n",
    "- Standardize features for stable training.\n",
    "- Use `CrossEntropyLoss` for multi-class logits with integer labels.\n",
    "- Keep the model simple first; verify it learns, then iterate.\n",
    "\n",
    "Try these extensions:\n",
    "\n",
    "- Add `nn.Dropout` or `nn.BatchNorm1d` between layers.\n",
    "- Tune hyperparameters: `hidden sizes`, `epochs`, `learning rate`, `batch size`.\n",
    "- Add an `accuracy` calculation during training.\n",
    "- Plot the training loss curve and confusion matrix for the test set.\n",
    "- Switch to a real dataset (e.g., from your `Dataset/` folder) and adjust `input_dim` accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
