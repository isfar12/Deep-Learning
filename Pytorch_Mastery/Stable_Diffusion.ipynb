{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4437791d",
   "metadata": {},
   "source": [
    "Stable Diffusion is a **deep learning model** used for **generating images** from text.\n",
    "It became very famous because the model weights are open-source and people can run it on normal GPUs (even consumer GPUs).\n",
    "\n",
    "---\n",
    "\n",
    "## What it is (in simple words)\n",
    "\n",
    "Stable Diffusion is a type of **text-to-image generative AI model**.\n",
    "\n",
    "You write a prompt → it creates a picture.\n",
    "\n",
    "Example:\n",
    "\n",
    "> “a realistic tiger sitting on a chair in a classroom”\n",
    "\n",
    "→ Stable Diffusion will generate that image.\n",
    "\n",
    "---\n",
    "\n",
    "## What type of model is it?\n",
    "\n",
    "It is a **Latent Diffusion Model (LDM)**.\n",
    "\n",
    "This is a sub-type of “Diffusion Models”.\n",
    "\n",
    "---\n",
    "\n",
    "## How it works (simple explanation)\n",
    "\n",
    "It works in 3 main stages:\n",
    "\n",
    "| Step                                           | Meaning                                        |\n",
    "| ---------------------------------------------- | ---------------------------------------------- |\n",
    "| 1) Start with random noise                     | The model begins from pure random pixels       |\n",
    "| 2) Gradually remove the noise                  | It learns to “denoise” step-by-step            |\n",
    "| 3) Turn hidden representation into final image | The latent representation becomes a full image |\n",
    "\n",
    "### More clearly:\n",
    "\n",
    "* The text prompt is encoded into vectors using **CLIP text encoder**\n",
    "* That text embedding guides the model on what kind of image to form\n",
    "* The diffusion model slowly removes noise to match the meaning of the text\n",
    "* A decoder (VAE decoder) converts the final latent representation into an actual pixel image\n",
    "\n",
    "---\n",
    "\n",
    "## Why is it called “latent”?\n",
    "\n",
    "Because the diffusion happens in **latent space** (compressed representation) instead of full resolution pixels.\n",
    "\n",
    "This makes it:\n",
    "\n",
    "* faster\n",
    "* cheaper to run\n",
    "* possible to run on less powerful GPUs (e.g. RTX 3060)\n",
    "\n",
    "GANs used pixel space → very heavy\n",
    "Stable Diffusion uses latent space → much lighter\n",
    "\n",
    "---\n",
    "\n",
    "## Why is Stable Diffusion popular?\n",
    "\n",
    "| Reason       | Explanation                              |\n",
    "| ------------ | ---------------------------------------- |\n",
    "| Open-source  | people can download weights and finetune |\n",
    "| Good quality | images look artistic & realistic         |\n",
    "| Customizable | LoRA, DreamBooth, Textual Inversion      |\n",
    "| Runs locally | does not need huge servers like DALL-E   |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary in one sentence\n",
    "\n",
    "Stable Diffusion is a latent diffusion model that turns text into images by starting from random noise and step-by-step removing noise guided by a text embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f524732f",
   "metadata": {},
   "source": [
    "### What are “Generative Models”?\n",
    "\n",
    "In machine learning, **generative models** are models that can **generate new data** that looks similar to the real data they were trained on.\n",
    "\n",
    "Example:\n",
    "\n",
    "* If trained on images → they can generate new images\n",
    "* If trained on text → they can generate new text sentences\n",
    "* If trained on music → they can generate new music\n",
    "\n",
    "They learn the **distribution** of the data, and then **sample** from it.\n",
    "\n",
    "---\n",
    "\n",
    "### So, diffusion models are one type of generative model\n",
    "\n",
    "Other types of generative models include:\n",
    "\n",
    "| Type of Generative Model | Example models                 |\n",
    "| ------------------------ | ------------------------------ |\n",
    "| GANs                     | StyleGAN, CycleGAN             |\n",
    "| VAEs                     | Variational Autoencoders       |\n",
    "| Autoregressive           | GPT, PixelRNN                  |\n",
    "| Flow based               | Glow                           |\n",
    "| Diffusion Models         | DDPM, Stable Diffusion, Imagen |\n",
    "\n",
    "---\n",
    "\n",
    "### What is special about diffusion models?\n",
    "\n",
    "Diffusion models create data in two steps:\n",
    "\n",
    "1. **Forward process**: take real image → gradually add noise → until becomes pure noise\n",
    "2. **Reverse process**: learn to remove noise step-by-step → become real image again\n",
    "\n",
    "After training, model starts from random noise and removes noise → to generate a fresh new image that never existed before.\n",
    "\n",
    "So diffusion models learn the reverse denoising process.\n",
    "\n",
    "---\n",
    "\n",
    "### Why are they good?\n",
    "\n",
    "| Advantage             | Meaning                                            |\n",
    "| --------------------- | -------------------------------------------------- |\n",
    "| Stable to train       | GANs are very hard to train (mode collapse)        |\n",
    "| Very realistic images | Good global coherence                              |\n",
    "| Easy to condition     | You can add text, segmentation map, depth map, etc |\n",
    "\n",
    "This is why diffusion models are dominating image generation today (Stable Diffusion, Imagen, DALL-E 3 internally are also diffusion-based).\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Generative models** = models that can generate new samples.\n",
    "\n",
    "**Diffusion models** = one category of generative models that generate data by **starting with noise and denoising it step-by-step**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ea7200",
   "metadata": {},
   "source": [
    "\n",
    "## Why we think in “probability distribution”?\n",
    "\n",
    "Because in real world things are **not fixed**, they are **varied**.\n",
    "\n",
    "Example:\n",
    "\n",
    "* People's ages vary\n",
    "* People's heights vary\n",
    "\n",
    "So a model needs to learn **how likely** things are.\n",
    "\n",
    "It should learn:\n",
    "\n",
    "* which values are common\n",
    "* which values are rare\n",
    "* which values almost never happen\n",
    "\n",
    "For example: a 3-year-old child being 130 cm tall is very unlikely → so the probability of that combination should be very low.\n",
    "\n",
    "So a generative model learns the **joint probability** of all variables together.\n",
    "\n",
    "In images: every pixel also has probability.\n",
    "But pixels are connected together → so many variables depend on each other.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is this important?\n",
    "\n",
    "Because if the model learns this distribution very well, then:\n",
    "\n",
    "* we can *sample* from it (like throwing a special weighted coin)\n",
    "\n",
    "Sampling means: randomly selecting values based on how probable they are.\n",
    "\n",
    "→ This gives us **new realistic data**.\n",
    "\n",
    "---\n",
    "\n",
    "## How this connects to Stable Diffusion:\n",
    "\n",
    "Stable Diffusion has been trained on huge amount of images and learns a very complex probability distribution of all images (pixels, shapes, patterns, etc).\n",
    "\n",
    "After training:\n",
    "\n",
    "* We “sample” from the distribution\n",
    "* We get new images that look realistic\n",
    "* But the images are **new** (not taken from training data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1a0cb",
   "metadata": {},
   "source": [
    "<img src=\"asset/sd_forward_reverse.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11247f7b",
   "metadata": {},
   "source": [
    "## 1) Forward Process (Diffusion)\n",
    "\n",
    "**Idea / example:**\n",
    "We take a real image → and keep adding small random noise step by step → until it becomes pure noise (like adding more snow static on a TV).\n",
    "\n",
    "**Why this step exists:**\n",
    "So we get clean + noisy pairs.\n",
    "This teaches the model what “noise destroying image” looks like.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Reverse Process (Diffusion)\n",
    "\n",
    "**Idea / example:**\n",
    "Now the model learns the opposite direction → how to remove noise step by step → until the image becomes clear.\n",
    "\n",
    "**Why this step matters:**\n",
    "During generation, we start from pure noise and go backwards (step-by-step denoising) to create a brand new image that never existed before.\n",
    "\n",
    "---\n",
    "\n",
    "## 3) VAE + Latent Space\n",
    "\n",
    "**Idea / example:**\n",
    "Images are huge and messy in pixel space.\n",
    "VAE compresses images into a **small hidden space** where similar images are mapped close together.\n",
    "\n",
    "Think of this like: instead of working on full 4K photos, we work on a tiny meaningful 64×64 version.\n",
    "This smaller space is smooth: small changes in this space = smooth visual changes in image (not random garbage pixels).\n",
    "\n",
    "<img src=\"asset/sd_vae_concept.png\" width=800>\n",
    "\n",
    "**Why Stable Diffusion needs VAE:**\n",
    "Diffusion happens in this **latent space** (not pixels).\n",
    "This makes generation faster, cheaper, and more stable.\n",
    "After denoising is done, the VAE decoder converts the final latent back to the full image.\n",
    "\n",
    "---\n",
    "\n",
    "### Final One-Line Summary\n",
    "\n",
    "Forward adds noise to learn destruction → Reverse removes noise to generate images → VAE gives a smooth small “latent world” where diffusion happens efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875cd2ef",
   "metadata": {},
   "source": [
    "## Text-to-Image Model\n",
    "\n",
    "<img src=\"asset/sd_text_to_image.png\" width=800>\n",
    "\n",
    "This is the architecture for a text-to-image diffusion model.\n",
    "\n",
    "It works by taking random noise and a text prompt (encoded by CLIP). A U-Net model then iteratively denoises the noise for 'T' steps, using the text prompt and time embeddings as guidance, until a final Decoder converts the refined data into the output image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3373b4f",
   "metadata": {},
   "source": [
    "## Image-to-Image Model\n",
    "\n",
    "<img src=\"asset/sd_image_to_image.png\" width=800>\n",
    "\n",
    "This diagram shows an **Image-to-Image** diffusion architecture.\n",
    "\n",
    "It's similar to the Text-to-Image model, but instead of starting with random noise, it **encodes an input image (X)** into a latent representation (Z) and **adds noise** to it. This noised latent is then iteratively refined by the U-Net, guided by the **text prompt** (from CLIP). This process allows the model to **modify an existing image** based on the prompt, rather than generating one from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ddef69",
   "metadata": {},
   "source": [
    "## In-Painting Model\n",
    "\n",
    "<img src=\"asset/sd_image_edit.png\" width=800 >\n",
    "\n",
    "This diagram shows an **In-Painting** architecture.\n",
    "\n",
    "This model is designed to **regenerate or change only a specific, masked portion** of an image while keeping the rest untouched.\n",
    "\n",
    "It works by:\n",
    "1.  Encoding the original image (X) and the text prompt (\"A dog running\").\n",
    "2.  Running the standard diffusion process (U-Net) guided by the text prompt.\n",
    "3.  **Crucially**, at each denoising step, it **combines** the model's current output with the **original, unmasked parts** of the image (using a noised version of the masked image).\n",
    "\n",
    "This process \"fools\" the model by forcing it to preserve the unmasked areas, ensuring that it only generates new content (making the dog run) *inside* the specified mask, effectively replacing the original content (the ball) in that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab929f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a0f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Residual(nn.Module):\n",
    "    def __init__(self, in_channels,out_channels):\n",
    "        super().__init__()\n",
    "        self.groupnorm_1 = nn.GroupNorm(num_groups=32,in_channels=in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.groupnorm_2 = nn.GroupNorm(num_groups=32,in_channels=out_channels)\n",
    "        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        if in_channels == out_channels:\n",
    "            self.residual_layer = nn.Identity()\n",
    "        else:\n",
    "            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        residue=x\n",
    "        \n",
    "        x = self.groupnorm_1(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.groupnorm_2(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x += self.residual_layer(residue)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "867127b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_AttentionBlock(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, stride=2, padding=1),\n",
    "            \n",
    "            VAE_Residual(128, 128),\n",
    "            VAE_Residual(128, 128),\n",
    "            \n",
    "            nn.Conv2d(128,128, kernel_size=3, stride=2, padding=0),\n",
    "            \n",
    "            VAE_Residual(128, 256),\n",
    "            VAE_Residual(256, 256),\n",
    "            \n",
    "            nn.Conv2d(256,256, kernel_size=3, stride=2, padding=0),\n",
    "            \n",
    "            VAE_Residual(256, 512),\n",
    "            VAE_Residual(512, 512),\n",
    "            \n",
    "            nn.Conv2d(512,512, kernel_size=3, stride=2, padding=0),\n",
    "            \n",
    "            VAE_Residual(512, 512),\n",
    "            VAE_Residual(512, 512),\n",
    "            VAE_Residual(512, 512),\n",
    "            \n",
    "            VAE_AttentionBlock(512),\n",
    "            \n",
    "            VAE_Residual(512, 512),\n",
    "            \n",
    "            nn.GroupNorm(num_groups=32,in_channels=512),\n",
    "            \n",
    "            nn.SiLU(),\n",
    "            \n",
    "            nn.Conv2d(512, 8, kernel_size=3, padding=1),\n",
    "            nn.Conv2d(8, 8, kernel_size=1,padding=0)\n",
    "        )\n",
    "        def forward(self,x:torch.Tensor,noise_level:torch.Tensor):\n",
    "            for module in self:\n",
    "                if getattr(module,\"stride\",None)==(2,2):\n",
    "                    x=F.pad(x, (1,0,1,0))\n",
    "                x=module(x)\n",
    "            mean, log_variance = torch.chunk(x, 2, dim=1)\n",
    "            \n",
    "            log_variance = torch.clamp(log_variance, min=-30.0, max=20.0)\n",
    "            \n",
    "            variance=log_variance.exp()\n",
    "            \n",
    "            stdev = torch.sqrt(variance)\n",
    "            \n",
    "            x=mean +stdev * noise_level\n",
    "            \n",
    "            x*=.18215\n",
    "            \n",
    "            return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
