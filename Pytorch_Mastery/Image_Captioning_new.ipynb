{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6a072b",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60d7205b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\LENOVO\\.cache\\kagglehub\\datasets\\adityajn105\\flickr8k\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Import kagglehub to download the Flickr8k dataset\n",
    "import kagglehub\n",
    "\n",
    "# Download the Flickr8k dataset (8,000 images with 5 captions each)\n",
    "# Returns the local path where the dataset is stored\n",
    "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1f838",
   "metadata": {},
   "source": [
    "# Configure VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4b0ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Load pre-trained VGG16 model with ImageNet weights\n",
    "# VGG16 is a CNN trained on millions of images, good for feature extraction\n",
    "model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "\n",
    "# Replace the last layer (classification layer) with Identity\n",
    "# This removes the 1000-class classifier and keeps the 4096-dim feature vector\n",
    "model.classifier[-1] = torch.nn.Identity()\n",
    "\n",
    "# Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ccfa4",
   "metadata": {},
   "source": [
    "**What This Code Does:**\n",
    "\n",
    "We're setting up VGG16, a pre-trained image recognition model, to extract features from our images.\n",
    "\n",
    "**Simple Explanation:**\n",
    "1. **Load VGG16**: We use a model that's already trained to recognize images (trained on 14 million images)\n",
    "2. **Remove Classification Layer**: VGG16 normally classifies images into 1000 categories (like \"dog\", \"cat\"). We don't need that, so we remove it\n",
    "3. **Keep Feature Extractor**: Now VGG16 gives us a 4096-number summary of what's in each image\n",
    "4. **Set to Evaluation Mode**: This turns off training features we don't need\n",
    "\n",
    "**Why VGG16?**\n",
    "- It's already trained, so we don't start from scratch\n",
    "- It's good at understanding what's in images\n",
    "- We use its \"knowledge\" to help generate captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723e8296",
   "metadata": {},
   "source": [
    "# Convert Images to Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21a43f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset: ['captions.txt', 'Images']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "# List all files in the downloaded dataset directory\n",
    "files = os.listdir(path)\n",
    "print(\"Files in dataset:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92ddfb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1000268201_693b08cb0e.jpg', '1001773457_577c3a7d70.jpg', '1002674143_1b742ab4b8.jpg', '1003163366_44323f5815.jpg', '1007129816_e794419615.jpg', '1007320043_627395c3d8.jpg', '1009434119_febe49276a.jpg', '1012212859_01547e3f17.jpg', '1015118661_980735411b.jpg', '1015584366_dfcec3c85a.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Construct path to the Images folder containing all Flickr8k images\n",
    "img_directory = os.path.join(path, \"Images\")\n",
    "# Display first 10 image filenames as a sanity check\n",
    "print(os.listdir(img_directory)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822f9883",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- Constructs the path to the Images folder within the dataset\n",
    "- Lists first 10 image filenames to verify dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5270840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Define image preprocessing pipeline for VGG16\n",
    "transform = transforms.Compose([\n",
    "    # Resize images to 224x224 (VGG16's required input size)\n",
    "    transforms.Resize((224, 224)),\n",
    "    # Convert PIL Image to PyTorch tensor (values 0-1)\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize using ImageNet mean and std (required for pre-trained VGG16)\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet RGB channel means\n",
    "        std=[0.229, 0.224, 0.225]     # ImageNet RGB channel stds\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf528bb",
   "metadata": {},
   "source": [
    "**Code Explanation (Line by Line):**\n",
    "\n",
    "**Line 1: Import PIL**\n",
    "- `from PIL import Image`: Python Imaging Library for loading/manipulating images\n",
    "\n",
    "**Line 3-13: Create Transformation Pipeline**\n",
    "- `transforms.Compose([...])`: Chains transformations into a single callable function\n",
    "  - Takes list of transformations\n",
    "  - Applies them sequentially to input image\n",
    "  - Returns transformed tensor\n",
    "\n",
    "**Line 5-6: Resize Transform**\n",
    "- `transforms.Resize((224, 224))`: Resizes image to 224√ó224 pixels\n",
    "  - **Why 224?** VGG16 was trained on 224√ó224 images (fixed input size)\n",
    "  - Maintains aspect ratio may cause distortion, but ensures compatibility\n",
    "  - Uses bilinear interpolation by default\n",
    "\n",
    "**Line 7-8: Tensor Conversion**\n",
    "- `transforms.ToTensor()`: Converts PIL Image to PyTorch tensor\n",
    "  - **Input**: PIL Image with pixel values [0, 255] (uint8)\n",
    "  - **Output**: Torch tensor with values [0.0, 1.0] (float32)\n",
    "  - **Shape**: (H, W, C) ‚Üí (C, H, W) (channels-first format)\n",
    "  - Divides by 255 automatically: `pixel / 255.0`\n",
    "\n",
    "**Line 9-13: Normalization**\n",
    "- `transforms.Normalize(mean=[...], std=[...])`: Standardizes pixel values\n",
    "  - **Formula**: `output = (input - mean) / std` for each channel\n",
    "  - **mean=[0.485, 0.456, 0.406]**: ImageNet dataset RGB channel means\n",
    "  - **std=[0.229, 0.224, 0.225]**: ImageNet dataset RGB channel standard deviations\n",
    "  - **Why?** VGG16 was trained on normalized ImageNet images\n",
    "  - **Result**: Each channel has ~zero mean and ~unit variance\n",
    "  - **Example**: Red channel pixel 0.8 ‚Üí `(0.8 - 0.485) / 0.229 ‚âà 1.375`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8d289c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary to store image_id -> feature_vector mappings\n",
    "# features = {}\n",
    "\n",
    "# # Loop through all images in the dataset\n",
    "# for img_name in tqdm.tqdm(os.listdir(img_directory)):\n",
    "#     # Construct full path to image file\n",
    "#     img_path = img_directory + \"/\" + img_name\n",
    "    \n",
    "#     # Load image and ensure it's in RGB format (not grayscale)\n",
    "#     image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "#     # Apply preprocessing transformations (resize, normalize)\n",
    "#     image = transform(image)\n",
    "    \n",
    "#     # Add batch dimension: (C, H, W) -> (1, C, H, W)\n",
    "#     image = image.unsqueeze(0)\n",
    "    \n",
    "#     # Extract 4096-dimensional feature vector using VGG16\n",
    "#     feature = model(image) \n",
    "    \n",
    "#     # Extract image ID by removing file extension (e.g., \"123.jpg\" -> \"123\")\n",
    "#     image_id = img_name.split(\".\")[0]\n",
    "    \n",
    "#     # Store feature vector as numpy array (remove batch dim, detach from graph)\n",
    "#     features[image_id] = feature.squeeze(0).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b2e24",
   "metadata": {},
   "source": [
    "**Code Explanation (Line by Line):**\n",
    "\n",
    "**Line 1-2: Initialize Storage**\n",
    "- `features = {}`: Empty dictionary to store `{image_id: feature_vector}` pairs\n",
    "  - Keys: Image IDs (strings like \"1000268201_693b08cb0e\")\n",
    "  - Values: NumPy arrays of shape (4096,)\n",
    "\n",
    "**Line 4-5: Loop Through Images**\n",
    "- `for img_name in tqdm.tqdm(os.listdir(img_directory)):`: Iterates over all image files\n",
    "  - `os.listdir(img_directory)`: Returns list of filenames (e.g., [\"1234.jpg\", \"5678.jpg\", ...])\n",
    "  - `tqdm.tqdm(...)`: Wraps iterable to show progress bar with ETA\n",
    "  - `img_name`: Current filename (e.g., \"1000268201_693b08cb0e.jpg\")\n",
    "\n",
    "**Line 6-7: Construct Image Path**\n",
    "- `img_path = img_directory + \"/\" + img_name`: Creates full path\n",
    "  - Example: `\"/path/to/Images/\" + \"1234.jpg\"` ‚Üí `\"/path/to/Images/1234.jpg\"`\n",
    "\n",
    "**Line 9-10: Load and Convert Image**\n",
    "- `image = Image.open(img_path).convert(\"RGB\")`:\n",
    "  - `Image.open(img_path)`: Loads image from disk using PIL\n",
    "  - `.convert(\"RGB\")`: Ensures 3-channel RGB format\n",
    "    - Some images might be grayscale (1 channel)\n",
    "    - Some might have alpha channel (RGBA, 4 channels)\n",
    "    - This standardizes all to RGB (3 channels)\n",
    "\n",
    "**Line 12-13: Apply Transformations**\n",
    "- `image = transform(image)`: Applies the preprocessing pipeline\n",
    "  - Resizes to 224√ó224\n",
    "  - Converts to tensor (values 0-1)\n",
    "  - Normalizes using ImageNet stats\n",
    "  - **Output Shape**: (3, 224, 224) tensor\n",
    "\n",
    "**Line 15-16: Add Batch Dimension**\n",
    "- `image = image.unsqueeze(0)`: Adds batch dimension at position 0\n",
    "  - **Before**: Shape (3, 224, 224) - single image\n",
    "  - **After**: Shape (1, 3, 224, 224) - batch of 1 image\n",
    "  - **Why?** Neural networks expect batched inputs: (batch_size, channels, height, width)\n",
    "  - VGG16 forward pass requires 4D tensor\n",
    "\n",
    "**Line 18-19: Extract Features**\n",
    "- `feature = model(image)`: Pass image through VGG16\n",
    "  - Processes through conv layers ‚Üí extracts visual patterns\n",
    "  - Passes through modified classifier ‚Üí outputs 4096-dim vector\n",
    "  - **Output Shape**: (1, 4096) - batch of 1 feature vector\n",
    "  - Contains high-level visual information (objects, textures, scenes)\n",
    "\n",
    "**Line 21-22: Extract Image ID**\n",
    "- `image_id = img_name.split(\".\")[0]`: Removes file extension\n",
    "  - `img_name = \"1000268201_693b08cb0e.jpg\"`\n",
    "  - `img_name.split(\".\")` ‚Üí `[\"1000268201_693b08cb0e\", \"jpg\"]`\n",
    "  - `[0]` ‚Üí `\"1000268201_693b08cb0e\"`\n",
    "  - Used as key to match with captions later\n",
    "\n",
    "**Line 24-25: Store Feature Vector**\n",
    "- `features[image_id] = feature.squeeze(0).detach().numpy()`:\n",
    "  - `.squeeze(0)`: Removes batch dimension (1, 4096) ‚Üí (4096,)\n",
    "  - `.detach()`: Detaches tensor from computation graph (saves memory)\n",
    "  - `.numpy()`: Converts PyTorch tensor to NumPy array\n",
    "  - Stores in dictionary: `features[\"1000268201_693b08cb0e\"] = array([0.23, 0.45, ...])`\n",
    "\n",
    "**Overall Process:**\n",
    "- Loops through 8,000 images\n",
    "- Each image ‚Üí 4096-dim feature vector\n",
    "- Takes ~10-15 minutes on CPU, ~2-3 minutes on GPU\n",
    "- Result: Dictionary with 8,000 entries ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "707d34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the extracted features to disk to avoid re-computing every time\n",
    "# This saves significant time - VGG16 feature extraction takes ~10 minutes\n",
    "# with open(\"features.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04b7263",
   "metadata": {},
   "source": [
    "**Code Explanation (Line by Line):**\n",
    "\n",
    "**Line 1: Import Pickle**\n",
    "- `import pickle`: Python module for object serialization (saving Python objects to disk)\n",
    "  - Converts Python objects ‚Üí byte streams ‚Üí files\n",
    "  - Can reconstruct objects later without recomputation\n",
    "\n",
    "**Line 3-5: Save Features**\n",
    "- `with open(\"features.pkl\", \"wb\") as f:`: Opens file in write-binary mode\n",
    "  - `\"features.pkl\"`: Filename (.pkl extension by convention)\n",
    "  - `\"wb\"`: Write mode + binary mode (pickle requires binary)\n",
    "  - `as f`: File handle for writing\n",
    "  - `with`: Context manager (automatically closes file when done)\n",
    "\n",
    "**Line 5: Pickle Dump**\n",
    "- `pickle.dump(features, f)`: Serializes dictionary to file\n",
    "  - `features`: Dictionary with 8,000 entries {img_id: 4096-dim array}\n",
    "  - `f`: File handle to write to\n",
    "  - **File Size**: ~130 MB (8,000 √ó 4,096 floats √ó 4 bytes/float)\n",
    "\n",
    "**Why Save?**\n",
    "- **Time Saving**: VGG16 extraction takes 10-15 minutes\n",
    "- **Reusability**: Load features in seconds instead of re-extracting\n",
    "- **Consistency**: Same features across multiple training runs\n",
    "- **Convenience**: Can share pre-computed features with others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e9bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features.pkl\", \"rb\") as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160b611",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- Loads pre-computed features from disk (skip VGG16 if already extracted)\n",
    "- Much faster than re-running feature extraction (seconds vs. minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9cecb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of caption lines: 40456\n",
      "Total characters in file: 3319280\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "with open(os.path.join(path,\"captions.txt\")) as f:\n",
    "    next(f) # skip header line\n",
    "    captions_data = f.read()\n",
    "print(\"Number of caption lines:\", len(captions_data.split(\"\\n\")))\n",
    "print(\"Total characters in file:\", len(captions_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf2910",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- Opens `captions.txt` file (format: \"image_id.jpg,caption text\")\n",
    "- `next(f)`: Skips header line (\"image,caption\")\n",
    "- Reads all caption data into a single string for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49233dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40456/40456 [00:00<00:00, 879277.67it/s]\n"
     ]
    }
   ],
   "source": [
    "mapped_captions = {}\n",
    "\n",
    "for line in tqdm.tqdm(captions_data.split(\"\\n\")):\n",
    "    tokens = line.split(\",\")\n",
    "    if len(tokens) < 2:\n",
    "        continue\n",
    "    img_id = tokens[0].split(\".\")[0]\n",
    "    caption = tokens[1].strip().lower()\n",
    "    \n",
    "    if img_id not in mapped_captions:\n",
    "        mapped_captions[img_id] = []\n",
    "    \n",
    "    # Always append the caption\n",
    "    mapped_captions[img_id].append(caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539bdd4b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Final Structure:**\n",
    "```python\n",
    "{\n",
    "  \"1000268201_693b08cb0e\": [\n",
    "    \"child in pink dress is climbing up set of stairs in an entry way\",\n",
    "    \"girl going into wooden building\",\n",
    "    \"little girl climbing into wooden playhouse\",\n",
    "    \"little girl climbing the stairs to her playhouse\",\n",
    "    \"little girl in pink dress going into wooden cabin\"\n",
    "  ],\n",
    "  \"1001773457_577c3a7d70\": [...],  # 5 more captions\n",
    "  ...  # 8,000 total images\n",
    "}\n",
    "```\n",
    "\n",
    "**Result**: Dictionary with 8,000 images, each having exactly 5 captions (40,000 total caption-image pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df772ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS> child in pink dress is climbing up set of stairs in an entry way <EOS>',\n",
       " '<SOS> girl going into wooden building <EOS>',\n",
       " '<SOS> little girl climbing into wooden playhouse <EOS>',\n",
       " '<SOS> little girl climbing the stairs to her playhouse <EOS>',\n",
       " '<SOS> little girl in pink dress going into wooden cabin <EOS>']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_captions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60fbd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8091"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapped_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_caption(mapped_captions):\n",
    "    for key,caption_list in mapped_captions.items():\n",
    "        for i in range(len(caption_list)):\n",
    "            caption= caption_list[i]\n",
    "            caption= re.sub(r\"[^a-zA-Z]\",\" \",caption)\n",
    "            caption= caption.split()\n",
    "            caption= [word for word in caption if len(word)>1]\n",
    "            caption= \" \".join(caption)\n",
    "            caption = \"<SOS> \" + caption + \" <EOS>\"\n",
    "            caption_list[i]= caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28119634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS> child in pink dress is climbing up set of stairs in an entry way <EOS>',\n",
       " '<SOS> girl going into wooden building <EOS>',\n",
       " '<SOS> little girl climbing into wooden playhouse <EOS>',\n",
       " '<SOS> little girl climbing the stairs to her playhouse <EOS>',\n",
       " '<SOS> little girl in pink dress going into wooden cabin <EOS>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_caption(mapped_captions)\n",
    "mapped_captions[\"1000268201_693b08cb0e\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875cb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40455"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions=[]\n",
    "for caption_list in mapped_captions.values():\n",
    "    all_captions.extend(caption_list)\n",
    "    \n",
    "len(all_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8348210",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- Flattens all captions into a single list\n",
    "- Used to calculate statistics like max caption length\n",
    "- Total: ~40,000 captions (8,000 images √ó 5 captions each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70888daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS> child in pink dress is climbing up set of stairs in an entry way <EOS>',\n",
       " '<SOS> girl going into wooden building <EOS>',\n",
       " '<SOS> little girl climbing into wooden playhouse <EOS>',\n",
       " '<SOS> little girl climbing the stairs to her playhouse <EOS>',\n",
       " '<SOS> little girl in pink dress going into wooden cabin <EOS>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_captions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734df095",
   "metadata": {},
   "source": [
    "# Initialize Tokenizer and Prepare Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded with vocab size: 30524\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use pre-trained tokenizer with existing vocabulary\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# Add custom special tokens for image captioning\n",
    "special_tokens_dict = {'additional_special_tokens': ['<SOS>', '<EOS>']}\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "print(f\"Tokenizer loaded with vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc25ba",
   "metadata": {},
   "source": [
    "**What This Code Does:**\n",
    "\n",
    "We set up a tokenizer (a tool that converts words to numbers) using BERT's vocabulary.\n",
    "\n",
    "**Simple Explanation:**\n",
    "\n",
    "- **Tokenizer**: Converts words into numbers that the computer can understand- These markers tell the model when a caption starts and ends\n",
    "\n",
    "- **BERT Vocabulary**: Uses 30,000 common words that BERT already knows- **Special Tokens**: We add `<SOS>` (Start Of Sentence) and `<EOS>` (End Of Sentence) markers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa117fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length=max(len(caption.split()) for caption in all_captions)\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498bcc6e",
   "metadata": {},
   "source": [
    "**What This Code Does:**\n",
    "\n",
    "Finds the longest caption in our dataset. We need this to make all captions the same length by adding padding (empty spaces)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5eac24",
   "metadata": {},
   "source": [
    "# Split Data: Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083c0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_ids=list(mapped_captions.keys())\n",
    "\n",
    "train_ids, val_ids = train_test_split(image_ids, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c82823",
   "metadata": {},
   "source": [
    "**What This Code Does:**\n",
    "\n",
    "Divides our images into two groups:\n",
    "- **Training set (80%)**: ~6,400 images to teach the model\n",
    "\n",
    "- **Validation set (20%)**: ~1,600 images to test how well it learnedThis way we can check if the model works on images it hasn't seen before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add75591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30524"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f4ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(img_ids, mapped_captions, features, tokenizer, max_length, batch_size):\n",
    "    X_img, X_seq, y_seq = [], [], []\n",
    "\n",
    "    while True:\n",
    "        for img_id in img_ids: # the list of the image ids\n",
    "            for caption in mapped_captions[img_id]:\n",
    "\n",
    "                seq = tokenizer.encode(\n",
    "                    caption,\n",
    "                    add_special_tokens=False,\n",
    "                    max_length=max_length,\n",
    "                    truncation=True\n",
    "                )\n",
    "\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq = seq[:i]\n",
    "                    out_seq = seq[i]\n",
    "\n",
    "                    in_seq = in_seq + [tokenizer.pad_token_id] * (max_length - len(in_seq))\n",
    "\n",
    "                    X_img.append(features[img_id])\n",
    "                    X_seq.append(in_seq)\n",
    "                    y_seq.append(out_seq)\n",
    "\n",
    "                    if len(X_img) == batch_size:\n",
    "                        yield (\n",
    "                            np.array(X_img, dtype=np.float32),\n",
    "                            np.array(X_seq, dtype=np.int64),\n",
    "                            np.array(y_seq, dtype=np.int64),\n",
    "                        )\n",
    "                        X_img, X_seq, y_seq = [], [], []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c747a21",
   "metadata": {},
   "source": [
    "**Code Explanation (Line by Line):**\n",
    "\n",
    "**Line 1: Function Signature**\n",
    "- `def data_generator(img_ids, mapped_captions, features, tokenizer, max_length, batch_size):`:\n",
    "  - `img_ids`: List of image IDs for this split (train or val)\n",
    "  - `mapped_captions`: Dictionary `{img_id: [captions]}`\n",
    "  - `features`: Dictionary `{img_id: 4096-dim vector}`\n",
    "  - `tokenizer`: BERT tokenizer for text ‚Üí token IDs\n",
    "  - `max_length`: Maximum caption length for padding\n",
    "  - `batch_size`: Number of samples per batch (e.g., 64)\n",
    "\n",
    "**Line 2-3: Initialize Batch Lists**\n",
    "- `X_img, X_seq, y_seq = [], [], []`: Three empty lists for batch data\n",
    "  - `X_img`: Image features\n",
    "  - `X_seq`: Input sequences (partial captions)\n",
    "  - `y_seq`: Target words (one-hot vectors)\n",
    "\n",
    "**Line 4: Get Vocabulary Size**\n",
    "- `vocab_size = len(tokenizer)`: Total number of unique tokens\n",
    "  - For BERT: ~30,522 tokens\n",
    "  - Used to create one-hot target vectors\n",
    "\n",
    "**Line 6: Infinite Loop**\n",
    "- `while True:`: Generator runs forever (yields batches indefinitely)\n",
    "  - Training calls `next(generator)` repeatedly\n",
    "  - Generator cycles through data infinitely (re-starts after all images processed)\n",
    "\n",
    "**Line 7: Loop Through Images**\n",
    "- `for img_id in img_ids:`: Process each image in the split\n",
    "  - For training: ~6,400 image IDs\n",
    "  - For validation: ~1,600 image IDs\n",
    "\n",
    "**Line 8: Get Captions for Image**\n",
    "- `caption_list = mapped_captions[img_id]`: Retrieves all 5 captions\n",
    "  - Each caption format: `\"<SOS> words here <EOS>\"`\n",
    "\n",
    "**Line 9: Loop Through Captions**\n",
    "- `for caption in caption_list:`: Process each of the 5 captions\n",
    "  - Creates 5 training samples per image\n",
    "  - Total: 8,000 images √ó 5 captions = 40,000 caption-image pairs\n",
    "\n",
    "**Line 10-11: Tokenize Caption**\n",
    "- `seq = tokenizer.encode(caption, add_special_tokens=False, max_length=max_length, truncation=True)`:\n",
    "  - `tokenizer.encode(...)`: Converts text to token IDs\n",
    "  - `add_special_tokens=False`: **CRITICAL** - Don't add BERT's [CLS]/[SEP] tokens\n",
    "    - Captions already have `<SOS>`/`<EOS>`\n",
    "    - Adding BERT tokens would contaminate training\n",
    "  - `max_length=max_length`: Maximum length before truncation\n",
    "  - `truncation=True`: Cut off if caption exceeds max_length\n",
    "  - **Example**: `\"<SOS> dog runs <EOS>\"` ‚Üí `[101, 3899, 3216, 102]` (token IDs)\n",
    "\n",
    "**Line 13: Create Autoregressive Samples**\n",
    "- `for i in range(1, len(seq)):`: Loop from index 1 to end\n",
    "  - **Autoregressive Training**: Model predicts next word given previous words\n",
    "  - **Example**: Caption = `[SOS, dog, runs, park, EOS]` creates 4 samples:\n",
    "    ```\n",
    "    Input: [SOS]           ‚Üí Target: dog\n",
    "    Input: [SOS, dog]      ‚Üí Target: runs  \n",
    "    Input: [SOS, dog, runs] ‚Üí Target: park\n",
    "    Input: [SOS, dog, runs, park] ‚Üí Target: EOS\n",
    "    ```\n",
    "\n",
    "**Line 14: Create Input Sequence**\n",
    "- `in_seq = seq[:i]`: Slice from start to current position\n",
    "  - `i=1`: `seq[:1]` = `[SOS]`\n",
    "  - `i=2`: `seq[:2]` = `[SOS, dog]`\n",
    "  - `i=3`: `seq[:3]` = `[SOS, dog, runs]`\n",
    "\n",
    "**Line 15: Create Target**\n",
    "- `out_seq = seq[i]`: Next word (single token ID)\n",
    "  - `i=1`: `seq[1]` = `dog`\n",
    "  - `i=2`: `seq[2]` = `runs`\n",
    "\n",
    "**Line 16-17: Pad Input Sequence**\n",
    "- `in_seq = in_seq + [tokenizer.pad_token_id] * (max_length - len(in_seq))`:\n",
    "  - **Padding**: Extends short sequences to max_length\n",
    "  - `tokenizer.pad_token_id`: Special padding token (usually 0)\n",
    "  - **Example**: `[SOS, dog]` with max_length=5 ‚Üí `[SOS, dog, PAD, PAD, PAD]`\n",
    "  - **Why?** Neural networks need fixed-size inputs for batching\n",
    "\n",
    "**Line 19-20: Create One-Hot Target**\n",
    "- `out_seq_categorical = np.zeros(vocab_size)`: Create zero vector of size vocab_size\n",
    "  - Example: `[0, 0, 0, ..., 0]` (30,522 zeros)\n",
    "- `out_seq_categorical[out_seq] = 1`: Set target word position to 1\n",
    "  - If `out_seq = 3899` (dog), creates: `[0, 0, ..., 1, ..., 0]` (1 at index 3899)\n",
    "  - **One-hot encoding**: Only one element is 1, rest are 0\n",
    "\n",
    "**Line 22-24: Accumulate Batch Samples**\n",
    "- `X_img.append(features[img_id])`: Add image features (4096-dim)\n",
    "- `X_seq.append(in_seq)`: Add padded input sequence\n",
    "- `y_seq.append(out_seq_categorical)`: Add one-hot target\n",
    "\n",
    "**Line 26-28: Yield Batch When Full**\n",
    "- `if len(X_img) == batch_size:`: Check if batch complete\n",
    "  - Typically batch_size = 64 samples\n",
    "- `yield np.array(X_img), np.array(X_seq), np.array(y_seq)`: Return batch\n",
    "  - Converts lists to NumPy arrays\n",
    "  - **Shapes**: X_img: (64, 4096), X_seq: (64, max_length), y_seq: (64, vocab_size)\n",
    "- `X_img, X_seq, y_seq = [], [], []`: Reset lists for next batch\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Autoregressive Training**: Model learns to predict next word given context\n",
    "- **Teacher Forcing**: During training, model sees correct previous words (not its predictions)\n",
    "- **Padding**: All sequences same length for efficient batching\n",
    "- **One-Hot Targets**: CrossEntropyLoss expects this format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b55b5dd",
   "metadata": {},
   "source": [
    "## üîç Deep Dive: How the Data Generator Works\n",
    "\n",
    "The `data_generator` function is the **heart of the training pipeline**. It converts our caption data into autoregressive training samples.\n",
    "\n",
    "### üéØ Main Goal\n",
    "Transform each caption into multiple training samples where the model learns to predict **one word at a time** based on previous words.\n",
    "\n",
    "### üìä Example Walkthrough\n",
    "\n",
    "**Input Caption**: `\"<SOS> dog runs fast <EOS>\"`  \n",
    "**Tokenized**: `[30522, 3899, 3216, 2698, 30523]`\n",
    "\n",
    "**Generated Training Samples**:\n",
    "```\n",
    "Sample 1:  Input: [30522]                      ‚Üí Target: 3899 (dog)\n",
    "Sample 2:  Input: [30522, 3899]                ‚Üí Target: 3216 (runs)\n",
    "Sample 3:  Input: [30522, 3899, 3216]          ‚Üí Target: 2698 (fast)\n",
    "Sample 4:  Input: [30522, 3899, 3216, 2698]    ‚Üí Target: 30523 (<EOS>)\n",
    "```\n",
    "\n",
    "Each input is **padded to max_length** (e.g., 40 tokens) with padding tokens (0):\n",
    "```\n",
    "Sample 1:  [30522, 0, 0, 0, ..., 0]  ‚Üí Target: 3899\n",
    "Sample 2:  [30522, 3899, 0, 0, ..., 0]  ‚Üí Target: 3216\n",
    "```\n",
    "\n",
    "### üîÑ Training Flow\n",
    "\n",
    "1. **Image Loop**: Process each of 6,400 training images\n",
    "2. **Caption Loop**: Each image has 5 captions\n",
    "3. **Autoregressive Loop**: Each caption generates 3-35 training samples (depends on length)\n",
    "4. **Batch Accumulation**: Collect 64 samples before yielding\n",
    "\n",
    "**Total Training Samples per Epoch**:\n",
    "- 6,400 images √ó 5 captions √ó ~15 words avg = **~480,000 training samples**\n",
    "- Organized into ~7,500 batches (480,000 √∑ 64)\n",
    "\n",
    "### üß† Why This Approach?\n",
    "\n",
    "**Teacher Forcing**: During training, the model always sees the **correct previous words**, not its own predictions. This:\n",
    "- Stabilizes training\n",
    "- Speeds up convergence\n",
    "- Prevents error accumulation\n",
    "\n",
    "**Autoregressive Learning**: The model learns the **sequential nature** of language:\n",
    "- After seeing \"dog\", it learns \"runs\" is likely\n",
    "- After \"dog runs\", it learns \"fast\" or \"in\" might follow\n",
    "- Captures grammar, context, and image-text relationships\n",
    "\n",
    "### ‚ö° Key Implementation Details\n",
    "\n",
    "1. **No Special Tokens**: `add_special_tokens=False` prevents BERT's [CLS]/[SEP] from contaminating captions\n",
    "2. **Padding Strategy**: All sequences padded to `max_length` for efficient GPU batching\n",
    "3. **One-Hot Encoding**: Target words converted to one-hot vectors for CrossEntropyLoss\n",
    "4. **Infinite Generator**: `while True` loop allows unlimited epochs without restarting\n",
    "5. **Same Image Features**: All 5 captions for an image share the same VGG16 features (efficiency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d98ab",
   "metadata": {},
   "source": [
    "# Create the Caption Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b743940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_idx=0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Image ‚Üí initial LSTM state\n",
    "        self.img_to_h = nn.Linear(4096, 256)\n",
    "        self.img_to_c = nn.Linear(4096, 256)\n",
    "\n",
    "        # Text embedding + LSTM\n",
    "        self.embedding = nn.Embedding(vocab_size, 256, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(256, 256, batch_first=True)\n",
    "\n",
    "        # Predict next word at EACH timestep\n",
    "        self.fc = nn.Linear(256, vocab_size)\n",
    "\n",
    "    def forward(self, img_features, captions):\n",
    "\n",
    "\n",
    "        # Initialize LSTM hidden & cell from image\n",
    "        h0 = torch.tanh(self.img_to_h(img_features)).unsqueeze(0)  # (1, B, 256)\n",
    "        c0 = torch.tanh(self.img_to_c(img_features)).unsqueeze(0)  # (1, B, 256)\n",
    "\n",
    "        # Embed caption tokens\n",
    "        emb = self.embedding(captions)                             # (B, T, 256)\n",
    "\n",
    "        # LSTM over sequence\n",
    "        outputs, _ = self.lstm(emb, (h0, c0))                      # (B, T, 256)\n",
    "\n",
    "        # Vocabulary prediction at each timestep\n",
    "        logits = self.fc(outputs)                                  # (B, T, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceac2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ImageCaptionModel(vocab_size=len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fe765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=[(1, 4096), (1, max_length)], dtypes=[torch.float32, torch.long])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5220e12",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- Uses `torchinfo.summary()` to display model architecture\n",
    "- Shows layer-by-layer parameters, shapes, and total parameter count\n",
    "- Useful for debugging and understanding model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda8d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = tokenizer.pad_token_id  # or 0 if you used 0\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX) # withouth ignore_index the model will mosttly  predict pad\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs=10  # Increased for better learning\n",
    "batch_size=64\n",
    "steps_per_epoch = len(train_ids) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa5f14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    generator = data_generator(\n",
    "        train_ids, mapped_captions, features,\n",
    "        tokenizer, max_length, batch_size\n",
    "    )\n",
    "\n",
    "    for step in tqdm.tqdm(range(steps_per_epoch)):\n",
    "        img_features, seqs, targets = next(generator)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img_features_tensor = torch.tensor(img_features, dtype=torch.float32)\n",
    "        seqs_tensor = torch.tensor(seqs, dtype=torch.long)\n",
    "        targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "        outputs = model(img_features_tensor, seqs_tensor)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Extract predictions at the last non-padding position for each sequence\n",
    "        # Find the length of each sequence (number of non-pad tokens)\n",
    "        lengths = (seqs_tensor != tokenizer.pad_token_id).sum(dim=1)  # (B,)\n",
    "        \n",
    "        # Get the prediction at the last valid position\n",
    "        batch_indices = torch.arange(outputs.size(0))\n",
    "        last_outputs = outputs[batch_indices, lengths - 1]  # (B, vocab_size)\n",
    "\n",
    "        loss = criterion(last_outputs, targets_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/steps_per_epoch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11057aa",
   "metadata": {},
   "source": [
    "## üîÑ Understanding `steps_per_epoch` and Yielding\n",
    "\n",
    "### **The Calculation**\n",
    "```python\n",
    "steps_per_epoch = len(train_ids) // batch_size\n",
    "# = 6,400 images √∑ 64 samples per batch = 100 steps\n",
    "```\n",
    "\n",
    "### **Why Do We Need `steps_per_epoch`?**\n",
    "\n",
    "**Problem:** Our `data_generator` uses `while True:` - an **infinite loop** that never stops on its own.\n",
    "\n",
    "```python\n",
    "def data_generator(...):\n",
    "    while True:  # ‚Üê Runs forever!\n",
    "        for img_id in img_ids:\n",
    "            # Generate batches...\n",
    "            yield batch  # ‚Üê Returns batch but doesn't exit\n",
    "```\n",
    "\n",
    "**Without `steps_per_epoch`:** Training would run indefinitely, never advancing to the next epoch.\n",
    "\n",
    "**With `steps_per_epoch`:** We manually control how many batches to process:\n",
    "\n",
    "```python\n",
    "for step in range(steps_per_epoch):  # ‚Üê Stops after exactly 100 iterations\n",
    "    batch = next(generator)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What is `yield`?**\n",
    "\n",
    "`yield` creates a **generator function** that:\n",
    "1. **Pauses execution** and returns a value\n",
    "2. **Remembers state** (variables, loop position)\n",
    "3. **Resumes** from where it left off when called again\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Generator yields batches one at a time\n",
    "def data_generator():\n",
    "    while True:\n",
    "        # ... process data ...\n",
    "        yield batch  # ‚Üê Pause here, return batch, wait for next call\n",
    "\n",
    "# Training loop\n",
    "for step in range(100):\n",
    "    batch = next(generator)  # ‚Üê Resume generator, get next batch\n",
    "```\n",
    "\n",
    "**Memory Efficiency:** Instead of loading all 480,000 training samples into RAM, we generate batches **on-demand**.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works Together**\n",
    "\n",
    "| Step | What Happens |\n",
    "|------|-------------|\n",
    "| 1 | `generator = data_generator(...)` creates generator (doesn't execute yet) |\n",
    "| 2 | `next(generator)` calls generator ‚Üí processes data ‚Üí hits `yield` ‚Üí returns batch |\n",
    "| 3 | Generator **pauses** (remembers position in loops) |\n",
    "| 4 | `next(generator)` again ‚Üí generator **resumes** ‚Üí processes more data ‚Üí yields next batch |\n",
    "| 5 | Repeat 100 times (`steps_per_epoch`) |\n",
    "| 6 | After 100 steps, loop exits ‚Üí **epoch complete** |\n",
    "| 7 | Next epoch creates **new generator** (restarts from beginning) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Visual Flow**\n",
    "\n",
    "```\n",
    "Epoch 1:\n",
    "  Step 1: next(gen) ‚Üí yields batch 1 (64 samples)\n",
    "  Step 2: next(gen) ‚Üí yields batch 2 (64 samples)\n",
    "  ...\n",
    "  Step 100: next(gen) ‚Üí yields batch 100 (64 samples)\n",
    "  ‚úì Total: 6,400 samples processed (1 full pass through training data)\n",
    "\n",
    "Epoch 2:\n",
    "  Create new generator (restarts)\n",
    "  Step 1-100: Process same 6,400 samples again\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insight**\n",
    "\n",
    "**`steps_per_epoch`** acts as a **manual epoch boundary** for infinite generators. It ensures:\n",
    "- ‚úÖ Each epoch processes the full training dataset exactly once\n",
    "- ‚úÖ Training progresses through multiple epochs\n",
    "- ‚úÖ Loss is calculated per epoch for monitoring\n",
    "- ‚úÖ Training eventually completes after all epochs\n",
    "\n",
    "Without it, the `while True` loop would **never allow the epoch loop to advance**, causing the model to train indefinitely on the first epoch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bc132",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"image_caption_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c73a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer,tokenizer):\n",
    "    return tokenizer.decode([integer],skip_special_tokens=True,clean_up_tokenization_spaces=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a12e4",
   "metadata": {},
   "source": [
    "**Code Explanation:**\n",
    "- Helper function to convert token ID back to word using tokenizer\n",
    "- `skip_special_tokens=True`: Removes `<SOS>`, `<EOS>`, `[PAD]` from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df63429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(img_id, model, tokenizer, max_length, features, temperature=0.8):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    img_feature = features[img_id]\n",
    "\n",
    "    sos_id = tokenizer.encode(\"<SOS>\", add_special_tokens=False)[0]\n",
    "    eos_id = tokenizer.encode(\"<EOS>\", add_special_tokens=False)[0]\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    caption = [sos_id]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - 1):\n",
    "            # Pad caption to max_length\n",
    "            padded_caption = caption + [pad_id] * (max_length - len(caption))\n",
    "            seq_tensor = torch.tensor([padded_caption], dtype=torch.long)\n",
    "            img_tensor = torch.tensor([img_feature], dtype=torch.float32)\n",
    "\n",
    "            # Forward pass - single prediction per step\n",
    "            logits = model(img_tensor, seq_tensor)  # (1, vocab_size)\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits, dim=-1)  # (1, vocab_size)\n",
    "            next_word = torch.multinomial(probs.squeeze(0), 1).item()\n",
    "\n",
    "            if next_word in (eos_id, pad_id):\n",
    "                break\n",
    "\n",
    "            caption.append(next_word)\n",
    "\n",
    "    return tokenizer.decode(caption, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d00e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model weights exist\n",
    "import os\n",
    "if not os.path.exists(\"image_caption_model.pth\"):\n",
    "    print(\"ERROR: Model weights not found. Make sure to run the training cell first!\")\n",
    "else:\n",
    "    # Load the trained model\n",
    "    caption_model = ImageCaptionModel(vocab_size=len(tokenizer))\n",
    "    caption_model.load_state_dict(torch.load(\"image_caption_model.pth\"))\n",
    "    caption_model.eval()\n",
    "    \n",
    "    # Test caption generation on validation samples\n",
    "    print(\"Testing caption generation on validation samples:\\n\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        img_id = val_ids[i]\n",
    "        \n",
    "        # Generate caption using the trained caption model\n",
    "        generated = predict_caption(img_id, caption_model, tokenizer, max_length, features)\n",
    "        \n",
    "        # Get actual captions\n",
    "        actual_captions = mapped_captions[img_id]\n",
    "        \n",
    "        print(f\"Image ID: {img_id}\")\n",
    "        print(f\"Generated caption: {generated}\")\n",
    "        print(f\"Actual captions:\")\n",
    "        for cap in actual_captions:\n",
    "            print(f\"  - {cap}\")\n",
    "        print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
